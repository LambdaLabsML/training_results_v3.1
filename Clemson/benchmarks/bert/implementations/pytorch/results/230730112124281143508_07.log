Beginning trial 07 of 10
:::DLPAL /scratch/nnisbet/bert.sif 1339 18 node[0400-0401,0403-0418]
hosts=node0400 node0401 node0403 node0404 node0405 node0406 node0407 node0408 node0409 node0410 node0411 node0412 node0413 node0414 node0415 node0416 node0417 node0418 
 1: INFO:    underlay of /etc/localtime required more than 50 (96) bind mounts
 0: INFO:    underlay of /etc/localtime required more than 50 (96) bind mounts
 9: INFO:    underlay of /etc/localtime required more than 50 (96) bind mounts
 2: INFO:    underlay of /etc/localtime required more than 50 (96) bind mounts
17: INFO:    underlay of /etc/localtime required more than 50 (96) bind mounts
18: INFO:    underlay of /etc/localtime required more than 50 (96) bind mounts
29: INFO:    underlay of /etc/localtime required more than 50 (96) bind mounts
 0: INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (478) bind mounts
 1: INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (478) bind mounts
 6: INFO:    underlay of /etc/localtime required more than 50 (96) bind mounts
10: INFO:    underlay of /etc/localtime required more than 50 (96) bind mounts
 8: INFO:    underlay of /etc/localtime required more than 50 (96) bind mounts
14: INFO:    underlay of /etc/localtime required more than 50 (96) bind mounts
21: INFO:    underlay of /etc/localtime required more than 50 (96) bind mounts
33: INFO:    underlay of /etc/localtime required more than 50 (96) bind mounts
22: INFO:    underlay of /etc/localtime required more than 50 (96) bind mounts
 3: INFO:    underlay of /etc/localtime required more than 50 (96) bind mounts
 4: INFO:    underlay of /etc/localtime required more than 50 (96) bind mounts
24: INFO:    underlay of /etc/localtime required more than 50 (96) bind mounts
26: INFO:    underlay of /etc/localtime required more than 50 (96) bind mounts
16: INFO:    underlay of /etc/localtime required more than 50 (96) bind mounts
19: INFO:    underlay of /etc/localtime required more than 50 (96) bind mounts
35: INFO:    underlay of /etc/localtime required more than 50 (96) bind mounts
13: INFO:    underlay of /etc/localtime required more than 50 (96) bind mounts
31: INFO:    underlay of /etc/localtime required more than 50 (96) bind mounts
28: INFO:    underlay of /etc/localtime required more than 50 (96) bind mounts
 7: INFO:    underlay of /etc/localtime required more than 50 (96) bind mounts
11: INFO:    underlay of /etc/localtime required more than 50 (96) bind mounts
15: INFO:    underlay of /etc/localtime required more than 50 (96) bind mounts
20: INFO:    underlay of /etc/localtime required more than 50 (96) bind mounts
32: INFO:    underlay of /etc/localtime required more than 50 (96) bind mounts
23: INFO:    underlay of /etc/localtime required more than 50 (96) bind mounts
 5: INFO:    underlay of /etc/localtime required more than 50 (96) bind mounts
25: INFO:    underlay of /etc/localtime required more than 50 (96) bind mounts
27: INFO:    underlay of /etc/localtime required more than 50 (96) bind mounts
34: INFO:    underlay of /etc/localtime required more than 50 (96) bind mounts
12: INFO:    underlay of /etc/localtime required more than 50 (96) bind mounts
30: INFO:    underlay of /etc/localtime required more than 50 (96) bind mounts
 0: 13:4: not a valid test operator: (
 0: 13:4: not a valid test operator: 525.125.06
 1: 13:4: not a valid test operator: (
 1: 13:4: not a valid test operator: 525.125.06
19: INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (478) bind mounts
18: INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (478) bind mounts
20: INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (478) bind mounts
21: INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (478) bind mounts
17: INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (478) bind mounts
16: INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (478) bind mounts
22: INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (478) bind mounts
23: INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (478) bind mounts
29: INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (478) bind mounts
28: INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (478) bind mounts
 5: INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (478) bind mounts
15: INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (478) bind mounts
 4: INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (478) bind mounts
14: INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (478) bind mounts
24: INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (478) bind mounts
10: INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (478) bind mounts
25: INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (478) bind mounts
11: INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (478) bind mounts
 7: INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (478) bind mounts
 6: INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (478) bind mounts
35: INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (478) bind mounts
34: INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (478) bind mounts
 9: INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (478) bind mounts
 8: INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (478) bind mounts
32: INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (478) bind mounts
33: INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (478) bind mounts
 3: INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (478) bind mounts
 2: INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (478) bind mounts
13: INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (478) bind mounts
12: INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (478) bind mounts
26: INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (478) bind mounts
27: INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (478) bind mounts
31: INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (478) bind mounts
30: INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (478) bind mounts
20: 13:4: not a valid test operator: (
20: 13:4: not a valid test operator: 525.125.06
19: 13:4: not a valid test operator: (
19: 13:4: not a valid test operator: 525.125.06
18: 13:4: not a valid test operator: (
18: 13:4: not a valid test operator: 525.125.06
21: 13:4: not a valid test operator: (
21: 13:4: not a valid test operator: 525.125.06
16: 13:4: not a valid test operator: (
16: 13:4: not a valid test operator: 525.125.06
17: 13:4: not a valid test operator: (
17: 13:4: not a valid test operator: 525.125.06
22: 13:4: not a valid test operator: (
22: 13:4: not a valid test operator: 525.125.06
23: 13:4: not a valid test operator: (
23: 13:4: not a valid test operator: 525.125.06
28: 13:4: not a valid test operator: (
28: 13:4: not a valid test operator: 525.125.06
 5: 13:4: not a valid test operator: (
 5: 13:4: not a valid test operator: 525.125.06
15: 13:4: not a valid test operator: (
15: 13:4: not a valid test operator: 525.125.06
14: 13:4: not a valid test operator: (
14: 13:4: not a valid test operator: 525.125.06
29: 13:4: not a valid test operator: (
29: 13:4: not a valid test operator: 525.125.06
10: 13:4: not a valid test operator: (
10: 13:4: not a valid test operator: 525.125.06
24: 13:4: not a valid test operator: (
24: 13:4: not a valid test operator: 525.125.06
 4: 13:4: not a valid test operator: (
 4: 13:4: not a valid test operator: 525.125.06
25: 13:4: not a valid test operator: (
25: 13:4: not a valid test operator: 525.125.06
35: 13:4: not a valid test operator: (
35: 13:4: not a valid test operator: 525.125.06
34: 13:4: not a valid test operator: (
34: 13:4: not a valid test operator: 525.125.06
 7: 13:4: not a valid test operator: (
 7: 13:4: not a valid test operator: 525.125.06
 6: 13:4: not a valid test operator: (
 6: 13:4: not a valid test operator: 525.125.06
11: 13:4: not a valid test operator: (
11: 13:4: not a valid test operator: 525.125.06
32: 13:4: not a valid test operator: (
32: 13:4: not a valid test operator: 525.125.06
 8: 13:4: not a valid test operator: (
 8: 13:4: not a valid test operator: 525.125.06
 9: 13:4: not a valid test operator: (
 9: 13:4: not a valid test operator: 525.125.06
33: 13:4: not a valid test operator: (
33: 13:4: not a valid test operator: 525.125.06
13: 13:4: not a valid test operator: (
13: 13:4: not a valid test operator: 525.125.06
 3: 13:4: not a valid test operator: (
 3: 13:4: not a valid test operator: 525.125.06
12: 13:4: not a valid test operator: (
12: 13:4: not a valid test operator: 525.125.06
 2: 13:4: not a valid test operator: (
 2: 13:4: not a valid test operator: 525.125.06
27: 13:4: not a valid test operator: (
27: 13:4: not a valid test operator: 525.125.06
26: 13:4: not a valid test operator: (
26: 13:4: not a valid test operator: 525.125.06
30: 13:4: not a valid test operator: (
30: 13:4: not a valid test operator: 525.125.06
31: 13:4: not a valid test operator: (
31: 13:4: not a valid test operator: 525.125.06
 1: Run vars: id 1339 gpus 2 mparams ''
 1: STARTING TIMING RUN AT 2023-07-30 12:22:25 PM
 1: WORLD_SIZE=36
 1: slurm_job_nodelist=node[0400-0401,0403-0418]
 1: MASTER_ADDR=node0400
 1: HOSTNAME=node0400
 1:      python -u     /workspace/bert/run_pretraining.py         --train_batch_size=36     --learning_rate=0.0006     --opt_lamb_beta_1=0.8     --opt_lamb_beta_2=0.996     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=100000     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data     --init_checkpoint=/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt     --max_pack_factor=3          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.1     --max_samples_termination=1000000000     --eval_iter_start_samples=400000 --eval_iter_samples=200000     --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples=10000     --cache_eval_data     --output_dir=/results     --fp16     --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1     --gradient_accumulation_steps=1     --log_freq=0     --bert_config_path=/wor
 1: kspace/phase1/bert_config.json  --local_rank=1  --allreduce_post_accumulation --allreduce_post_accumulation_fp16  --dense_seq_output --pad_fmha --fused_bias_fc --fused_bias_mha --fused_dropout_add --fused_gemm_gelu --packed_samples --cuda_graph_mode 'segmented'   --seed=26202
 0: Run vars: id 1339 gpus 2 mparams ''
 0: STARTING TIMING RUN AT 2023-07-30 12:22:25 PM
 0: WORLD_SIZE=36
 0: slurm_job_nodelist=node[0400-0401,0403-0418]
 0: MASTER_ADDR=node0400
 0: HOSTNAME=node0400
 0:      python -u     /workspace/bert/run_pretraining.py         --train_batch_size=36     --learning_rate=0.0006     --opt_lamb_beta_1=0.8     --opt_lamb_beta_2=0.996     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=100000     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data     --init_checkpoint=/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt     --max_pack_factor=3          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.1     --max_samples_termination=1000000000     --eval_iter_start_samples=400000 --eval_iter_samples=200000     --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples=10000     --cache_eval_data     --output_dir=/results     --fp16     --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1     --gradient_accumulation_steps=1     --log_freq=0     --bert_config_path=/wor
 0: kspace/phase1/bert_config.json  --local_rank=0  --allreduce_post_accumulation --allreduce_post_accumulation_fp16  --dense_seq_output --pad_fmha --fused_bias_fc --fused_bias_mha --fused_dropout_add --fused_gemm_gelu --packed_samples --cuda_graph_mode 'segmented'   --seed=10141
18: Run vars: id 1339 gpus 2 mparams ''
10: Run vars: id 1339 gpus 2 mparams ''
18: STARTING TIMING RUN AT 2023-07-30 12:22:25 PM
18: WORLD_SIZE=36
10: STARTING TIMING RUN AT 2023-07-30 12:22:25 PM
10: WORLD_SIZE=36
18: slurm_job_nodelist=node[0400-0401,0403-0418]
18: MASTER_ADDR=node0400
18: HOSTNAME=node0400
18:      python -u     /workspace/bert/run_pretraining.py         --train_batch_size=36     --learning_rate=0.0006     --opt_lamb_beta_1=0.8     --opt_lamb_beta_2=0.996     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=100000     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data     --init_checkpoint=/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt     --max_pack_factor=3          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.1     --max_samples_termination=1000000000     --eval_iter_start_samples=400000 --eval_iter_samples=200000     --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples=10000     --cache_eval_data     --output_dir=/results     --fp16     --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1     --gradient_accumulation_steps=1     --log_freq=0     --bert_config_path=/wor
18: kspace/phase1/bert_config.json  --local_rank=0  --allreduce_post_accumulation --allreduce_post_accumulation_fp16  --dense_seq_output --pad_fmha --fused_bias_fc --fused_bias_mha --fused_dropout_add --fused_gemm_gelu --packed_samples --cuda_graph_mode 'segmented'   --seed=8810
10: slurm_job_nodelist=node[0400-0401,0403-0418]
10: MASTER_ADDR=node0400
10: HOSTNAME=node0400
10:      python -u     /workspace/bert/run_pretraining.py         --train_batch_size=36     --learning_rate=0.0006     --opt_lamb_beta_1=0.8     --opt_lamb_beta_2=0.996     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=100000     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data     --init_checkpoint=/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt     --max_pack_factor=3          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.1     --max_samples_termination=1000000000     --eval_iter_start_samples=400000 --eval_iter_samples=200000     --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples=10000     --cache_eval_data     --output_dir=/results     --fp16     --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1     --gradient_accumulation_steps=1     --log_freq=0     --bert_config_path=/wor
10: kspace/phase1/bert_config.json  --local_rank=0  --allreduce_post_accumulation --allreduce_post_accumulation_fp16  --dense_seq_output --pad_fmha --fused_bias_fc --fused_bias_mha --fused_dropout_add --fused_gemm_gelu --packed_samples --cuda_graph_mode 'segmented'   --seed=17015
20: Run vars: id 1339 gpus 2 mparams ''
16: Run vars: id 1339 gpus 2 mparams ''
16: STARTING TIMING RUN AT 2023-07-30 12:22:25 PM
16: WORLD_SIZE=36
20: STARTING TIMING RUN AT 2023-07-30 12:22:25 PM
20: WORLD_SIZE=36
34: Run vars: id 1339 gpus 2 mparams ''
16: slurm_job_nodelist=node[0400-0401,0403-0418]
16: MASTER_ADDR=node0400
16: HOSTNAME=node0400
16:      python -u     /workspace/bert/run_pretraining.py         --train_batch_size=36     --learning_rate=0.0006     --opt_lamb_beta_1=0.8     --opt_lamb_beta_2=0.996     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=100000     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data     --init_checkpoint=/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt     --max_pack_factor=3          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.1     --max_samples_termination=1000000000     --eval_iter_start_samples=400000 --eval_iter_samples=200000     --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples=10000     --cache_eval_data     --output_dir=/results     --fp16     --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1     --gradient_accumulation_steps=1     --log_freq=0     --bert_config_path=/wor
16: kspace/phase1/bert_config.json  --local_rank=0  --allreduce_post_accumulation --allreduce_post_accumulation_fp16  --dense_seq_output --pad_fmha --fused_bias_fc --fused_bias_mha --fused_dropout_add --fused_gemm_gelu --packed_samples --cuda_graph_mode 'segmented'   --seed=9591
20: slurm_job_nodelist=node[0400-0401,0403-0418]
20: MASTER_ADDR=node0400
20: HOSTNAME=node0400
20:      python -u     /workspace/bert/run_pretraining.py         --train_batch_size=36     --learning_rate=0.0006     --opt_lamb_beta_1=0.8     --opt_lamb_beta_2=0.996     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=100000     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data     --init_checkpoint=/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt     --max_pack_factor=3          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.1     --max_samples_termination=1000000000     --eval_iter_start_samples=400000 --eval_iter_samples=200000     --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples=10000     --cache_eval_data     --output_dir=/results     --fp16     --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1     --gradient_accumulation_steps=1     --log_freq=0     --bert_config_path=/wor
20: kspace/phase1/bert_config.json  --local_rank=0  --allreduce_post_accumulation --allreduce_post_accumulation_fp16  --dense_seq_output --pad_fmha --fused_bias_fc --fused_bias_mha --fused_dropout_add --fused_gemm_gelu --packed_samples --cuda_graph_mode 'segmented'   --seed=6293
34: STARTING TIMING RUN AT 2023-07-30 12:22:25 PM
34: WORLD_SIZE=36
 5: Run vars: id 1339 gpus 2 mparams ''
 7: Run vars: id 1339 gpus 2 mparams ''
34: slurm_job_nodelist=node[0400-0401,0403-0418]
34: MASTER_ADDR=node0400
34: HOSTNAME=node0400
34:      python -u     /workspace/bert/run_pretraining.py         --train_batch_size=36     --learning_rate=0.0006     --opt_lamb_beta_1=0.8     --opt_lamb_beta_2=0.996     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=100000     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data     --init_checkpoint=/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt     --max_pack_factor=3          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.1     --max_samples_termination=1000000000     --eval_iter_start_samples=400000 --eval_iter_samples=200000     --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples=10000     --cache_eval_data     --output_dir=/results     --fp16     --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1     --gradient_accumulation_steps=1     --log_freq=0     --bert_config_path=/wor
34: kspace/phase1/bert_config.json  --local_rank=0  --allreduce_post_accumulation --allreduce_post_accumulation_fp16  --dense_seq_output --pad_fmha --fused_bias_fc --fused_bias_mha --fused_dropout_add --fused_gemm_gelu --packed_samples --cuda_graph_mode 'segmented'   --seed=13601
24: Run vars: id 1339 gpus 2 mparams ''
 8: Run vars: id 1339 gpus 2 mparams ''
 5: STARTING TIMING RUN AT 2023-07-30 12:22:25 PM
 5: WORLD_SIZE=36
23: Run vars: id 1339 gpus 2 mparams ''
 7: STARTING TIMING RUN AT 2023-07-30 12:22:25 PM
 7: WORLD_SIZE=36
 5: slurm_job_nodelist=node[0400-0401,0403-0418]
 5: MASTER_ADDR=node0400
 5: HOSTNAME=node0400
 5:      python -u     /workspace/bert/run_pretraining.py         --train_batch_size=36     --learning_rate=0.0006     --opt_lamb_beta_1=0.8     --opt_lamb_beta_2=0.996     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=100000     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data     --init_checkpoint=/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt     --max_pack_factor=3          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.1     --max_samples_termination=1000000000     --eval_iter_start_samples=400000 --eval_iter_samples=200000     --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples=10000     --cache_eval_data     --output_dir=/results     --fp16     --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1     --gradient_accumulation_steps=1     --log_freq=0     --bert_config_path=/wor
 5: kspace/phase1/bert_config.json  --local_rank=1  --allreduce_post_accumulation --allreduce_post_accumulation_fp16  --dense_seq_output --pad_fmha --fused_bias_fc --fused_bias_mha --fused_dropout_add --fused_gemm_gelu --packed_samples --cuda_graph_mode 'segmented'   --seed=6137
24: STARTING TIMING RUN AT 2023-07-30 12:22:25 PM
24: WORLD_SIZE=36
 8: STARTING TIMING RUN AT 2023-07-30 12:22:25 PM
 7: slurm_job_nodelist=node[0400-0401,0403-0418]
 7: MASTER_ADDR=node0400
 7: HOSTNAME=node0400
 8: WORLD_SIZE=36
 7:      python -u     /workspace/bert/run_pretraining.py         --train_batch_size=36     --learning_rate=0.0006     --opt_lamb_beta_1=0.8     --opt_lamb_beta_2=0.996     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=100000     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data     --init_checkpoint=/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt     --max_pack_factor=3          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.1     --max_samples_termination=1000000000     --eval_iter_start_samples=400000 --eval_iter_samples=200000     --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples=10000     --cache_eval_data     --output_dir=/results     --fp16     --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1     --gradient_accumulation_steps=1     --log_freq=0     --bert_config_path=/wor
 7: kspace/phase1/bert_config.json  --local_rank=1  --allreduce_post_accumulation --allreduce_post_accumulation_fp16  --dense_seq_output --pad_fmha --fused_bias_fc --fused_bias_mha --fused_dropout_add --fused_gemm_gelu --packed_samples --cuda_graph_mode 'segmented'   --seed=4942
24: slurm_job_nodelist=node[0400-0401,0403-0418]
24: MASTER_ADDR=node0400
24: HOSTNAME=node0400
23: STARTING TIMING RUN AT 2023-07-30 12:22:25 PM
23: WORLD_SIZE=36
24:      python -u     /workspace/bert/run_pretraining.py         --train_batch_size=36     --learning_rate=0.0006     --opt_lamb_beta_1=0.8     --opt_lamb_beta_2=0.996     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=100000     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data     --init_checkpoint=/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt     --max_pack_factor=3          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.1     --max_samples_termination=1000000000     --eval_iter_start_samples=400000 --eval_iter_samples=200000     --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples=10000     --cache_eval_data     --output_dir=/results     --fp16     --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1     --gradient_accumulation_steps=1     --log_freq=0     --bert_config_path=/wor
24: kspace/phase1/bert_config.json  --local_rank=0  --allreduce_post_accumulation --allreduce_post_accumulation_fp16  --dense_seq_output --pad_fmha --fused_bias_fc --fused_bias_mha --fused_dropout_add --fused_gemm_gelu --packed_samples --cuda_graph_mode 'segmented'   --seed=30772
 8: slurm_job_nodelist=node[0400-0401,0403-0418]
 8: MASTER_ADDR=node0400
 8: HOSTNAME=node0400
 8:      python -u     /workspace/bert/run_pretraining.py         --train_batch_size=36     --learning_rate=0.0006     --opt_lamb_beta_1=0.8     --opt_lamb_beta_2=0.996     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=100000     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data     --init_checkpoint=/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt     --max_pack_factor=3          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.1     --max_samples_termination=1000000000     --eval_iter_start_samples=400000 --eval_iter_samples=200000     --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples=10000     --cache_eval_data     --output_dir=/results     --fp16     --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1     --gradient_accumulation_steps=1     --log_freq=0     --bert_config_path=/wor
 8: kspace/phase1/bert_config.json  --local_rank=0  --allreduce_post_accumulation --allreduce_post_accumulation_fp16  --dense_seq_output --pad_fmha --fused_bias_fc --fused_bias_mha --fused_dropout_add --fused_gemm_gelu --packed_samples --cuda_graph_mode 'segmented'   --seed=21337
23: slurm_job_nodelist=node[0400-0401,0403-0418]
23: MASTER_ADDR=node0400
23: HOSTNAME=node0400
23:      python -u     /workspace/bert/run_pretraining.py         --train_batch_size=36     --learning_rate=0.0006     --opt_lamb_beta_1=0.8     --opt_lamb_beta_2=0.996     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=100000     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data     --init_checkpoint=/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt     --max_pack_factor=3          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.1     --max_samples_termination=1000000000     --eval_iter_start_samples=400000 --eval_iter_samples=200000     --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples=10000     --cache_eval_data     --output_dir=/results     --fp16     --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1     --gradient_accumulation_steps=1     --log_freq=0     --bert_config_path=/wor
23: kspace/phase1/bert_config.json  --local_rank=1  --allreduce_post_accumulation --allreduce_post_accumulation_fp16  --dense_seq_output --pad_fmha --fused_bias_fc --fused_bias_mha --fused_dropout_add --fused_gemm_gelu --packed_samples --cuda_graph_mode 'segmented'   --seed=30039
32: Run vars: id 1339 gpus 2 mparams ''
32: STARTING TIMING RUN AT 2023-07-30 12:22:25 PM
32: WORLD_SIZE=36
13: Run vars: id 1339 gpus 2 mparams ''
32: slurm_job_nodelist=node[0400-0401,0403-0418]
32: MASTER_ADDR=node0400
32: HOSTNAME=node0400
32:      python -u     /workspace/bert/run_pretraining.py         --train_batch_size=36     --learning_rate=0.0006     --opt_lamb_beta_1=0.8     --opt_lamb_beta_2=0.996     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=100000     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data     --init_checkpoint=/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt     --max_pack_factor=3          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.1     --max_samples_termination=1000000000     --eval_iter_start_samples=400000 --eval_iter_samples=200000     --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples=10000     --cache_eval_data     --output_dir=/results     --fp16     --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1     --gradient_accumulation_steps=1     --log_freq=0     --bert_config_path=/wor
32: kspace/phase1/bert_config.json  --local_rank=0  --allreduce_post_accumulation --allreduce_post_accumulation_fp16  --dense_seq_output --pad_fmha --fused_bias_fc --fused_bias_mha --fused_dropout_add --fused_gemm_gelu --packed_samples --cuda_graph_mode 'segmented'   --seed=21546
14: Run vars: id 1339 gpus 2 mparams ''
13: STARTING TIMING RUN AT 2023-07-30 12:22:25 PM
13: WORLD_SIZE=36
19: Run vars: id 1339 gpus 2 mparams ''
28: Run vars: id 1339 gpus 2 mparams ''
11: Run vars: id 1339 gpus 2 mparams ''
14: STARTING TIMING RUN AT 2023-07-30 12:22:25 PM
14: WORLD_SIZE=36
 2: Run vars: id 1339 gpus 2 mparams ''
13: slurm_job_nodelist=node[0400-0401,0403-0418]
13: MASTER_ADDR=node0400
13: HOSTNAME=node0400
13:      python -u     /workspace/bert/run_pretraining.py         --train_batch_size=36     --learning_rate=0.0006     --opt_lamb_beta_1=0.8     --opt_lamb_beta_2=0.996     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=100000     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data     --init_checkpoint=/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt     --max_pack_factor=3          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.1     --max_samples_termination=1000000000     --eval_iter_start_samples=400000 --eval_iter_samples=200000     --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples=10000     --cache_eval_data     --output_dir=/results     --fp16     --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1     --gradient_accumulation_steps=1     --log_freq=0     --bert_config_path=/wor
13: kspace/phase1/bert_config.json  --local_rank=1  --allreduce_post_accumulation --allreduce_post_accumulation_fp16  --dense_seq_output --pad_fmha --fused_bias_fc --fused_bias_mha --fused_dropout_add --fused_gemm_gelu --packed_samples --cuda_graph_mode 'segmented'   --seed=31560
27: Run vars: id 1339 gpus 2 mparams ''
28: STARTING TIMING RUN AT 2023-07-30 12:22:25 PM
28: WORLD_SIZE=36
14: slurm_job_nodelist=node[0400-0401,0403-0418]
14: MASTER_ADDR=node0400
14: HOSTNAME=node0400
14:      python -u     /workspace/bert/run_pretraining.py         --train_batch_size=36     --learning_rate=0.0006     --opt_lamb_beta_1=0.8     --opt_lamb_beta_2=0.996     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=100000     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data     --init_checkpoint=/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt     --max_pack_factor=3          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.1     --max_samples_termination=1000000000     --eval_iter_start_samples=400000 --eval_iter_samples=200000     --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples=10000     --cache_eval_data     --output_dir=/results     --fp16     --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1     --gradient_accumulation_steps=1     --log_freq=0     --bert_config_path=/wor
19: STARTING TIMING RUN AT 2023-07-30 12:22:25 PM
14: kspace/phase1/bert_config.json  --local_rank=0  --allreduce_post_accumulation --allreduce_post_accumulation_fp16  --dense_seq_output --pad_fmha --fused_bias_fc --fused_bias_mha --fused_dropout_add --fused_gemm_gelu --packed_samples --cuda_graph_mode 'segmented'   --seed=21970
19: WORLD_SIZE=36
11: STARTING TIMING RUN AT 2023-07-30 12:22:25 PM
11: WORLD_SIZE=36
 2: STARTING TIMING RUN AT 2023-07-30 12:22:25 PM
 2: WORLD_SIZE=36
28: slurm_job_nodelist=node[0400-0401,0403-0418]
28: MASTER_ADDR=node0400
28: HOSTNAME=node0400
28:      python -u     /workspace/bert/run_pretraining.py         --train_batch_size=36     --learning_rate=0.0006     --opt_lamb_beta_1=0.8     --opt_lamb_beta_2=0.996     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=100000     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data     --init_checkpoint=/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt     --max_pack_factor=3          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.1     --max_samples_termination=1000000000     --eval_iter_start_samples=400000 --eval_iter_samples=200000     --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples=10000     --cache_eval_data     --output_dir=/results     --fp16     --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1     --gradient_accumulation_steps=1     --log_freq=0     --bert_config_path=/wor
27: STARTING TIMING RUN AT 2023-07-30 12:22:25 PM
28: kspace/phase1/bert_config.json  --local_rank=0  --allreduce_post_accumulation --allreduce_post_accumulation_fp16  --dense_seq_output --pad_fmha --fused_bias_fc --fused_bias_mha --fused_dropout_add --fused_gemm_gelu --packed_samples --cuda_graph_mode 'segmented'   --seed=19352
27: WORLD_SIZE=36
30: Run vars: id 1339 gpus 2 mparams ''
11: slurm_job_nodelist=node[0400-0401,0403-0418]
11: MASTER_ADDR=node0400
11: HOSTNAME=node0400
19: slurm_job_nodelist=node[0400-0401,0403-0418]
19: MASTER_ADDR=node0400
19: HOSTNAME=node0400
19:      python -u     /workspace/bert/run_pretraining.py         --train_batch_size=36     --learning_rate=0.0006     --opt_lamb_beta_1=0.8     --opt_lamb_beta_2=0.996     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=100000     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data     --init_checkpoint=/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt     --max_pack_factor=3          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.1     --max_samples_termination=1000000000     --eval_iter_start_samples=400000 --eval_iter_samples=200000     --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples=10000     --cache_eval_data     --output_dir=/results     --fp16     --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1     --gradient_accumulation_steps=1     --log_freq=0     --bert_config_path=/wor
19: kspace/phase1/bert_config.json  --local_rank=1  --allreduce_post_accumulation --allreduce_post_accumulation_fp16  --dense_seq_output --pad_fmha --fused_bias_fc --fused_bias_mha --fused_dropout_add --fused_gemm_gelu --packed_samples --cuda_graph_mode 'segmented'   --seed=8142
11:      python -u     /workspace/bert/run_pretraining.py         --train_batch_size=36     --learning_rate=0.0006     --opt_lamb_beta_1=0.8     --opt_lamb_beta_2=0.996     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=100000     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data     --init_checkpoint=/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt     --max_pack_factor=3          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.1     --max_samples_termination=1000000000     --eval_iter_start_samples=400000 --eval_iter_samples=200000     --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples=10000     --cache_eval_data     --output_dir=/results     --fp16     --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1     --gradient_accumulation_steps=1     --log_freq=0     --bert_config_path=/wor
11: kspace/phase1/bert_config.json  --local_rank=1  --allreduce_post_accumulation --allreduce_post_accumulation_fp16  --dense_seq_output --pad_fmha --fused_bias_fc --fused_bias_mha --fused_dropout_add --fused_gemm_gelu --packed_samples --cuda_graph_mode 'segmented'   --seed=1160
 2: slurm_job_nodelist=node[0400-0401,0403-0418]
 2: MASTER_ADDR=node0400
 2: HOSTNAME=node0400
 2:      python -u     /workspace/bert/run_pretraining.py         --train_batch_size=36     --learning_rate=0.0006     --opt_lamb_beta_1=0.8     --opt_lamb_beta_2=0.996     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=100000     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data     --init_checkpoint=/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt     --max_pack_factor=3          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.1     --max_samples_termination=1000000000     --eval_iter_start_samples=400000 --eval_iter_samples=200000     --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples=10000     --cache_eval_data     --output_dir=/results     --fp16     --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1     --gradient_accumulation_steps=1     --log_freq=0     --bert_config_path=/wor
 2: kspace/phase1/bert_config.json  --local_rank=0  --allreduce_post_accumulation --allreduce_post_accumulation_fp16  --dense_seq_output --pad_fmha --fused_bias_fc --fused_bias_mha --fused_dropout_add --fused_gemm_gelu --packed_samples --cuda_graph_mode 'segmented'   --seed=5023
27: slurm_job_nodelist=node[0400-0401,0403-0418]
27: MASTER_ADDR=node0400
27: HOSTNAME=node0400
27:      python -u     /workspace/bert/run_pretraining.py         --train_batch_size=36     --learning_rate=0.0006     --opt_lamb_beta_1=0.8     --opt_lamb_beta_2=0.996     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=100000     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data     --init_checkpoint=/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt     --max_pack_factor=3          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.1     --max_samples_termination=1000000000     --eval_iter_start_samples=400000 --eval_iter_samples=200000     --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples=10000     --cache_eval_data     --output_dir=/results     --fp16     --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1     --gradient_accumulation_steps=1     --log_freq=0     --bert_config_path=/wor
27: kspace/phase1/bert_config.json  --local_rank=1  --allreduce_post_accumulation --allreduce_post_accumulation_fp16  --dense_seq_output --pad_fmha --fused_bias_fc --fused_bias_mha --fused_dropout_add --fused_gemm_gelu --packed_samples --cuda_graph_mode 'segmented'   --seed=8691
21: Run vars: id 1339 gpus 2 mparams ''
30: STARTING TIMING RUN AT 2023-07-30 12:22:25 PM
30: WORLD_SIZE=36
21: STARTING TIMING RUN AT 2023-07-30 12:22:25 PM
21: WORLD_SIZE=36
30: slurm_job_nodelist=node[0400-0401,0403-0418]
30: MASTER_ADDR=node0400
30: HOSTNAME=node0400
30:      python -u     /workspace/bert/run_pretraining.py         --train_batch_size=36     --learning_rate=0.0006     --opt_lamb_beta_1=0.8     --opt_lamb_beta_2=0.996     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=100000     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data     --init_checkpoint=/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt     --max_pack_factor=3          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.1     --max_samples_termination=1000000000     --eval_iter_start_samples=400000 --eval_iter_samples=200000     --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples=10000     --cache_eval_data     --output_dir=/results     --fp16     --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1     --gradient_accumulation_steps=1     --log_freq=0     --bert_config_path=/wor
30: kspace/phase1/bert_config.json  --local_rank=0  --allreduce_post_accumulation --allreduce_post_accumulation_fp16  --dense_seq_output --pad_fmha --fused_bias_fc --fused_bias_mha --fused_dropout_add --fused_gemm_gelu --packed_samples --cuda_graph_mode 'segmented'   --seed=16923
21: slurm_job_nodelist=node[0400-0401,0403-0418]
21: MASTER_ADDR=node0400
21: HOSTNAME=node0400
21:      python -u     /workspace/bert/run_pretraining.py         --train_batch_size=36     --learning_rate=0.0006     --opt_lamb_beta_1=0.8     --opt_lamb_beta_2=0.996     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=100000     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data     --init_checkpoint=/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt     --max_pack_factor=3          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.1     --max_samples_termination=1000000000     --eval_iter_start_samples=400000 --eval_iter_samples=200000     --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples=10000     --cache_eval_data     --output_dir=/results     --fp16     --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1     --gradient_accumulation_steps=1     --log_freq=0     --bert_config_path=/wor
21: kspace/phase1/bert_config.json  --local_rank=1  --allreduce_post_accumulation --allreduce_post_accumulation_fp16  --dense_seq_output --pad_fmha --fused_bias_fc --fused_bias_mha --fused_dropout_add --fused_gemm_gelu --packed_samples --cuda_graph_mode 'segmented'   --seed=85
17: Run vars: id 1339 gpus 2 mparams ''
 6: Run vars: id 1339 gpus 2 mparams ''
25: Run vars: id 1339 gpus 2 mparams ''
35: Run vars: id 1339 gpus 2 mparams ''
 4: Run vars: id 1339 gpus 2 mparams ''
17: STARTING TIMING RUN AT 2023-07-30 12:22:25 PM
17: WORLD_SIZE=36
22: Run vars: id 1339 gpus 2 mparams ''
 9: Run vars: id 1339 gpus 2 mparams ''
 6: STARTING TIMING RUN AT 2023-07-30 12:22:25 PM
 6: WORLD_SIZE=36
25: STARTING TIMING RUN AT 2023-07-30 12:22:25 PM
35: STARTING TIMING RUN AT 2023-07-30 12:22:25 PM
35: WORLD_SIZE=36
25: WORLD_SIZE=36
17: slurm_job_nodelist=node[0400-0401,0403-0418]
17: MASTER_ADDR=node0400
17: HOSTNAME=node0400
17:      python -u     /workspace/bert/run_pretraining.py         --train_batch_size=36     --learning_rate=0.0006     --opt_lamb_beta_1=0.8     --opt_lamb_beta_2=0.996     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=100000     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data     --init_checkpoint=/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt     --max_pack_factor=3          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.1     --max_samples_termination=1000000000     --eval_iter_start_samples=400000 --eval_iter_samples=200000     --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples=10000     --cache_eval_data     --output_dir=/results     --fp16     --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1     --gradient_accumulation_steps=1     --log_freq=0     --bert_config_path=/wor
17: kspace/phase1/bert_config.json  --local_rank=1  --allreduce_post_accumulation --allreduce_post_accumulation_fp16  --dense_seq_output --pad_fmha --fused_bias_fc --fused_bias_mha --fused_dropout_add --fused_gemm_gelu --packed_samples --cuda_graph_mode 'segmented'   --seed=19664
 6: slurm_job_nodelist=node[0400-0401,0403-0418]
 6: MASTER_ADDR=node0400
 6: HOSTNAME=node0400
 6:      python -u     /workspace/bert/run_pretraining.py         --train_batch_size=36     --learning_rate=0.0006     --opt_lamb_beta_1=0.8     --opt_lamb_beta_2=0.996     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=100000     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data     --init_checkpoint=/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt     --max_pack_factor=3          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.1     --max_samples_termination=1000000000     --eval_iter_start_samples=400000 --eval_iter_samples=200000     --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples=10000     --cache_eval_data     --output_dir=/results     --fp16     --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1     --gradient_accumulation_steps=1     --log_freq=0     --bert_config_path=/wor
 6: kspace/phase1/bert_config.json  --local_rank=0  --allreduce_post_accumulation --allreduce_post_accumulation_fp16  --dense_seq_output --pad_fmha --fused_bias_fc --fused_bias_mha --fused_dropout_add --fused_gemm_gelu --packed_samples --cuda_graph_mode 'segmented'   --seed=31309
 4: STARTING TIMING RUN AT 2023-07-30 12:22:25 PM
 4: WORLD_SIZE=36
25: slurm_job_nodelist=node[0400-0401,0403-0418]
25: MASTER_ADDR=node0400
25: HOSTNAME=node0400
22: STARTING TIMING RUN AT 2023-07-30 12:22:25 PM
25:      python -u     /workspace/bert/run_pretraining.py         --train_batch_size=36     --learning_rate=0.0006     --opt_lamb_beta_1=0.8     --opt_lamb_beta_2=0.996     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=100000     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data     --init_checkpoint=/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt     --max_pack_factor=3          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.1     --max_samples_termination=1000000000     --eval_iter_start_samples=400000 --eval_iter_samples=200000     --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples=10000     --cache_eval_data     --output_dir=/results     --fp16     --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1     --gradient_accumulation_steps=1     --log_freq=0     --bert_config_path=/wor
22: WORLD_SIZE=36
35: slurm_job_nodelist=node[0400-0401,0403-0418]
35: MASTER_ADDR=node0400
35: HOSTNAME=node0400
25: kspace/phase1/bert_config.json  --local_rank=1  --allreduce_post_accumulation --allreduce_post_accumulation_fp16  --dense_seq_output --pad_fmha --fused_bias_fc --fused_bias_mha --fused_dropout_add --fused_gemm_gelu --packed_samples --cuda_graph_mode 'segmented'   --seed=18329
35:      python -u     /workspace/bert/run_pretraining.py         --train_batch_size=36     --learning_rate=0.0006     --opt_lamb_beta_1=0.8     --opt_lamb_beta_2=0.996     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=100000     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data     --init_checkpoint=/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt     --max_pack_factor=3          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.1     --max_samples_termination=1000000000     --eval_iter_start_samples=400000 --eval_iter_samples=200000     --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples=10000     --cache_eval_data     --output_dir=/results     --fp16     --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1     --gradient_accumulation_steps=1     --log_freq=0     --bert_config_path=/wor
35: kspace/phase1/bert_config.json  --local_rank=1  --allreduce_post_accumulation --allreduce_post_accumulation_fp16  --dense_seq_output --pad_fmha --fused_bias_fc --fused_bias_mha --fused_dropout_add --fused_gemm_gelu --packed_samples --cuda_graph_mode 'segmented'   --seed=9898
 9: STARTING TIMING RUN AT 2023-07-30 12:22:25 PM
 9: WORLD_SIZE=36
 4: slurm_job_nodelist=node[0400-0401,0403-0418]
 4: MASTER_ADDR=node0400
 4: HOSTNAME=node0400
 4:      python -u     /workspace/bert/run_pretraining.py         --train_batch_size=36     --learning_rate=0.0006     --opt_lamb_beta_1=0.8     --opt_lamb_beta_2=0.996     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=100000     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data     --init_checkpoint=/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt     --max_pack_factor=3          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.1     --max_samples_termination=1000000000     --eval_iter_start_samples=400000 --eval_iter_samples=200000     --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples=10000     --cache_eval_data     --output_dir=/results     --fp16     --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1     --gradient_accumulation_steps=1     --log_freq=0     --bert_config_path=/wor
 4: kspace/phase1/bert_config.json  --local_rank=0  --allreduce_post_accumulation --allreduce_post_accumulation_fp16  --dense_seq_output --pad_fmha --fused_bias_fc --fused_bias_mha --fused_dropout_add --fused_gemm_gelu --packed_samples --cuda_graph_mode 'segmented'   --seed=29186
22: slurm_job_nodelist=node[0400-0401,0403-0418]
22: MASTER_ADDR=node0400
22: HOSTNAME=node0400
22:      python -u     /workspace/bert/run_pretraining.py         --train_batch_size=36     --learning_rate=0.0006     --opt_lamb_beta_1=0.8     --opt_lamb_beta_2=0.996     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=100000     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data     --init_checkpoint=/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt     --max_pack_factor=3          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.1     --max_samples_termination=1000000000     --eval_iter_start_samples=400000 --eval_iter_samples=200000     --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples=10000     --cache_eval_data     --output_dir=/results     --fp16     --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1     --gradient_accumulation_steps=1     --log_freq=0     --bert_config_path=/wor
22: kspace/phase1/bert_config.json  --local_rank=0  --allreduce_post_accumulation --allreduce_post_accumulation_fp16  --dense_seq_output --pad_fmha --fused_bias_fc --fused_bias_mha --fused_dropout_add --fused_gemm_gelu --packed_samples --cuda_graph_mode 'segmented'   --seed=406
 9: slurm_job_nodelist=node[0400-0401,0403-0418]
 9: MASTER_ADDR=node0400
 9: HOSTNAME=node0400
 9:      python -u     /workspace/bert/run_pretraining.py         --train_batch_size=36     --learning_rate=0.0006     --opt_lamb_beta_1=0.8     --opt_lamb_beta_2=0.996     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=100000     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data     --init_checkpoint=/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt     --max_pack_factor=3          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.1     --max_samples_termination=1000000000     --eval_iter_start_samples=400000 --eval_iter_samples=200000     --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples=10000     --cache_eval_data     --output_dir=/results     --fp16     --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1     --gradient_accumulation_steps=1     --log_freq=0     --bert_config_path=/wor
 9: kspace/phase1/bert_config.json  --local_rank=1  --allreduce_post_accumulation --allreduce_post_accumulation_fp16  --dense_seq_output --pad_fmha --fused_bias_fc --fused_bias_mha --fused_dropout_add --fused_gemm_gelu --packed_samples --cuda_graph_mode 'segmented'   --seed=11533
33: Run vars: id 1339 gpus 2 mparams ''
15: Run vars: id 1339 gpus 2 mparams ''
33: STARTING TIMING RUN AT 2023-07-30 12:22:25 PM
12: Run vars: id 1339 gpus 2 mparams ''
33: WORLD_SIZE=36
33: slurm_job_nodelist=node[0400-0401,0403-0418]
33: MASTER_ADDR=node0400
33: HOSTNAME=node0400
33:      python -u     /workspace/bert/run_pretraining.py         --train_batch_size=36     --learning_rate=0.0006     --opt_lamb_beta_1=0.8     --opt_lamb_beta_2=0.996     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=100000     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data     --init_checkpoint=/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt     --max_pack_factor=3          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.1     --max_samples_termination=1000000000     --eval_iter_start_samples=400000 --eval_iter_samples=200000     --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples=10000     --cache_eval_data     --output_dir=/results     --fp16     --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1     --gradient_accumulation_steps=1     --log_freq=0     --bert_config_path=/wor
33: kspace/phase1/bert_config.json  --local_rank=1  --allreduce_post_accumulation --allreduce_post_accumulation_fp16  --dense_seq_output --pad_fmha --fused_bias_fc --fused_bias_mha --fused_dropout_add --fused_gemm_gelu --packed_samples --cuda_graph_mode 'segmented'   --seed=21159
15: STARTING TIMING RUN AT 2023-07-30 12:22:25 PM
15: WORLD_SIZE=36
12: STARTING TIMING RUN AT 2023-07-30 12:22:25 PM
12: WORLD_SIZE=36
26: Run vars: id 1339 gpus 2 mparams ''
29: Run vars: id 1339 gpus 2 mparams ''
15: slurm_job_nodelist=node[0400-0401,0403-0418]
15: MASTER_ADDR=node0400
15: HOSTNAME=node0400
15:      python -u     /workspace/bert/run_pretraining.py         --train_batch_size=36     --learning_rate=0.0006     --opt_lamb_beta_1=0.8     --opt_lamb_beta_2=0.996     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=100000     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data     --init_checkpoint=/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt     --max_pack_factor=3          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.1     --max_samples_termination=1000000000     --eval_iter_start_samples=400000 --eval_iter_samples=200000     --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples=10000     --cache_eval_data     --output_dir=/results     --fp16     --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1     --gradient_accumulation_steps=1     --log_freq=0     --bert_config_path=/wor
15: kspace/phase1/bert_config.json  --local_rank=1  --allreduce_post_accumulation --allreduce_post_accumulation_fp16  --dense_seq_output --pad_fmha --fused_bias_fc --fused_bias_mha --fused_dropout_add --fused_gemm_gelu --packed_samples --cuda_graph_mode 'segmented'   --seed=12283
12: slurm_job_nodelist=node[0400-0401,0403-0418]
12: MASTER_ADDR=node0400
12: HOSTNAME=node0400
12:      python -u     /workspace/bert/run_pretraining.py         --train_batch_size=36     --learning_rate=0.0006     --opt_lamb_beta_1=0.8     --opt_lamb_beta_2=0.996     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=100000     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data     --init_checkpoint=/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt     --max_pack_factor=3          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.1     --max_samples_termination=1000000000     --eval_iter_start_samples=400000 --eval_iter_samples=200000     --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples=10000     --cache_eval_data     --output_dir=/results     --fp16     --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1     --gradient_accumulation_steps=1     --log_freq=0     --bert_config_path=/wor
12: kspace/phase1/bert_config.json  --local_rank=0  --allreduce_post_accumulation --allreduce_post_accumulation_fp16  --dense_seq_output --pad_fmha --fused_bias_fc --fused_bias_mha --fused_dropout_add --fused_gemm_gelu --packed_samples --cuda_graph_mode 'segmented'   --seed=12662
 3: Run vars: id 1339 gpus 2 mparams ''
31: Run vars: id 1339 gpus 2 mparams ''
29: STARTING TIMING RUN AT 2023-07-30 12:22:25 PM
29: WORLD_SIZE=36
26: STARTING TIMING RUN AT 2023-07-30 12:22:25 PM
26: WORLD_SIZE=36
29: slurm_job_nodelist=node[0400-0401,0403-0418]
29: MASTER_ADDR=node0400
29: HOSTNAME=node0400
 3: STARTING TIMING RUN AT 2023-07-30 12:22:25 PM
29:      python -u     /workspace/bert/run_pretraining.py         --train_batch_size=36     --learning_rate=0.0006     --opt_lamb_beta_1=0.8     --opt_lamb_beta_2=0.996     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=100000     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data     --init_checkpoint=/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt     --max_pack_factor=3          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.1     --max_samples_termination=1000000000     --eval_iter_start_samples=400000 --eval_iter_samples=200000     --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples=10000     --cache_eval_data     --output_dir=/results     --fp16     --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1     --gradient_accumulation_steps=1     --log_freq=0     --bert_config_path=/wor
29: kspace/phase1/bert_config.json  --local_rank=1  --allreduce_post_accumulation --allreduce_post_accumulation_fp16  --dense_seq_output --pad_fmha --fused_bias_fc --fused_bias_mha --fused_dropout_add --fused_gemm_gelu --packed_samples --cuda_graph_mode 'segmented'   --seed=27775
 3: WORLD_SIZE=36
26: slurm_job_nodelist=node[0400-0401,0403-0418]
26: MASTER_ADDR=node0400
26: HOSTNAME=node0400
26:      python -u     /workspace/bert/run_pretraining.py         --train_batch_size=36     --learning_rate=0.0006     --opt_lamb_beta_1=0.8     --opt_lamb_beta_2=0.996     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=100000     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data     --init_checkpoint=/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt     --max_pack_factor=3          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.1     --max_samples_termination=1000000000     --eval_iter_start_samples=400000 --eval_iter_samples=200000     --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples=10000     --cache_eval_data     --output_dir=/results     --fp16     --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1     --gradient_accumulation_steps=1     --log_freq=0     --bert_config_path=/wor
26: kspace/phase1/bert_config.json  --local_rank=0  --allreduce_post_accumulation --allreduce_post_accumulation_fp16  --dense_seq_output --pad_fmha --fused_bias_fc --fused_bias_mha --fused_dropout_add --fused_gemm_gelu --packed_samples --cuda_graph_mode 'segmented'   --seed=24155
31: STARTING TIMING RUN AT 2023-07-30 12:22:25 PM
31: WORLD_SIZE=36
 3: slurm_job_nodelist=node[0400-0401,0403-0418]
 3: MASTER_ADDR=node0400
 3: HOSTNAME=node0400
 3:      python -u     /workspace/bert/run_pretraining.py         --train_batch_size=36     --learning_rate=0.0006     --opt_lamb_beta_1=0.8     --opt_lamb_beta_2=0.996     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=100000     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data     --init_checkpoint=/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt     --max_pack_factor=3          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.1     --max_samples_termination=1000000000     --eval_iter_start_samples=400000 --eval_iter_samples=200000     --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples=10000     --cache_eval_data     --output_dir=/results     --fp16     --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1     --gradient_accumulation_steps=1     --log_freq=0     --bert_config_path=/wor
 3: kspace/phase1/bert_config.json  --local_rank=1  --allreduce_post_accumulation --allreduce_post_accumulation_fp16  --dense_seq_output --pad_fmha --fused_bias_fc --fused_bias_mha --fused_dropout_add --fused_gemm_gelu --packed_samples --cuda_graph_mode 'segmented'   --seed=21425
31: slurm_job_nodelist=node[0400-0401,0403-0418]
31: MASTER_ADDR=node0400
31: HOSTNAME=node0400
31:      python -u     /workspace/bert/run_pretraining.py         --train_batch_size=36     --learning_rate=0.0006     --opt_lamb_beta_1=0.8     --opt_lamb_beta_2=0.996     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=100000     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data     --init_checkpoint=/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt     --max_pack_factor=3          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.1     --max_samples_termination=1000000000     --eval_iter_start_samples=400000 --eval_iter_samples=200000     --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples=10000     --cache_eval_data     --output_dir=/results     --fp16     --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1     --gradient_accumulation_steps=1     --log_freq=0     --bert_config_path=/wor
31: kspace/phase1/bert_config.json  --local_rank=1  --allreduce_post_accumulation --allreduce_post_accumulation_fp16  --dense_seq_output --pad_fmha --fused_bias_fc --fused_bias_mha --fused_dropout_add --fused_gemm_gelu --packed_samples --cuda_graph_mode 'segmented'   --seed=32336
 1: /usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 1:   warnings.warn(msg, DeprecatedFeatureWarning)
 0: /usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 0:   warnings.warn(msg, DeprecatedFeatureWarning)
10: /usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
10:   warnings.warn(msg, DeprecatedFeatureWarning)
11: /usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
11:   warnings.warn(msg, DeprecatedFeatureWarning)
21: /usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
21:   warnings.warn(msg, DeprecatedFeatureWarning)
20: /usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
20:   warnings.warn(msg, DeprecatedFeatureWarning)
17: /usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
17:   warnings.warn(msg, DeprecatedFeatureWarning)
16: /usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
16:   warnings.warn(msg, DeprecatedFeatureWarning)
 6: /usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 6:   warnings.warn(msg, DeprecatedFeatureWarning)
 7: /usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 7:   warnings.warn(msg, DeprecatedFeatureWarning)
 4: /usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 4:   warnings.warn(msg, DeprecatedFeatureWarning)
 5: /usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 5:   warnings.warn(msg, DeprecatedFeatureWarning)
22: /usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
22:   warnings.warn(msg, DeprecatedFeatureWarning)
23: /usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
23:   warnings.warn(msg, DeprecatedFeatureWarning)
35: /usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
35:   warnings.warn(msg, DeprecatedFeatureWarning)
34: /usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
34:   warnings.warn(msg, DeprecatedFeatureWarning)
18: /usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
18:   warnings.warn(msg, DeprecatedFeatureWarning)
19: /usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
19:   warnings.warn(msg, DeprecatedFeatureWarning)
32: /usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
32:   warnings.warn(msg, DeprecatedFeatureWarning)
33: /usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
33:   warnings.warn(msg, DeprecatedFeatureWarning)
12: /usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
12:   warnings.warn(msg, DeprecatedFeatureWarning)
13: /usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
13:   warnings.warn(msg, DeprecatedFeatureWarning)
25: /usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
25:   warnings.warn(msg, DeprecatedFeatureWarning)
24: /usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
24:   warnings.warn(msg, DeprecatedFeatureWarning)
29: /usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
29:   warnings.warn(msg, DeprecatedFeatureWarning)
28: /usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
28:   warnings.warn(msg, DeprecatedFeatureWarning)
14: /usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
14:   warnings.warn(msg, DeprecatedFeatureWarning)
15: /usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
15:   warnings.warn(msg, DeprecatedFeatureWarning)
 9: /usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 9:   warnings.warn(msg, DeprecatedFeatureWarning)
 8: /usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 8:   warnings.warn(msg, DeprecatedFeatureWarning)
31: /usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
31:   warnings.warn(msg, DeprecatedFeatureWarning)
30: /usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
30:   warnings.warn(msg, DeprecatedFeatureWarning)
 2: /usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 2:   warnings.warn(msg, DeprecatedFeatureWarning)
 3: /usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 3:   warnings.warn(msg, DeprecatedFeatureWarning)
27: /usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
27:   warnings.warn(msg, DeprecatedFeatureWarning)
26: /usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
26:   warnings.warn(msg, DeprecatedFeatureWarning)
21: :::MLLOG {"namespace": "", "time_ms": 1690734151476, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1263}}
20: :::MLLOG {"namespace": "", "time_ms": 1690734151476, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1263}}
 1: :::MLLOG {"namespace": "", "time_ms": 1690734151482, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1263}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734151482, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1263}}
11: :::MLLOG {"namespace": "", "time_ms": 1690734151500, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1263}}
10: :::MLLOG {"namespace": "", "time_ms": 1690734151500, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1263}}
 6: :::MLLOG {"namespace": "", "time_ms": 1690734151526, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1263}}
 7: :::MLLOG {"namespace": "", "time_ms": 1690734151528, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1263}}
17: :::MLLOG {"namespace": "", "time_ms": 1690734151524, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1263}}
16: :::MLLOG {"namespace": "", "time_ms": 1690734151525, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1263}}
 4: :::MLLOG {"namespace": "", "time_ms": 1690734151659, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1263}}
35: :::MLLOG {"namespace": "", "time_ms": 1690734151649, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1263}}
34: :::MLLOG {"namespace": "", "time_ms": 1690734151653, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1263}}
 5: :::MLLOG {"namespace": "", "time_ms": 1690734151659, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1263}}
24: :::MLLOG {"namespace": "", "time_ms": 1690734151672, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1263}}
22: :::MLLOG {"namespace": "", "time_ms": 1690734151683, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1263}}
23: :::MLLOG {"namespace": "", "time_ms": 1690734151683, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1263}}
25: :::MLLOG {"namespace": "", "time_ms": 1690734151673, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1263}}
18: :::MLLOG {"namespace": "", "time_ms": 1690734151688, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1263}}
19: :::MLLOG {"namespace": "", "time_ms": 1690734151688, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1263}}
12: :::MLLOG {"namespace": "", "time_ms": 1690734151710, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1263}}
13: :::MLLOG {"namespace": "", "time_ms": 1690734151710, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1263}}
32: :::MLLOG {"namespace": "", "time_ms": 1690734151722, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1263}}
33: :::MLLOG {"namespace": "", "time_ms": 1690734151722, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1263}}
28: :::MLLOG {"namespace": "", "time_ms": 1690734151792, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1263}}
29: :::MLLOG {"namespace": "", "time_ms": 1690734151791, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1263}}
15: :::MLLOG {"namespace": "", "time_ms": 1690734151805, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1263}}
14: :::MLLOG {"namespace": "", "time_ms": 1690734151805, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1263}}
 9: :::MLLOG {"namespace": "", "time_ms": 1690734151956, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1263}}
 8: :::MLLOG {"namespace": "", "time_ms": 1690734151956, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1263}}
30: :::MLLOG {"namespace": "", "time_ms": 1690734152005, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1263}}
31: :::MLLOG {"namespace": "", "time_ms": 1690734152006, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1263}}
 3: :::MLLOG {"namespace": "", "time_ms": 1690734152020, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1263}}
 2: :::MLLOG {"namespace": "", "time_ms": 1690734152022, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1263}}
26: :::MLLOG {"namespace": "", "time_ms": 1690734152033, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1263}}
27: :::MLLOG {"namespace": "", "time_ms": 1690734152033, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1263}}
 0: device: cuda:0 n_gpu: 36, distributed training: True, 16-bits training: True
 0: :::MLLOG {"namespace": "", "time_ms": 1690734152901, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "bert", "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734152901, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "Clemson Research Computing and Data", "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 1: device: cuda:1 n_gpu: 36, distributed training: True, 16-bits training: True
 0: :::MLLOG {"namespace": "", "time_ms": 1690734152902, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734152902, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734152902, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "18x1xR750xax2A100-80GB-PCIe", "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734152902, "event_type": "POINT_IN_TIME", "key": "seed", "value": 10141, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1272}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734152902, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 2592, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1274}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734152902, "event_type": "POINT_IN_TIME", "key": "d_batch_size", "value": 36, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1276}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734152902, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1278}}
27: device: cuda:1 n_gpu: 36, distributed training: True, 16-bits training: True
 0: :::MLLOG {"namespace": "", "time_ms": 1690734152902, "event_type": "POINT_IN_TIME", "key": "max_predictions_per_seq", "value": 76, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1280}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734152902, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_training_steps", "value": 100000.0, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1282}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734152902, "event_type": "POINT_IN_TIME", "key": "num_warmup_steps", "value": 0, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1284}}
17: device: cuda:1 n_gpu: 36, distributed training: True, 16-bits training: True
 0: parsed args:
 0: Namespace(allreduce_post_accumulation=True, allreduce_post_accumulation_fp16=True, average_packing_rate=2, bert_config_path='/workspace/phase1/bert_config.json', bert_model='bert-large-uncased', bypass_amp=False, cache_eval_data=True, checkpoint_activations=False, cuda_graph_mode='segmented', ddp_type='apex', dense_seq_output=True, device=device(type='cuda', index=0), disable_apex_softmax=False, disable_fuse_mask=False, disable_fuse_qkv=False, disable_fuse_scale=False, distributed_lamb=True, do_train=True, dwu_e5m2_allgather=False, dwu_group_size=0, dwu_num_ag_pg=1, dwu_num_ar_pg=1, dwu_num_blocks=1, dwu_num_chunks=1, dwu_num_rs_pg=1, dwu_overlap_reductions=False, enable_fuse_dropout=False, enable_stream=False, eval_batch_size=16, eval_dir='/workspace/evaldata', eval_iter_samples=200000, eval_iter_start_samples=400000, exchange_padding=False, fp16=True, fused_bias_fc=True, fused_bias_fc_loss_head=False, fused_bias_mha=True, fused_dropout_add=True, fused_gelu_bias=False, fused_gemm_gelu=True, fused_mha=False, 
 0: gradient_accumulation_steps=1, init_checkpoint='/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt', init_tf_checkpoint=None, input_dir='/workspace/data', keep_n_most_recent_checkpoints=20, learning_rate=0.0006, local_rank=0, log_freq=0.0, loss_scale=0.0, max_iterations_per_graph=4, max_pack_factor=3, max_predictions_per_seq=76, max_samples_termination=1000000000.0, max_seq_length=512, max_steps=100000.0, min_samples_to_start_checkpoints=3000000, n_gpu=36, num_epochs_to_generate_seeds_for=2, num_eval_examples=10000, num_samples_per_checkpoint=500000, opt_lamb_beta_1=0.8, opt_lamb_beta_2=0.996, order_samples=False, output_dir='/results', packed_samples=True, pad=False, pad_fmha=True, phase2=True, resume_from_checkpoint=False, seed=10141, skip_checkpoint=True, start_warmup_step=0.0, synthetic_input=False, target_mlm_accuracy=0.72, train_batch_size=36, train_mlm_accuracy_window_size=0, unpad=False, unpad_fmha=False, use_cuda_graph=False, use_ddp=False, use_env=False, use_gradient_as_bucket_view=False, use_transforme
 0: r_engine2=False, warmup_proportion=0.0, warmup_steps=0.0, weight_decay_rate=0.1)
14: device: cuda:0 n_gpu: 36, distributed training: True, 16-bits training: True
32: device: cuda:0 n_gpu: 36, distributed training: True, 16-bits training: True
33: device: cuda:1 n_gpu: 36, distributed training: True, 16-bits training: True
16: device: cuda:0 n_gpu: 36, distributed training: True, 16-bits training: True
19: device: cuda:1 n_gpu: 36, distributed training: True, 16-bits training: True
24: device: cuda:0 n_gpu: 36, distributed training: True, 16-bits training: True
10: device: cuda:0 n_gpu: 36, distributed training: True, 16-bits training: True
22: device: cuda:0 n_gpu: 36, distributed training: True, 16-bits training: True
29: device: cuda:1 n_gpu: 36, distributed training: True, 16-bits training: True
12: device: cuda:0 n_gpu: 36, distributed training: True, 16-bits training: True
28: device: cuda:0 n_gpu: 36, distributed training: True, 16-bits training: True
13: device: cuda:1 n_gpu: 36, distributed training: True, 16-bits training: True
 7: device: cuda:1 n_gpu: 36, distributed training: True, 16-bits training: True
35: device: cuda:1 n_gpu: 36, distributed training: True, 16-bits training: True
25: device: cuda:1 n_gpu: 36, distributed training: True, 16-bits training: True
 6: device: cuda:0 n_gpu: 36, distributed training: True, 16-bits training: True
30: device: cuda:0 n_gpu: 36, distributed training: True, 16-bits training: True
31: device: cuda:1 n_gpu: 36, distributed training: True, 16-bits training: True
11: device: cuda:1 n_gpu: 36, distributed training: True, 16-bits training: True
 8: device: cuda:0 n_gpu: 36, distributed training: True, 16-bits training: True
20: device: cuda:0 n_gpu: 36, distributed training: True, 16-bits training: True
21: device: cuda:1 n_gpu: 36, distributed training: True, 16-bits training: True
15: device: cuda:1 n_gpu: 36, distributed training: True, 16-bits training: True
23: device: cuda:1 n_gpu: 36, distributed training: True, 16-bits training: True
34: device: cuda:0 n_gpu: 36, distributed training: True, 16-bits training: True
 9: device: cuda:1 n_gpu: 36, distributed training: True, 16-bits training: True
 4: device: cuda:0 n_gpu: 36, distributed training: True, 16-bits training: True
 2: device: cuda:0 n_gpu: 36, distributed training: True, 16-bits training: True
 5: device: cuda:1 n_gpu: 36, distributed training: True, 16-bits training: True
26: device: cuda:0 n_gpu: 36, distributed training: True, 16-bits training: True
18: device: cuda:0 n_gpu: 36, distributed training: True, 16-bits training: True
 3: device: cuda:1 n_gpu: 36, distributed training: True, 16-bits training: True
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156084, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/embeddings/word_embeddings"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156084, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/embeddings/position_embeddings"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156084, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/embeddings/token_type_embeddings"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156084, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/embeddings/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156084, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/embeddings/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156084, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_0/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156084, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_0/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156084, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_0/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156084, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_0/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156084, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_0/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156084, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_0/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156084, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_0/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156085, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_0/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156085, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_0/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156085, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_0/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156085, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_0/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156085, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_0/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156085, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_0/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156085, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_0/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156085, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_0/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156085, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_0/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156085, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_1/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156085, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_1/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156085, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_1/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156085, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_1/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156085, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_1/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156085, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_1/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156085, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_1/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156085, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_1/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156085, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_1/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156086, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_1/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156086, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_1/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156086, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_1/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156086, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_1/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156086, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_1/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156086, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_1/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156086, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_1/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156086, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_2/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156086, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_2/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156086, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_2/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156086, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_2/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156086, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_2/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156086, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_2/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156086, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_2/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156086, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_2/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156086, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_2/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156086, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_2/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156086, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_2/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156086, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_2/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156087, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_2/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156087, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_2/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156087, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_2/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156087, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_2/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156087, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_3/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156087, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_3/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156087, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_3/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156087, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_3/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156087, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_3/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156087, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_3/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156087, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_3/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156087, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_3/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156087, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_3/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156087, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_3/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156087, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_3/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156087, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_3/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156087, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_3/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156087, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_3/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156088, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_3/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156088, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_3/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156088, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_4/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156088, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_4/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156088, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_4/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156088, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_4/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156088, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_4/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156088, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_4/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156088, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_4/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156088, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_4/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156088, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_4/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156088, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_4/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156088, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_4/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156088, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_4/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156088, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_4/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156088, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_4/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156088, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_4/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156088, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_4/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156089, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_5/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156089, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_5/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156089, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_5/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156089, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_5/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156089, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_5/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156089, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_5/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156089, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_5/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156089, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_5/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156089, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_5/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156089, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_5/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156089, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_5/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156089, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_5/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156089, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_5/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156089, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_5/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156089, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_5/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156089, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_5/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156089, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_6/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156089, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_6/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156089, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_6/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156090, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_6/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156090, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_6/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156090, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_6/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156090, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_6/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156090, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_6/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156090, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_6/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156090, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_6/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156090, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_6/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156090, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_6/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156090, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_6/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156090, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_6/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156090, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_6/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156090, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_6/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156090, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_7/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156090, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_7/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156090, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_7/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156090, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_7/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156090, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_7/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156091, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_7/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156091, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_7/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156091, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_7/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156091, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_7/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156091, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_7/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156091, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_7/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156091, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_7/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156091, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_7/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156091, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_7/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156091, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_7/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156091, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_7/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156091, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_8/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156091, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_8/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156091, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_8/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156091, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_8/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156091, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_8/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156091, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_8/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156091, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_8/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156092, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_8/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156092, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_8/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156092, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_8/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156092, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_8/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156092, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_8/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156092, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_8/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156092, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_8/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156092, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_8/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156092, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_8/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156092, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_9/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156092, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_9/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156092, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_9/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156092, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_9/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156092, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_9/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156092, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_9/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156092, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_9/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156092, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_9/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156092, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_9/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156092, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_9/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156093, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_9/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156093, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_9/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156093, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_9/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156093, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_9/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156093, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_9/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156093, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_9/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156093, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_10/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156093, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_10/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156093, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_10/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156093, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_10/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156093, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_10/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156093, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_10/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156093, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_10/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156093, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_10/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156093, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_10/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156093, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_10/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156093, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_10/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156093, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_10/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156094, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_10/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156094, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_10/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156094, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_10/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156094, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_10/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156094, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_11/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156094, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_11/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156094, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_11/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156094, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_11/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156094, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_11/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156094, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_11/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156094, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_11/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156094, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_11/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156094, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_11/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156094, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_11/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156094, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_11/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156094, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_11/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156094, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_11/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156094, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_11/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156095, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_11/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156095, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_11/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156095, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_12/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156095, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_12/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156095, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_12/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156095, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_12/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156095, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_12/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156095, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_12/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156095, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_12/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156095, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_12/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156095, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_12/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156095, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_12/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156095, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_12/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156095, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_12/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156095, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_12/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156095, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_12/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156095, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_12/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156095, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_12/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156096, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_13/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156096, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_13/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156096, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_13/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156096, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_13/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156096, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_13/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156096, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_13/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156096, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_13/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156096, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_13/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156096, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_13/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156096, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_13/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156096, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_13/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156096, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_13/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156096, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_13/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156096, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_13/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156096, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_13/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156096, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_13/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156096, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_14/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156096, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_14/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156096, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_14/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156097, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_14/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156097, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_14/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156097, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_14/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156097, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_14/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156097, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_14/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156097, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_14/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156097, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_14/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156097, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_14/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156097, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_14/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156097, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_14/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156097, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_14/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156097, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_14/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156097, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_14/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156097, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_15/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156097, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_15/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156097, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_15/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156097, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_15/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156097, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_15/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156098, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_15/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156098, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_15/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156098, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_15/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156098, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_15/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156098, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_15/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156098, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_15/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156098, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_15/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156098, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_15/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156098, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_15/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156098, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_15/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156098, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_15/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156098, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_16/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156098, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_16/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156098, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_16/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156098, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_16/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156098, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_16/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156098, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_16/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156098, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_16/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156099, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_16/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156099, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_16/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156099, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_16/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156099, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_16/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156099, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_16/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156099, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_16/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156099, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_16/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156099, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_16/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156099, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_16/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156099, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_17/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156099, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_17/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156099, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_17/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156099, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_17/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156099, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_17/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156099, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_17/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156099, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_17/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156099, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_17/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156099, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_17/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156100, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_17/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156100, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_17/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156100, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_17/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156100, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_17/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156100, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_17/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156100, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_17/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156100, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_17/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156100, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_18/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156100, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_18/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156100, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_18/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156100, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_18/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156100, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_18/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156100, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_18/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156100, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_18/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156100, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_18/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156100, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_18/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156100, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_18/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156100, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_18/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156100, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_18/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156101, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_18/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156101, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_18/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156101, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_18/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156101, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_18/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156101, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_19/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156101, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_19/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156101, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_19/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156101, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_19/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156101, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_19/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156101, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_19/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156101, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_19/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156101, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_19/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156101, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_19/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156101, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_19/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156101, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_19/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156101, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_19/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156101, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_19/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156101, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_19/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156102, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_19/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156102, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_19/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156102, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_20/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156102, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_20/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156102, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_20/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156102, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_20/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156102, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_20/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156102, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_20/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156102, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_20/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156102, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_20/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156102, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_20/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156102, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_20/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156102, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_20/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156102, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_20/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156102, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_20/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156102, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_20/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156102, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_20/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156102, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_20/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156102, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_21/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156103, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_21/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156103, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_21/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156103, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_21/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156103, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_21/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156103, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_21/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156103, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_21/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156103, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_21/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156103, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_21/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156103, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_21/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156103, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_21/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156103, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_21/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156103, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_21/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156103, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_21/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156103, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_21/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156103, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_21/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156103, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_22/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156103, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_22/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156103, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_22/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156104, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_22/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156104, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_22/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156104, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_22/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156104, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_22/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156104, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_22/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156104, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_22/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156104, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_22/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156104, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_22/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156104, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_22/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156104, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_22/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156104, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_22/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156104, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_22/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156104, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_22/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156104, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_23/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156104, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_23/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156104, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_23/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156104, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_23/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156104, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_23/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156105, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_23/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156105, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_23/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156105, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_23/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156105, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_23/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156105, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_23/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156105, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_23/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156105, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_23/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156105, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_23/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156105, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_23/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156105, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_23/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156105, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_23/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156105, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/pooler/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156105, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/pooler/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156105, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "cls/predictions/output_bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156105, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "cls/predictions/transform/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156105, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "cls/predictions/transform/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156105, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "cls/predictions/transform/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156105, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "cls/predictions/transform/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156105, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "cls/seq_relationship/output_weights"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734156106, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "cls/seq_relationship/output_bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734159665, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.0006, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 898}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734162298, "event_type": "POINT_IN_TIME", "key": "opt_epsilon", "value": 1e-06, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 938}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734162299, "event_type": "POINT_IN_TIME", "key": "opt_lamb_beta_1", "value": 0.8, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 940}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734162299, "event_type": "POINT_IN_TIME", "key": "opt_lamb_beta_2", "value": 0.996, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 941}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734162299, "event_type": "POINT_IN_TIME", "key": "opt_lamb_weight_decay_rate", "value": 0.1, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 942}}
17: /workspace/bert/run_pretraining.py:1000: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
17:   param_storage = optimizer._new_params.storage()
10: /workspace/bert/run_pretraining.py:1000: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10:   param_storage = optimizer._new_params.storage()
20: /workspace/bert/run_pretraining.py:1000: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
20:   param_storage = optimizer._new_params.storage()
16: /workspace/bert/run_pretraining.py:1000: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
16:   param_storage = optimizer._new_params.storage()
24: /workspace/bert/run_pretraining.py:1000: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
24:   param_storage = optimizer._new_params.storage()
21: /workspace/bert/run_pretraining.py:1000: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
21:   param_storage = optimizer._new_params.storage()
25: /workspace/bert/run_pretraining.py:1000: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
25:   param_storage = optimizer._new_params.storage()
31: /workspace/bert/run_pretraining.py:1000: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
31:   param_storage = optimizer._new_params.storage()
 8: /workspace/bert/run_pretraining.py:1000: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
 8:   param_storage = optimizer._new_params.storage()
 9: /workspace/bert/run_pretraining.py:1000: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
 9:   param_storage = optimizer._new_params.storage()
14: /workspace/bert/run_pretraining.py:1000: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
14:   param_storage = optimizer._new_params.storage()
15: /workspace/bert/run_pretraining.py:1000: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
15:   param_storage = optimizer._new_params.storage()
29: /workspace/bert/run_pretraining.py:1000: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
29:   param_storage = optimizer._new_params.storage()
 3: /workspace/bert/run_pretraining.py:1000: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
 3:   param_storage = optimizer._new_params.storage()
34: /workspace/bert/run_pretraining.py:1000: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
34:   param_storage = optimizer._new_params.storage()
27: /workspace/bert/run_pretraining.py:1000: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
27:   param_storage = optimizer._new_params.storage()
28: /workspace/bert/run_pretraining.py:1000: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
28:   param_storage = optimizer._new_params.storage()
 2: /workspace/bert/run_pretraining.py:1000: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
 2:   param_storage = optimizer._new_params.storage()
11: /workspace/bert/run_pretraining.py:1000: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
11:   param_storage = optimizer._new_params.storage()
32: /workspace/bert/run_pretraining.py:1000: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
32:   param_storage = optimizer._new_params.storage()
33: /workspace/bert/run_pretraining.py:1000: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
33:   param_storage = optimizer._new_params.storage()
26: /workspace/bert/run_pretraining.py:1000: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
26:   param_storage = optimizer._new_params.storage()
35: /workspace/bert/run_pretraining.py:1000: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
35:   param_storage = optimizer._new_params.storage()
 6: /workspace/bert/run_pretraining.py:1000: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
 6:   param_storage = optimizer._new_params.storage()
30: /workspace/bert/run_pretraining.py:1000: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
30:   param_storage = optimizer._new_params.storage()
13: /workspace/bert/run_pretraining.py:1000: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
13:   param_storage = optimizer._new_params.storage()
19: /workspace/bert/run_pretraining.py:1000: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
19:   param_storage = optimizer._new_params.storage()
 5: /workspace/bert/run_pretraining.py:1000: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
 5:   param_storage = optimizer._new_params.storage()
12: /workspace/bert/run_pretraining.py:1000: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
12:   param_storage = optimizer._new_params.storage()
18: /workspace/bert/run_pretraining.py:1000: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
18:   param_storage = optimizer._new_params.storage()
 7: /workspace/bert/run_pretraining.py:1000: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
 7:   param_storage = optimizer._new_params.storage()
 1: /workspace/bert/run_pretraining.py:1000: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
 1:   param_storage = optimizer._new_params.storage()
 4: /workspace/bert/run_pretraining.py:1000: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
 4:   param_storage = optimizer._new_params.storage()
22: /workspace/bert/run_pretraining.py:1000: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
22:   param_storage = optimizer._new_params.storage()
23: /workspace/bert/run_pretraining.py:1000: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
23:   param_storage = optimizer._new_params.storage()
 0: :::MLLOG {"namespace": "", "time_ms": 1690734162332, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_steps", "value": 0, "metadata": {"file": "/workspace/bert/schedulers.py", "lineno": 86}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734162358, "event_type": "POINT_IN_TIME", "key": "opt_lamb_learning_rate_decay_poly_power", "value": 1.0, "metadata": {"file": "/workspace/bert/schedulers.py", "lineno": 87}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734162358, "event_type": "POINT_IN_TIME", "key": "start_warmup_step", "value": 0, "metadata": {"file": "/workspace/bert/schedulers.py", "lineno": 88}}
 0: /workspace/bert/run_pretraining.py:1000: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
 0:   param_storage = optimizer._new_params.storage()
35: Torch distributed is available.
35: Torch distributed is initialized.
16: Torch distributed is available.
16: Torch distributed is initialized.
34: Torch distributed is available.
34: Torch distributed is initialized.
 6: Torch distributed is available.
 6: Torch distributed is initialized.
14: Torch distributed is available.
14: Torch distributed is initialized.
 9: Torch distributed is available.
 9: Torch distributed is initialized.
17: Torch distributed is available.
17: Torch distributed is initialized.
25: Torch distributed is available.
25: Torch distributed is initialized.
15: Torch distributed is available.
15: Torch distributed is initialized.
 8: Torch distributed is available.
 8: Torch distributed is initialized.
26: Torch distributed is available.
26: Torch distributed is initialized.
 3: Torch distributed is available.
 3: Torch distributed is initialized.
 7: Torch distributed is available.
 7: Torch distributed is initialized.
18: Torch distributed is available.
18: Torch distributed is initialized.
13: Torch distributed is available.
13: Torch distributed is initialized.
12: Torch distributed is available.
12: Torch distributed is initialized.
 2: Torch distributed is available.
 2: Torch distributed is initialized.
21: Torch distributed is available.
21: Torch distributed is initialized.
 0: Torch distributed is available.
 0: Torch distributed is initialized.
33: Torch distributed is available.
33: Torch distributed is initialized.
24: Torch distributed is available.
24: Torch distributed is initialized.
20: Torch distributed is available.
20: Torch distributed is initialized.
19: Torch distributed is available.
19: Torch distributed is initialized.
27: Torch distributed is available.
27: Torch distributed is initialized.
11: Torch distributed is available.
11: Torch distributed is initialized.
 5: Torch distributed is available.
 5: Torch distributed is initialized.
28: Torch distributed is available.
28: Torch distributed is initialized.
10: Torch distributed is available.
10: Torch distributed is initialized.
 1: Torch distributed is available.
 1: Torch distributed is initialized.
32: Torch distributed is available.
32: Torch distributed is initialized.
23: Torch distributed is available.
23: Torch distributed is initialized.
22: Torch distributed is available.
22: Torch distributed is initialized.
30: Torch distributed is available.
30: Torch distributed is initialized.
 4: Torch distributed is available.
 4: Torch distributed is initialized.
29: Torch distributed is available.
29: Torch distributed is initialized.
31: Torch distributed is available.
31: Torch distributed is initialized.
 3: /workspace/bert/fmha.py:77: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
 3:   Wtmp.set_(Wqkv.storage(), Wqkv.storage_offset(), Wqkv.size(), Wqkv.stride())
 3: /workspace/bert/fmha.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
 3:   Btmp.set_(Bqkv.storage(), Bqkv.storage_offset(), Bqkv.size(), Bqkv.stride())
19: /workspace/bert/fmha.py:77: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
19:   Wtmp.set_(Wqkv.storage(), Wqkv.storage_offset(), Wqkv.size(), Wqkv.stride())
19: /workspace/bert/fmha.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
19:   Btmp.set_(Bqkv.storage(), Bqkv.storage_offset(), Bqkv.size(), Bqkv.stride())
 5: /workspace/bert/fmha.py:77: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
 5:   Wtmp.set_(Wqkv.storage(), Wqkv.storage_offset(), Wqkv.size(), Wqkv.stride())
 5: /workspace/bert/fmha.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
 5:   Btmp.set_(Bqkv.storage(), Bqkv.storage_offset(), Bqkv.size(), Bqkv.stride())
 7: /workspace/bert/fmha.py:77: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
 7:   Wtmp.set_(Wqkv.storage(), Wqkv.storage_offset(), Wqkv.size(), Wqkv.stride())
 7: /workspace/bert/fmha.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
 7:   Btmp.set_(Bqkv.storage(), Bqkv.storage_offset(), Bqkv.size(), Bqkv.stride())
25: /workspace/bert/fmha.py:77: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
25:   Wtmp.set_(Wqkv.storage(), Wqkv.storage_offset(), Wqkv.size(), Wqkv.stride())
25: /workspace/bert/fmha.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
25:   Btmp.set_(Bqkv.storage(), Bqkv.storage_offset(), Bqkv.size(), Bqkv.stride())
11: /workspace/bert/fmha.py:77: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
11:   Wtmp.set_(Wqkv.storage(), Wqkv.storage_offset(), Wqkv.size(), Wqkv.stride())
11: /workspace/bert/fmha.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
11:   Btmp.set_(Bqkv.storage(), Bqkv.storage_offset(), Bqkv.size(), Bqkv.stride())
29: /workspace/bert/fmha.py:77: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
29:   Wtmp.set_(Wqkv.storage(), Wqkv.storage_offset(), Wqkv.size(), Wqkv.stride())
29: /workspace/bert/fmha.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
29:   Btmp.set_(Bqkv.storage(), Bqkv.storage_offset(), Bqkv.size(), Bqkv.stride())
12: /workspace/bert/fmha.py:77: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
12:   Wtmp.set_(Wqkv.storage(), Wqkv.storage_offset(), Wqkv.size(), Wqkv.stride())
12: /workspace/bert/fmha.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
12:   Btmp.set_(Bqkv.storage(), Bqkv.storage_offset(), Bqkv.size(), Bqkv.stride())
18: /workspace/bert/fmha.py:77: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
18:   Wtmp.set_(Wqkv.storage(), Wqkv.storage_offset(), Wqkv.size(), Wqkv.stride())
18: /workspace/bert/fmha.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
18:   Btmp.set_(Bqkv.storage(), Bqkv.storage_offset(), Bqkv.size(), Bqkv.stride())
20: /workspace/bert/fmha.py:77: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
20:   Wtmp.set_(Wqkv.storage(), Wqkv.storage_offset(), Wqkv.size(), Wqkv.stride())
20: /workspace/bert/fmha.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
20:   Btmp.set_(Bqkv.storage(), Bqkv.storage_offset(), Bqkv.size(), Bqkv.stride())
 0: /workspace/bert/fmha.py:77: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
 0:   Wtmp.set_(Wqkv.storage(), Wqkv.storage_offset(), Wqkv.size(), Wqkv.stride())
 0: /workspace/bert/fmha.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
 0:   Btmp.set_(Bqkv.storage(), Bqkv.storage_offset(), Bqkv.size(), Bqkv.stride())
27: /workspace/bert/fmha.py:77: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
27:   Wtmp.set_(Wqkv.storage(), Wqkv.storage_offset(), Wqkv.size(), Wqkv.stride())
27: /workspace/bert/fmha.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
27:   Btmp.set_(Bqkv.storage(), Bqkv.storage_offset(), Bqkv.size(), Bqkv.stride())
16: /workspace/bert/fmha.py:77: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
16:   Wtmp.set_(Wqkv.storage(), Wqkv.storage_offset(), Wqkv.size(), Wqkv.stride())
16: /workspace/bert/fmha.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
16:   Btmp.set_(Bqkv.storage(), Bqkv.storage_offset(), Bqkv.size(), Bqkv.stride())
 1: /workspace/bert/fmha.py:77: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
 1:   Wtmp.set_(Wqkv.storage(), Wqkv.storage_offset(), Wqkv.size(), Wqkv.stride())
 1: /workspace/bert/fmha.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
 1:   Btmp.set_(Bqkv.storage(), Bqkv.storage_offset(), Bqkv.size(), Bqkv.stride())
24: /workspace/bert/fmha.py:77: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
24:   Wtmp.set_(Wqkv.storage(), Wqkv.storage_offset(), Wqkv.size(), Wqkv.stride())
24: /workspace/bert/fmha.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
24:   Btmp.set_(Bqkv.storage(), Bqkv.storage_offset(), Bqkv.size(), Bqkv.stride())
35: /workspace/bert/fmha.py:77: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
35:   Wtmp.set_(Wqkv.storage(), Wqkv.storage_offset(), Wqkv.size(), Wqkv.stride())
35: /workspace/bert/fmha.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
35:   Btmp.set_(Bqkv.storage(), Bqkv.storage_offset(), Bqkv.size(), Bqkv.stride())
10: /workspace/bert/fmha.py:77: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10:   Wtmp.set_(Wqkv.storage(), Wqkv.storage_offset(), Wqkv.size(), Wqkv.stride())
10: /workspace/bert/fmha.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10:   Btmp.set_(Bqkv.storage(), Bqkv.storage_offset(), Bqkv.size(), Bqkv.stride())
 8: /workspace/bert/fmha.py:77: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
 8:   Wtmp.set_(Wqkv.storage(), Wqkv.storage_offset(), Wqkv.size(), Wqkv.stride())
 8: /workspace/bert/fmha.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
 8:   Btmp.set_(Bqkv.storage(), Bqkv.storage_offset(), Bqkv.size(), Bqkv.stride())
23: /workspace/bert/fmha.py:77: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
23:   Wtmp.set_(Wqkv.storage(), Wqkv.storage_offset(), Wqkv.size(), Wqkv.stride())
23: /workspace/bert/fmha.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
23:   Btmp.set_(Bqkv.storage(), Bqkv.storage_offset(), Bqkv.size(), Bqkv.stride())
 9: /workspace/bert/fmha.py:77: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
 9:   Wtmp.set_(Wqkv.storage(), Wqkv.storage_offset(), Wqkv.size(), Wqkv.stride())
 9: /workspace/bert/fmha.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
 9:   Btmp.set_(Bqkv.storage(), Bqkv.storage_offset(), Bqkv.size(), Bqkv.stride())
28: /workspace/bert/fmha.py:77: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
28:   Wtmp.set_(Wqkv.storage(), Wqkv.storage_offset(), Wqkv.size(), Wqkv.stride())
28: /workspace/bert/fmha.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
28:   Btmp.set_(Bqkv.storage(), Bqkv.storage_offset(), Bqkv.size(), Bqkv.stride())
30: /workspace/bert/fmha.py:77: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
30:   Wtmp.set_(Wqkv.storage(), Wqkv.storage_offset(), Wqkv.size(), Wqkv.stride())
30: /workspace/bert/fmha.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
30:   Btmp.set_(Bqkv.storage(), Bqkv.storage_offset(), Bqkv.size(), Bqkv.stride())
13: /workspace/bert/fmha.py:77: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
13:   Wtmp.set_(Wqkv.storage(), Wqkv.storage_offset(), Wqkv.size(), Wqkv.stride())
13: /workspace/bert/fmha.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
13:   Btmp.set_(Bqkv.storage(), Bqkv.storage_offset(), Bqkv.size(), Bqkv.stride())
26: /workspace/bert/fmha.py:77: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
26:   Wtmp.set_(Wqkv.storage(), Wqkv.storage_offset(), Wqkv.size(), Wqkv.stride())
26: /workspace/bert/fmha.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
26:   Btmp.set_(Bqkv.storage(), Bqkv.storage_offset(), Bqkv.size(), Bqkv.stride())
15: /workspace/bert/fmha.py:77: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
15:   Wtmp.set_(Wqkv.storage(), Wqkv.storage_offset(), Wqkv.size(), Wqkv.stride())
15: /workspace/bert/fmha.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
15:   Btmp.set_(Bqkv.storage(), Bqkv.storage_offset(), Bqkv.size(), Bqkv.stride())
 6: /workspace/bert/fmha.py:77: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
 6:   Wtmp.set_(Wqkv.storage(), Wqkv.storage_offset(), Wqkv.size(), Wqkv.stride())
 6: /workspace/bert/fmha.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
 6:   Btmp.set_(Bqkv.storage(), Bqkv.storage_offset(), Bqkv.size(), Bqkv.stride())
17: /workspace/bert/fmha.py:77: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
17:   Wtmp.set_(Wqkv.storage(), Wqkv.storage_offset(), Wqkv.size(), Wqkv.stride())
17: /workspace/bert/fmha.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
17:   Btmp.set_(Bqkv.storage(), Bqkv.storage_offset(), Bqkv.size(), Bqkv.stride())
32: /workspace/bert/fmha.py:77: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
32:   Wtmp.set_(Wqkv.storage(), Wqkv.storage_offset(), Wqkv.size(), Wqkv.stride())
32: /workspace/bert/fmha.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
32:   Btmp.set_(Bqkv.storage(), Bqkv.storage_offset(), Bqkv.size(), Bqkv.stride())
14: /workspace/bert/fmha.py:77: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
14:   Wtmp.set_(Wqkv.storage(), Wqkv.storage_offset(), Wqkv.size(), Wqkv.stride())
14: /workspace/bert/fmha.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
14:   Btmp.set_(Bqkv.storage(), Bqkv.storage_offset(), Bqkv.size(), Bqkv.stride())
21: /workspace/bert/fmha.py:77: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
21:   Wtmp.set_(Wqkv.storage(), Wqkv.storage_offset(), Wqkv.size(), Wqkv.stride())
 2: /workspace/bert/fmha.py:77: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
 2:   Wtmp.set_(Wqkv.storage(), Wqkv.storage_offset(), Wqkv.size(), Wqkv.stride())
 2: /workspace/bert/fmha.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
 2:   Btmp.set_(Bqkv.storage(), Bqkv.storage_offset(), Bqkv.size(), Bqkv.stride())
21: /workspace/bert/fmha.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
21:   Btmp.set_(Bqkv.storage(), Bqkv.storage_offset(), Bqkv.size(), Bqkv.stride())
34: /workspace/bert/fmha.py:77: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
34:   Wtmp.set_(Wqkv.storage(), Wqkv.storage_offset(), Wqkv.size(), Wqkv.stride())
34: /workspace/bert/fmha.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
34:   Btmp.set_(Bqkv.storage(), Bqkv.storage_offset(), Bqkv.size(), Bqkv.stride())
33: /workspace/bert/fmha.py:77: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
33:   Wtmp.set_(Wqkv.storage(), Wqkv.storage_offset(), Wqkv.size(), Wqkv.stride())
33: /workspace/bert/fmha.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
33:   Btmp.set_(Bqkv.storage(), Bqkv.storage_offset(), Bqkv.size(), Bqkv.stride())
31: /workspace/bert/fmha.py:77: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
31:   Wtmp.set_(Wqkv.storage(), Wqkv.storage_offset(), Wqkv.size(), Wqkv.stride())
31: /workspace/bert/fmha.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
31:   Btmp.set_(Bqkv.storage(), Bqkv.storage_offset(), Bqkv.size(), Bqkv.stride())
 4: /workspace/bert/fmha.py:77: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
 4:   Wtmp.set_(Wqkv.storage(), Wqkv.storage_offset(), Wqkv.size(), Wqkv.stride())
 4: /workspace/bert/fmha.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
 4:   Btmp.set_(Bqkv.storage(), Bqkv.storage_offset(), Bqkv.size(), Bqkv.stride())
22: /workspace/bert/fmha.py:77: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
22:   Wtmp.set_(Wqkv.storage(), Wqkv.storage_offset(), Wqkv.size(), Wqkv.stride())
22: /workspace/bert/fmha.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
22:   Btmp.set_(Bqkv.storage(), Bqkv.storage_offset(), Bqkv.size(), Bqkv.stride())
29: /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:342: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
29:   warnings.warn(
31: /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:342: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
31:   warnings.warn(
30: /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:342: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
30:   warnings.warn(
32: /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:342: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
32:   warnings.warn(
25: /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:342: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
25:   warnings.warn(
28: /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:342: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
28:   warnings.warn(
34: /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:342: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
34:   warnings.warn(
27: /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:342: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
27:   warnings.warn(
24: /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:342: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
24:   warnings.warn(
33: /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:342: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
33:   warnings.warn(
35: /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:342: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
35:   warnings.warn(
20: /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:342: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
20:   warnings.warn(
23: /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:342: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
23:   warnings.warn(
 0: /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:342: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 0:   warnings.warn(
22: /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:342: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
22:   warnings.warn(
26: /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:342: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
26:   warnings.warn(
 1: /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:342: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 1:   warnings.warn(
18: /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:342: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
18:   warnings.warn(
21: /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:342: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
21:   warnings.warn(
13: /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:342: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
13:   warnings.warn(
16: /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:342: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
16:   warnings.warn(
 3: /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:342: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 3:   warnings.warn(
15: /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:342: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
15:   warnings.warn(
 9: /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:342: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 9:   warnings.warn(
 2: /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:342: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 2:   warnings.warn(
10: /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:342: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
10:   warnings.warn(
 6: /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:342: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 6:   warnings.warn(
19: /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:342: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
19:   warnings.warn(
17: /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:342: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
17:   warnings.warn(
12: /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:342: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
12:   warnings.warn(
 8: /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:342: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 8:   warnings.warn(
11: /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:342: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
11:   warnings.warn(
 7: /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:342: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 7:   warnings.warn(
 4: /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:342: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 4:   warnings.warn(
 5: /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:342: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 5:   warnings.warn(
14: /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:342: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
14:   warnings.warn(
 0: :::MLLOG {"namespace": "", "time_ms": 1690734170732, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1606}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734170733, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1606}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734170752, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1619, "epoch_num": 0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734170753, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1621, "first_epoch_num": 1, "epoch_count": 1}}
 0: parsed args:
 0: Namespace(allreduce_post_accumulation=True, allreduce_post_accumulation_fp16=True, average_packing_rate=2, bert_config_path='/workspace/phase1/bert_config.json', bert_model='bert-large-uncased', bypass_amp=False, cache_eval_data=True, checkpoint_activations=False, cuda_graph_mode='segmented', ddp_type='apex', dense_seq_output=True, device=device(type='cuda', index=0), disable_apex_softmax=False, disable_fuse_mask=False, disable_fuse_qkv=False, disable_fuse_scale=False, distributed_lamb=True, do_train=True, dwu_e5m2_allgather=False, dwu_group_size=0, dwu_num_ag_pg=1, dwu_num_ar_pg=1, dwu_num_blocks=1, dwu_num_chunks=1, dwu_num_rs_pg=1, dwu_overlap_reductions=False, enable_fuse_dropout=False, enable_stream=False, eval_batch_size=16, eval_dir='/workspace/evaldata', eval_iter_samples=200000, eval_iter_start_samples=400000, exchange_padding=False, fp16=True, fused_bias_fc=True, fused_bias_fc_loss_head=False, fused_bias_mha=True, fused_dropout_add=True, fused_gelu_bias=False, fused_gemm_gelu=True, fused_mha=False, 
 0: gradient_accumulation_steps=1, init_checkpoint='/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt', init_tf_checkpoint=None, input_dir='/workspace/data', keep_n_most_recent_checkpoints=20, learning_rate=0.0006, local_rank=0, log_freq=0.0, loss_scale=0.0, max_iterations_per_graph=4, max_pack_factor=3, max_predictions_per_seq=76, max_samples_termination=1000000000.0, max_seq_length=512, max_steps=100000.0, min_samples_to_start_checkpoints=3000000, n_gpu=36, num_epochs_to_generate_seeds_for=2, num_eval_examples=10000, num_samples_per_checkpoint=500000, opt_lamb_beta_1=0.8, opt_lamb_beta_2=0.996, order_samples=False, output_dir='/results', packed_samples=True, pad=False, pad_fmha=True, phase2=True, resume_from_checkpoint=False, resume_step=0, seed=10141, skip_checkpoint=True, start_warmup_step=0.0, synthetic_input=False, target_mlm_accuracy=0.72, train_batch_size=36, train_mlm_accuracy_window_size=0, unpad=False, unpad_fmha=False, use_cuda_graph=False, use_ddp=False, use_env=False, use_gradient_as_bucket_view=False,
 0:  use_transformer_engine2=False, warmup_proportion=0.0, warmup_steps=0.0, weight_decay_rate=0.1)
 0: epoch: 1
 0: :::MLLOG {"namespace": "", "time_ms": 1690734170753, "event_type": "POINT_IN_TIME", "key": "data_file", "value": "/workspace/data/part_01866.hdf5", "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1655}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734230610, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.3800133168697357, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1836, "epoch_num": 401606}}
 0: {'global_steps': 155, 'eval_loss': 4.038520812988281, 'eval_mlm_accuracy': 0.3800133168697357}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734260039, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.3952012360095978, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1836, "epoch_num": 600862}}
 0: {'global_steps': 232, 'eval_loss': 3.9017367362976074, 'eval_mlm_accuracy': 0.3952012360095978}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734289484, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.4107651114463806, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1836, "epoch_num": 800666}}
 0: {'global_steps': 309, 'eval_loss': 3.745793581008911, 'eval_mlm_accuracy': 0.4107651114463806}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734319016, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.4483729600906372, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1836, "epoch_num": 999630}}
 0: {'global_steps': 386, 'eval_loss': 3.4298171997070312, 'eval_mlm_accuracy': 0.4483729600906372}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734348737, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.48893481492996216, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1836, "epoch_num": 1199162}}
 0: {'global_steps': 463, 'eval_loss': 3.071279287338257, 'eval_mlm_accuracy': 0.48893481492996216}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734378818, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.5674617886543274, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1836, "epoch_num": 1401524}}
 0: {'global_steps': 541, 'eval_loss': 2.4294593334198, 'eval_mlm_accuracy': 0.5674617886543274}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734408287, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.6512755751609802, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1836, "epoch_num": 1600849}}
 0: {'global_steps': 618, 'eval_loss': 1.7761390209197998, 'eval_mlm_accuracy': 0.6512755751609802}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734437774, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7011010050773621, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1836, "epoch_num": 1800573}}
 0: {'global_steps': 695, 'eval_loss': 1.4256397485733032, 'eval_mlm_accuracy': 0.7011010050773621}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734467243, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7093628644943237, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1836, "epoch_num": 1999905}}
 0: {'global_steps': 772, 'eval_loss': 1.3674436807632446, 'eval_mlm_accuracy': 0.7093628644943237}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734496724, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7129496932029724, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1836, "epoch_num": 2199364}}
 0: {'global_steps': 849, 'eval_loss': 1.3478201627731323, 'eval_mlm_accuracy': 0.7129496932029724}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734526169, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7151049971580505, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1836, "epoch_num": 2398154}}
 0: {'global_steps': 926, 'eval_loss': 1.335854172706604, 'eval_mlm_accuracy': 0.7151049971580505}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734556143, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7166672348976135, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1836, "epoch_num": 2600302}}
 0: {'global_steps': 1004, 'eval_loss': 1.3275282382965088, 'eval_mlm_accuracy': 0.7166672348976135}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734586254, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7177110910415649, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1836, "epoch_num": 2799445}}
 0: {'global_steps': 1081, 'eval_loss': 1.3215653896331787, 'eval_mlm_accuracy': 0.7177110910415649}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734615736, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7185096740722656, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1836, "epoch_num": 2998719}}
 0: {'global_steps': 1158, 'eval_loss': 1.3167179822921753, 'eval_mlm_accuracy': 0.7185096740722656}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734645190, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7188903093338013, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1836, "epoch_num": 3198492}}
 0: {'global_steps': 1235, 'eval_loss': 1.3160375356674194, 'eval_mlm_accuracy': 0.7188903093338013}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734674657, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7185937762260437, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1836, "epoch_num": 3397882}}
 0: {'global_steps': 1312, 'eval_loss': 1.310072898864746, 'eval_mlm_accuracy': 0.7185937762260437}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734704135, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7201886773109436, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1836, "epoch_num": 3596949}}
 0: {'global_steps': 1389, 'eval_loss': 1.3075560331344604, 'eval_mlm_accuracy': 0.7201886773109436}
 0: 0.720189 > 0.720000, Target MLM Accuracy reached at 1389
 0: (1, 1389.0) {'final_loss': 0.0}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734704135, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1966, "first_epoch_num": 1}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734704135, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1969, "epoch_num": 3596949}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734704135, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 3596949, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1977}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734704136, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 10000, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1980}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734704136, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1983, "status": "success"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690734704136, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 6743.399918673154, "epoch_num": 3596949}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1987, "step": [2, 1389]}}
 0: {'e2e_time': 552.8298001289368, 'training_sequences_per_second': 482328.3525155705, 'final_loss': 0.0, 'raw_train_time': 537.3932480812073}
 6: ENDING TIMING RUN AT 2023-07-30 12:31:46 PM
 6: RESULT,bert,31309,561,nnisbet,2023-07-30 12:22:25 PM
 0: ENDING TIMING RUN AT 2023-07-30 12:31:46 PM
 0: RESULT,bert,10141,561,nnisbet,2023-07-30 12:22:25 PM
12: ENDING TIMING RUN AT 2023-07-30 12:31:46 PM
12: RESULT,bert,12662,561,nnisbet,2023-07-30 12:22:25 PM
22: ENDING TIMING RUN AT 2023-07-30 12:31:46 PM
22: RESULT,bert,406,561,nnisbet,2023-07-30 12:22:25 PM
18: ENDING TIMING RUN AT 2023-07-30 12:31:46 PM
18: RESULT,bert,8810,561,nnisbet,2023-07-30 12:22:25 PM
20: ENDING TIMING RUN AT 2023-07-30 12:31:46 PM
20: RESULT,bert,6293,561,nnisbet,2023-07-30 12:22:25 PM
32: ENDING TIMING RUN AT 2023-07-30 12:31:46 PM
32: RESULT,bert,21546,561,nnisbet,2023-07-30 12:22:25 PM
 8: ENDING TIMING RUN AT 2023-07-30 12:31:46 PM
 8: RESULT,bert,21337,561,nnisbet,2023-07-30 12:22:25 PM
34: ENDING TIMING RUN AT 2023-07-30 12:31:46 PM
34: RESULT,bert,13601,561,nnisbet,2023-07-30 12:22:25 PM
 3: ENDING TIMING RUN AT 2023-07-30 12:31:46 PM
 3: RESULT,bert,21425,561,nnisbet,2023-07-30 12:22:25 PM
31: ENDING TIMING RUN AT 2023-07-30 12:31:46 PM
31: RESULT,bert,32336,561,nnisbet,2023-07-30 12:22:25 PM
11: ENDING TIMING RUN AT 2023-07-30 12:31:46 PM
11: RESULT,bert,1160,561,nnisbet,2023-07-30 12:22:25 PM
 5: ENDING TIMING RUN AT 2023-07-30 12:31:46 PM
 5: RESULT,bert,6137,561,nnisbet,2023-07-30 12:22:25 PM
24: ENDING TIMING RUN AT 2023-07-30 12:31:46 PM
24: RESULT,bert,30772,561,nnisbet,2023-07-30 12:22:25 PM
28: ENDING TIMING RUN AT 2023-07-30 12:31:46 PM
28: RESULT,bert,19352,561,nnisbet,2023-07-30 12:22:25 PM
16: ENDING TIMING RUN AT 2023-07-30 12:31:46 PM
16: RESULT,bert,9591,561,nnisbet,2023-07-30 12:22:25 PM
15: ENDING TIMING RUN AT 2023-07-30 12:31:46 PM
15: RESULT,bert,12283,561,nnisbet,2023-07-30 12:22:25 PM
26: ENDING TIMING RUN AT 2023-07-30 12:31:46 PM
26: RESULT,bert,24155,561,nnisbet,2023-07-30 12:22:25 PM
 7: ENDING TIMING RUN AT 2023-07-30 12:31:46 PM
 7: RESULT,bert,4942,561,nnisbet,2023-07-30 12:22:25 PM
21: ENDING TIMING RUN AT 2023-07-30 12:31:46 PM
21: RESULT,bert,85,561,nnisbet,2023-07-30 12:22:25 PM
 1: ENDING TIMING RUN AT 2023-07-30 12:31:46 PM
 1: RESULT,bert,26202,561,nnisbet,2023-07-30 12:22:25 PM
13: ENDING TIMING RUN AT 2023-07-30 12:31:46 PM
13: RESULT,bert,31560,561,nnisbet,2023-07-30 12:22:25 PM
23: ENDING TIMING RUN AT 2023-07-30 12:31:46 PM
23: RESULT,bert,30039,561,nnisbet,2023-07-30 12:22:25 PM
35: ENDING TIMING RUN AT 2023-07-30 12:31:46 PM
35: RESULT,bert,9898,561,nnisbet,2023-07-30 12:22:25 PM
10: ENDING TIMING RUN AT 2023-07-30 12:31:46 PM
10: RESULT,bert,17015,561,nnisbet,2023-07-30 12:22:25 PM
19: ENDING TIMING RUN AT 2023-07-30 12:31:46 PM
19: RESULT,bert,8142,561,nnisbet,2023-07-30 12:22:25 PM
 9: ENDING TIMING RUN AT 2023-07-30 12:31:46 PM
 9: RESULT,bert,11533,561,nnisbet,2023-07-30 12:22:25 PM
33: ENDING TIMING RUN AT 2023-07-30 12:31:46 PM
33: RESULT,bert,21159,561,nnisbet,2023-07-30 12:22:25 PM
17: ENDING TIMING RUN AT 2023-07-30 12:31:46 PM
17: RESULT,bert,19664,561,nnisbet,2023-07-30 12:22:25 PM
14: ENDING TIMING RUN AT 2023-07-30 12:31:46 PM
14: RESULT,bert,21970,561,nnisbet,2023-07-30 12:22:25 PM
25: ENDING TIMING RUN AT 2023-07-30 12:31:46 PM
25: RESULT,bert,18329,561,nnisbet,2023-07-30 12:22:25 PM
 2: ENDING TIMING RUN AT 2023-07-30 12:31:46 PM
 2: RESULT,bert,5023,561,nnisbet,2023-07-30 12:22:25 PM
30: ENDING TIMING RUN AT 2023-07-30 12:31:46 PM
30: RESULT,bert,16923,561,nnisbet,2023-07-30 12:22:25 PM
27: ENDING TIMING RUN AT 2023-07-30 12:31:46 PM
27: RESULT,bert,8691,561,nnisbet,2023-07-30 12:22:25 PM
29: ENDING TIMING RUN AT 2023-07-30 12:31:46 PM
29: RESULT,bert,27775,561,nnisbet,2023-07-30 12:22:25 PM
 4: ENDING TIMING RUN AT 2023-07-30 12:31:46 PM
 4: RESULT,bert,29186,561,nnisbet,2023-07-30 12:22:25 PM
