Beginning trial 10 of 10
:::DLPAL /scratch/nnisbet/bert.sif 1339 18 node[0400-0401,0403-0418]
hosts=node0400 node0401 node0403 node0404 node0405 node0406 node0407 node0408 node0409 node0410 node0411 node0412 node0413 node0414 node0415 node0416 node0417 node0418 
 0: INFO:    underlay of /etc/localtime required more than 50 (96) bind mounts
 1: INFO:    underlay of /etc/localtime required more than 50 (96) bind mounts
 1: INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (478) bind mounts
 0: INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (478) bind mounts
 2: INFO:    underlay of /etc/localtime required more than 50 (96) bind mounts
12: INFO:    underlay of /etc/localtime required more than 50 (96) bind mounts
11: INFO:    underlay of /etc/localtime required more than 50 (96) bind mounts
 8: INFO:    underlay of /etc/localtime required more than 50 (96) bind mounts
15: INFO:    underlay of /etc/localtime required more than 50 (96) bind mounts
24: INFO:    underlay of /etc/localtime required more than 50 (96) bind mounts
16: INFO:    underlay of /etc/localtime required more than 50 (96) bind mounts
 5: INFO:    underlay of /etc/localtime required more than 50 (96) bind mounts
 6: INFO:    underlay of /etc/localtime required more than 50 (96) bind mounts
 3: INFO:    underlay of /etc/localtime required more than 50 (96) bind mounts
13: INFO:    underlay of /etc/localtime required more than 50 (96) bind mounts
18: INFO:    underlay of /etc/localtime required more than 50 (96) bind mounts
20: INFO:    underlay of /etc/localtime required more than 50 (96) bind mounts
10: INFO:    underlay of /etc/localtime required more than 50 (96) bind mounts
26: INFO:    underlay of /etc/localtime required more than 50 (96) bind mounts
28: INFO:    underlay of /etc/localtime required more than 50 (96) bind mounts
 9: INFO:    underlay of /etc/localtime required more than 50 (96) bind mounts
14: INFO:    underlay of /etc/localtime required more than 50 (96) bind mounts
30: INFO:    underlay of /etc/localtime required more than 50 (96) bind mounts
34: INFO:    underlay of /etc/localtime required more than 50 (96) bind mounts
25: INFO:    underlay of /etc/localtime required more than 50 (96) bind mounts
22: INFO:    underlay of /etc/localtime required more than 50 (96) bind mounts
17: INFO:    underlay of /etc/localtime required more than 50 (96) bind mounts
 4: INFO:    underlay of /etc/localtime required more than 50 (96) bind mounts
 7: INFO:    underlay of /etc/localtime required more than 50 (96) bind mounts
19: INFO:    underlay of /etc/localtime required more than 50 (96) bind mounts
21: INFO:    underlay of /etc/localtime required more than 50 (96) bind mounts
27: INFO:    underlay of /etc/localtime required more than 50 (96) bind mounts
29: INFO:    underlay of /etc/localtime required more than 50 (96) bind mounts
33: INFO:    underlay of /etc/localtime required more than 50 (96) bind mounts
 1: 13:4: not a valid test operator: (
 1: 13:4: not a valid test operator: 525.125.06
35: INFO:    underlay of /etc/localtime required more than 50 (96) bind mounts
31: INFO:    underlay of /etc/localtime required more than 50 (96) bind mounts
 0: 13:4: not a valid test operator: (
 0: 13:4: not a valid test operator: 525.125.06
23: INFO:    underlay of /etc/localtime required more than 50 (96) bind mounts
32: INFO:    underlay of /etc/localtime required more than 50 (96) bind mounts
12: INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (478) bind mounts
13: INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (478) bind mounts
15: INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (478) bind mounts
14: INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (478) bind mounts
21: INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (478) bind mounts
20: INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (478) bind mounts
11: INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (478) bind mounts
10: INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (478) bind mounts
28: INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (478) bind mounts
29: INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (478) bind mounts
19: INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (478) bind mounts
18: INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (478) bind mounts
24: INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (478) bind mounts
25: INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (478) bind mounts
16: INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (478) bind mounts
17: INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (478) bind mounts
 6: INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (478) bind mounts
 7: INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (478) bind mounts
 2: INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (478) bind mounts
 3: INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (478) bind mounts
 8: INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (478) bind mounts
 9: INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (478) bind mounts
 5: INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (478) bind mounts
 4: INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (478) bind mounts
34: INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (478) bind mounts
35: INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (478) bind mounts
23: INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (478) bind mounts
22: INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (478) bind mounts
26: INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (478) bind mounts
27: INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (478) bind mounts
33: INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (478) bind mounts
32: INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (478) bind mounts
30: INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (478) bind mounts
31: INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (478) bind mounts
12: 13:4: not a valid test operator: (
12: 13:4: not a valid test operator: 525.125.06
13: 13:4: not a valid test operator: (
13: 13:4: not a valid test operator: 525.125.06
14: 13:4: not a valid test operator: (
14: 13:4: not a valid test operator: 525.125.06
15: 13:4: not a valid test operator: (
15: 13:4: not a valid test operator: 525.125.06
20: 13:4: not a valid test operator: (
20: 13:4: not a valid test operator: 525.125.06
21: 13:4: not a valid test operator: (
21: 13:4: not a valid test operator: 525.125.06
10: 13:4: not a valid test operator: (
10: 13:4: not a valid test operator: 525.125.06
11: 13:4: not a valid test operator: (
11: 13:4: not a valid test operator: 525.125.06
29: 13:4: not a valid test operator: (
29: 13:4: not a valid test operator: 525.125.06
28: 13:4: not a valid test operator: (
28: 13:4: not a valid test operator: 525.125.06
24: 13:4: not a valid test operator: (
24: 13:4: not a valid test operator: 525.125.06
25: 13:4: not a valid test operator: (
25: 13:4: not a valid test operator: 525.125.06
18: 13:4: not a valid test operator: (
18: 13:4: not a valid test operator: 525.125.06
19: 13:4: not a valid test operator: (
19: 13:4: not a valid test operator: 525.125.06
17: 13:4: not a valid test operator: (
17: 13:4: not a valid test operator: 525.125.06
 5: 13:4: not a valid test operator: (
 5: 13:4: not a valid test operator: 525.125.06
 6: 13:4: not a valid test operator: (
 6: 13:4: not a valid test operator: 525.125.06
 7: 13:4: not a valid test operator: (
 7: 13:4: not a valid test operator: 525.125.06
16: 13:4: not a valid test operator: (
16: 13:4: not a valid test operator: 525.125.06
 3: 13:4: not a valid test operator: (
 3: 13:4: not a valid test operator: 525.125.06
 2: 13:4: not a valid test operator: (
 2: 13:4: not a valid test operator: 525.125.06
 4: 13:4: not a valid test operator: (
 4: 13:4: not a valid test operator: 525.125.06
 8: 13:4: not a valid test operator: (
 8: 13:4: not a valid test operator: 525.125.06
 9: 13:4: not a valid test operator: (
 9: 13:4: not a valid test operator: 525.125.06
23: 13:4: not a valid test operator: (
23: 13:4: not a valid test operator: 525.125.06
34: 13:4: not a valid test operator: (
34: 13:4: not a valid test operator: 525.125.06
35: 13:4: not a valid test operator: (
35: 13:4: not a valid test operator: 525.125.06
22: 13:4: not a valid test operator: (
22: 13:4: not a valid test operator: 525.125.06
27: 13:4: not a valid test operator: (
27: 13:4: not a valid test operator: 525.125.06
26: 13:4: not a valid test operator: (
26: 13:4: not a valid test operator: 525.125.06
33: 13:4: not a valid test operator: (
33: 13:4: not a valid test operator: 525.125.06
32: 13:4: not a valid test operator: (
32: 13:4: not a valid test operator: 525.125.06
30: 13:4: not a valid test operator: (
30: 13:4: not a valid test operator: 525.125.06
31: 13:4: not a valid test operator: (
31: 13:4: not a valid test operator: 525.125.06
 1: Run vars: id 1339 gpus 2 mparams ''
 1: STARTING TIMING RUN AT 2023-07-30 12:52:25 PM
 1: WORLD_SIZE=36
 1: slurm_job_nodelist=node[0400-0401,0403-0418]
 1: MASTER_ADDR=node0400
 1: HOSTNAME=node0400
 1:      python -u     /workspace/bert/run_pretraining.py         --train_batch_size=36     --learning_rate=0.0006     --opt_lamb_beta_1=0.8     --opt_lamb_beta_2=0.996     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=100000     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data     --init_checkpoint=/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt     --max_pack_factor=3          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.1     --max_samples_termination=1000000000     --eval_iter_start_samples=400000 --eval_iter_samples=200000     --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples=10000     --cache_eval_data     --output_dir=/results     --fp16     --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1     --gradient_accumulation_steps=1     --log_freq=0     --bert_config_path=/wor
 1: kspace/phase1/bert_config.json  --local_rank=1  --allreduce_post_accumulation --allreduce_post_accumulation_fp16  --dense_seq_output --pad_fmha --fused_bias_fc --fused_bias_mha --fused_dropout_add --fused_gemm_gelu --packed_samples --cuda_graph_mode 'segmented'   --seed=7036
 0: Run vars: id 1339 gpus 2 mparams ''
 0: STARTING TIMING RUN AT 2023-07-30 12:52:25 PM
 0: WORLD_SIZE=36
 0: slurm_job_nodelist=node[0400-0401,0403-0418]
 0: MASTER_ADDR=node0400
 0: HOSTNAME=node0400
 0:      python -u     /workspace/bert/run_pretraining.py         --train_batch_size=36     --learning_rate=0.0006     --opt_lamb_beta_1=0.8     --opt_lamb_beta_2=0.996     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=100000     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data     --init_checkpoint=/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt     --max_pack_factor=3          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.1     --max_samples_termination=1000000000     --eval_iter_start_samples=400000 --eval_iter_samples=200000     --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples=10000     --cache_eval_data     --output_dir=/results     --fp16     --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1     --gradient_accumulation_steps=1     --log_freq=0     --bert_config_path=/wor
 0: kspace/phase1/bert_config.json  --local_rank=0  --allreduce_post_accumulation --allreduce_post_accumulation_fp16  --dense_seq_output --pad_fmha --fused_bias_fc --fused_bias_mha --fused_dropout_add --fused_gemm_gelu --packed_samples --cuda_graph_mode 'segmented'   --seed=29586
15: Run vars: id 1339 gpus 2 mparams ''
12: Run vars: id 1339 gpus 2 mparams ''
15: STARTING TIMING RUN AT 2023-07-30 12:52:26 PM
15: WORLD_SIZE=36
15: slurm_job_nodelist=node[0400-0401,0403-0418]
15: MASTER_ADDR=node0400
15: HOSTNAME=node0400
15:      python -u     /workspace/bert/run_pretraining.py         --train_batch_size=36     --learning_rate=0.0006     --opt_lamb_beta_1=0.8     --opt_lamb_beta_2=0.996     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=100000     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data     --init_checkpoint=/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt     --max_pack_factor=3          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.1     --max_samples_termination=1000000000     --eval_iter_start_samples=400000 --eval_iter_samples=200000     --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples=10000     --cache_eval_data     --output_dir=/results     --fp16     --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1     --gradient_accumulation_steps=1     --log_freq=0     --bert_config_path=/wor
15: kspace/phase1/bert_config.json  --local_rank=1  --allreduce_post_accumulation --allreduce_post_accumulation_fp16  --dense_seq_output --pad_fmha --fused_bias_fc --fused_bias_mha --fused_dropout_add --fused_gemm_gelu --packed_samples --cuda_graph_mode 'segmented'   --seed=5811
12: STARTING TIMING RUN AT 2023-07-30 12:52:26 PM
12: WORLD_SIZE=36
12: slurm_job_nodelist=node[0400-0401,0403-0418]
12: MASTER_ADDR=node0400
12: HOSTNAME=node0400
12:      python -u     /workspace/bert/run_pretraining.py         --train_batch_size=36     --learning_rate=0.0006     --opt_lamb_beta_1=0.8     --opt_lamb_beta_2=0.996     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=100000     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data     --init_checkpoint=/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt     --max_pack_factor=3          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.1     --max_samples_termination=1000000000     --eval_iter_start_samples=400000 --eval_iter_samples=200000     --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples=10000     --cache_eval_data     --output_dir=/results     --fp16     --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1     --gradient_accumulation_steps=1     --log_freq=0     --bert_config_path=/wor
12: kspace/phase1/bert_config.json  --local_rank=0  --allreduce_post_accumulation --allreduce_post_accumulation_fp16  --dense_seq_output --pad_fmha --fused_bias_fc --fused_bias_mha --fused_dropout_add --fused_gemm_gelu --packed_samples --cuda_graph_mode 'segmented'   --seed=4617
 6: Run vars: id 1339 gpus 2 mparams ''
21: Run vars: id 1339 gpus 2 mparams ''
 6: STARTING TIMING RUN AT 2023-07-30 12:52:26 PM
 6: WORLD_SIZE=36
 6: slurm_job_nodelist=node[0400-0401,0403-0418]
 6: MASTER_ADDR=node0400
 6: HOSTNAME=node0400
 6:      python -u     /workspace/bert/run_pretraining.py         --train_batch_size=36     --learning_rate=0.0006     --opt_lamb_beta_1=0.8     --opt_lamb_beta_2=0.996     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=100000     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data     --init_checkpoint=/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt     --max_pack_factor=3          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.1     --max_samples_termination=1000000000     --eval_iter_start_samples=400000 --eval_iter_samples=200000     --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples=10000     --cache_eval_data     --output_dir=/results     --fp16     --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1     --gradient_accumulation_steps=1     --log_freq=0     --bert_config_path=/wor
 6: kspace/phase1/bert_config.json  --local_rank=0  --allreduce_post_accumulation --allreduce_post_accumulation_fp16  --dense_seq_output --pad_fmha --fused_bias_fc --fused_bias_mha --fused_dropout_add --fused_gemm_gelu --packed_samples --cuda_graph_mode 'segmented'   --seed=28063
19: Run vars: id 1339 gpus 2 mparams ''
21: STARTING TIMING RUN AT 2023-07-30 12:52:26 PM
21: WORLD_SIZE=36
17: Run vars: id 1339 gpus 2 mparams ''
21: slurm_job_nodelist=node[0400-0401,0403-0418]
21: MASTER_ADDR=node0400
21: HOSTNAME=node0400
21:      python -u     /workspace/bert/run_pretraining.py         --train_batch_size=36     --learning_rate=0.0006     --opt_lamb_beta_1=0.8     --opt_lamb_beta_2=0.996     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=100000     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data     --init_checkpoint=/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt     --max_pack_factor=3          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.1     --max_samples_termination=1000000000     --eval_iter_start_samples=400000 --eval_iter_samples=200000     --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples=10000     --cache_eval_data     --output_dir=/results     --fp16     --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1     --gradient_accumulation_steps=1     --log_freq=0     --bert_config_path=/wor
21: kspace/phase1/bert_config.json  --local_rank=1  --allreduce_post_accumulation --allreduce_post_accumulation_fp16  --dense_seq_output --pad_fmha --fused_bias_fc --fused_bias_mha --fused_dropout_add --fused_gemm_gelu --packed_samples --cuda_graph_mode 'segmented'   --seed=26427
24: Run vars: id 1339 gpus 2 mparams ''
 9: Run vars: id 1339 gpus 2 mparams ''
19: STARTING TIMING RUN AT 2023-07-30 12:52:26 PM
19: WORLD_SIZE=36
29: Run vars: id 1339 gpus 2 mparams ''
19: slurm_job_nodelist=node[0400-0401,0403-0418]
19: MASTER_ADDR=node0400
19: HOSTNAME=node0400
17: STARTING TIMING RUN AT 2023-07-30 12:52:26 PM
19:      python -u     /workspace/bert/run_pretraining.py         --train_batch_size=36     --learning_rate=0.0006     --opt_lamb_beta_1=0.8     --opt_lamb_beta_2=0.996     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=100000     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data     --init_checkpoint=/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt     --max_pack_factor=3          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.1     --max_samples_termination=1000000000     --eval_iter_start_samples=400000 --eval_iter_samples=200000     --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples=10000     --cache_eval_data     --output_dir=/results     --fp16     --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1     --gradient_accumulation_steps=1     --log_freq=0     --bert_config_path=/wor
19: kspace/phase1/bert_config.json  --local_rank=1  --allreduce_post_accumulation --allreduce_post_accumulation_fp16  --dense_seq_output --pad_fmha --fused_bias_fc --fused_bias_mha --fused_dropout_add --fused_gemm_gelu --packed_samples --cuda_graph_mode 'segmented'   --seed=7919
17: WORLD_SIZE=36
35: Run vars: id 1339 gpus 2 mparams ''
24: STARTING TIMING RUN AT 2023-07-30 12:52:26 PM
24: WORLD_SIZE=36
 9: STARTING TIMING RUN AT 2023-07-30 12:52:26 PM
 9: WORLD_SIZE=36
17: slurm_job_nodelist=node[0400-0401,0403-0418]
17: MASTER_ADDR=node0400
17: HOSTNAME=node0400
17:      python -u     /workspace/bert/run_pretraining.py         --train_batch_size=36     --learning_rate=0.0006     --opt_lamb_beta_1=0.8     --opt_lamb_beta_2=0.996     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=100000     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data     --init_checkpoint=/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt     --max_pack_factor=3          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.1     --max_samples_termination=1000000000     --eval_iter_start_samples=400000 --eval_iter_samples=200000     --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples=10000     --cache_eval_data     --output_dir=/results     --fp16     --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1     --gradient_accumulation_steps=1     --log_freq=0     --bert_config_path=/wor
17: kspace/phase1/bert_config.json  --local_rank=1  --allreduce_post_accumulation --allreduce_post_accumulation_fp16  --dense_seq_output --pad_fmha --fused_bias_fc --fused_bias_mha --fused_dropout_add --fused_gemm_gelu --packed_samples --cuda_graph_mode 'segmented'   --seed=16256
 5: Run vars: id 1339 gpus 2 mparams ''
29: STARTING TIMING RUN AT 2023-07-30 12:52:26 PM
29: WORLD_SIZE=36
24: slurm_job_nodelist=node[0400-0401,0403-0418]
24: MASTER_ADDR=node0400
24: HOSTNAME=node0400
24:      python -u     /workspace/bert/run_pretraining.py         --train_batch_size=36     --learning_rate=0.0006     --opt_lamb_beta_1=0.8     --opt_lamb_beta_2=0.996     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=100000     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data     --init_checkpoint=/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt     --max_pack_factor=3          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.1     --max_samples_termination=1000000000     --eval_iter_start_samples=400000 --eval_iter_samples=200000     --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples=10000     --cache_eval_data     --output_dir=/results     --fp16     --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1     --gradient_accumulation_steps=1     --log_freq=0     --bert_config_path=/wor
24: kspace/phase1/bert_config.json  --local_rank=0  --allreduce_post_accumulation --allreduce_post_accumulation_fp16  --dense_seq_output --pad_fmha --fused_bias_fc --fused_bias_mha --fused_dropout_add --fused_gemm_gelu --packed_samples --cuda_graph_mode 'segmented'   --seed=12319
35: STARTING TIMING RUN AT 2023-07-30 12:52:26 PM
35: WORLD_SIZE=36
 3: Run vars: id 1339 gpus 2 mparams ''
 9: slurm_job_nodelist=node[0400-0401,0403-0418]
 9: MASTER_ADDR=node0400
 9: HOSTNAME=node0400
 9:      python -u     /workspace/bert/run_pretraining.py         --train_batch_size=36     --learning_rate=0.0006     --opt_lamb_beta_1=0.8     --opt_lamb_beta_2=0.996     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=100000     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data     --init_checkpoint=/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt     --max_pack_factor=3          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.1     --max_samples_termination=1000000000     --eval_iter_start_samples=400000 --eval_iter_samples=200000     --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples=10000     --cache_eval_data     --output_dir=/results     --fp16     --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1     --gradient_accumulation_steps=1     --log_freq=0     --bert_config_path=/wor
 9: kspace/phase1/bert_config.json  --local_rank=1  --allreduce_post_accumulation --allreduce_post_accumulation_fp16  --dense_seq_output --pad_fmha --fused_bias_fc --fused_bias_mha --fused_dropout_add --fused_gemm_gelu --packed_samples --cuda_graph_mode 'segmented'   --seed=17537
35: slurm_job_nodelist=node[0400-0401,0403-0418]
35: MASTER_ADDR=node0400
35: HOSTNAME=node0400
35:      python -u     /workspace/bert/run_pretraining.py         --train_batch_size=36     --learning_rate=0.0006     --opt_lamb_beta_1=0.8     --opt_lamb_beta_2=0.996     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=100000     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data     --init_checkpoint=/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt     --max_pack_factor=3          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.1     --max_samples_termination=1000000000     --eval_iter_start_samples=400000 --eval_iter_samples=200000     --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples=10000     --cache_eval_data     --output_dir=/results     --fp16     --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1     --gradient_accumulation_steps=1     --log_freq=0     --bert_config_path=/wor
35: kspace/phase1/bert_config.json  --local_rank=1  --allreduce_post_accumulation --allreduce_post_accumulation_fp16  --dense_seq_output --pad_fmha --fused_bias_fc --fused_bias_mha --fused_dropout_add --fused_gemm_gelu --packed_samples --cuda_graph_mode 'segmented'   --seed=8428
29: slurm_job_nodelist=node[0400-0401,0403-0418]
29: MASTER_ADDR=node0400
29: HOSTNAME=node0400
29:      python -u     /workspace/bert/run_pretraining.py         --train_batch_size=36     --learning_rate=0.0006     --opt_lamb_beta_1=0.8     --opt_lamb_beta_2=0.996     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=100000     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data     --init_checkpoint=/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt     --max_pack_factor=3          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.1     --max_samples_termination=1000000000     --eval_iter_start_samples=400000 --eval_iter_samples=200000     --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples=10000     --cache_eval_data     --output_dir=/results     --fp16     --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1     --gradient_accumulation_steps=1     --log_freq=0     --bert_config_path=/wor
29: kspace/phase1/bert_config.json  --local_rank=1  --allreduce_post_accumulation --allreduce_post_accumulation_fp16  --dense_seq_output --pad_fmha --fused_bias_fc --fused_bias_mha --fused_dropout_add --fused_gemm_gelu --packed_samples --cuda_graph_mode 'segmented'   --seed=3869
 5: STARTING TIMING RUN AT 2023-07-30 12:52:26 PM
 5: WORLD_SIZE=36
22: Run vars: id 1339 gpus 2 mparams ''
 5: slurm_job_nodelist=node[0400-0401,0403-0418]
 5: MASTER_ADDR=node0400
 5: HOSTNAME=node0400
 5:      python -u     /workspace/bert/run_pretraining.py         --train_batch_size=36     --learning_rate=0.0006     --opt_lamb_beta_1=0.8     --opt_lamb_beta_2=0.996     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=100000     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data     --init_checkpoint=/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt     --max_pack_factor=3          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.1     --max_samples_termination=1000000000     --eval_iter_start_samples=400000 --eval_iter_samples=200000     --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples=10000     --cache_eval_data     --output_dir=/results     --fp16     --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1     --gradient_accumulation_steps=1     --log_freq=0     --bert_config_path=/wor
 5: kspace/phase1/bert_config.json  --local_rank=1  --allreduce_post_accumulation --allreduce_post_accumulation_fp16  --dense_seq_output --pad_fmha --fused_bias_fc --fused_bias_mha --fused_dropout_add --fused_gemm_gelu --packed_samples --cuda_graph_mode 'segmented'   --seed=15650
 3: STARTING TIMING RUN AT 2023-07-30 12:52:26 PM
 3: WORLD_SIZE=36
22: STARTING TIMING RUN AT 2023-07-30 12:52:26 PM
22: WORLD_SIZE=36
22: slurm_job_nodelist=node[0400-0401,0403-0418]
22: MASTER_ADDR=node0400
22: HOSTNAME=node0400
22:      python -u     /workspace/bert/run_pretraining.py         --train_batch_size=36     --learning_rate=0.0006     --opt_lamb_beta_1=0.8     --opt_lamb_beta_2=0.996     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=100000     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data     --init_checkpoint=/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt     --max_pack_factor=3          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.1     --max_samples_termination=1000000000     --eval_iter_start_samples=400000 --eval_iter_samples=200000     --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples=10000     --cache_eval_data     --output_dir=/results     --fp16     --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1     --gradient_accumulation_steps=1     --log_freq=0     --bert_config_path=/wor
22: kspace/phase1/bert_config.json  --local_rank=0  --allreduce_post_accumulation --allreduce_post_accumulation_fp16  --dense_seq_output --pad_fmha --fused_bias_fc --fused_bias_mha --fused_dropout_add --fused_gemm_gelu --packed_samples --cuda_graph_mode 'segmented'   --seed=23777
 3: slurm_job_nodelist=node[0400-0401,0403-0418]
 3: MASTER_ADDR=node0400
 3: HOSTNAME=node0400
 3:      python -u     /workspace/bert/run_pretraining.py         --train_batch_size=36     --learning_rate=0.0006     --opt_lamb_beta_1=0.8     --opt_lamb_beta_2=0.996     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=100000     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data     --init_checkpoint=/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt     --max_pack_factor=3          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.1     --max_samples_termination=1000000000     --eval_iter_start_samples=400000 --eval_iter_samples=200000     --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples=10000     --cache_eval_data     --output_dir=/results     --fp16     --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1     --gradient_accumulation_steps=1     --log_freq=0     --bert_config_path=/wor
 3: kspace/phase1/bert_config.json  --local_rank=1  --allreduce_post_accumulation --allreduce_post_accumulation_fp16  --dense_seq_output --pad_fmha --fused_bias_fc --fused_bias_mha --fused_dropout_add --fused_gemm_gelu --packed_samples --cuda_graph_mode 'segmented'   --seed=23471
13: Run vars: id 1339 gpus 2 mparams ''
33: Run vars: id 1339 gpus 2 mparams ''
14: Run vars: id 1339 gpus 2 mparams ''
13: STARTING TIMING RUN AT 2023-07-30 12:52:26 PM
13: WORLD_SIZE=36
14: STARTING TIMING RUN AT 2023-07-30 12:52:26 PM
14: WORLD_SIZE=36
33: STARTING TIMING RUN AT 2023-07-30 12:52:26 PM
33: WORLD_SIZE=36
30: Run vars: id 1339 gpus 2 mparams ''
14: slurm_job_nodelist=node[0400-0401,0403-0418]
14: MASTER_ADDR=node0400
14: HOSTNAME=node0400
14:      python -u     /workspace/bert/run_pretraining.py         --train_batch_size=36     --learning_rate=0.0006     --opt_lamb_beta_1=0.8     --opt_lamb_beta_2=0.996     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=100000     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data     --init_checkpoint=/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt     --max_pack_factor=3          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.1     --max_samples_termination=1000000000     --eval_iter_start_samples=400000 --eval_iter_samples=200000     --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples=10000     --cache_eval_data     --output_dir=/results     --fp16     --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1     --gradient_accumulation_steps=1     --log_freq=0     --bert_config_path=/wor
14: kspace/phase1/bert_config.json  --local_rank=0  --allreduce_post_accumulation --allreduce_post_accumulation_fp16  --dense_seq_output --pad_fmha --fused_bias_fc --fused_bias_mha --fused_dropout_add --fused_gemm_gelu --packed_samples --cuda_graph_mode 'segmented'   --seed=11473
13: slurm_job_nodelist=node[0400-0401,0403-0418]
13: MASTER_ADDR=node0400
13: HOSTNAME=node0400
13:      python -u     /workspace/bert/run_pretraining.py         --train_batch_size=36     --learning_rate=0.0006     --opt_lamb_beta_1=0.8     --opt_lamb_beta_2=0.996     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=100000     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data     --init_checkpoint=/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt     --max_pack_factor=3          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.1     --max_samples_termination=1000000000     --eval_iter_start_samples=400000 --eval_iter_samples=200000     --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples=10000     --cache_eval_data     --output_dir=/results     --fp16     --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1     --gradient_accumulation_steps=1     --log_freq=0     --bert_config_path=/wor
13: kspace/phase1/bert_config.json  --local_rank=1  --allreduce_post_accumulation --allreduce_post_accumulation_fp16  --dense_seq_output --pad_fmha --fused_bias_fc --fused_bias_mha --fused_dropout_add --fused_gemm_gelu --packed_samples --cuda_graph_mode 'segmented'   --seed=2756
33: slurm_job_nodelist=node[0400-0401,0403-0418]
33: MASTER_ADDR=node0400
33: HOSTNAME=node0400
33:      python -u     /workspace/bert/run_pretraining.py         --train_batch_size=36     --learning_rate=0.0006     --opt_lamb_beta_1=0.8     --opt_lamb_beta_2=0.996     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=100000     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data     --init_checkpoint=/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt     --max_pack_factor=3          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.1     --max_samples_termination=1000000000     --eval_iter_start_samples=400000 --eval_iter_samples=200000     --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples=10000     --cache_eval_data     --output_dir=/results     --fp16     --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1     --gradient_accumulation_steps=1     --log_freq=0     --bert_config_path=/wor
33: kspace/phase1/bert_config.json  --local_rank=1  --allreduce_post_accumulation --allreduce_post_accumulation_fp16  --dense_seq_output --pad_fmha --fused_bias_fc --fused_bias_mha --fused_dropout_add --fused_gemm_gelu --packed_samples --cuda_graph_mode 'segmented'   --seed=30789
 7: Run vars: id 1339 gpus 2 mparams ''
20: Run vars: id 1339 gpus 2 mparams ''
28: Run vars: id 1339 gpus 2 mparams ''
30: STARTING TIMING RUN AT 2023-07-30 12:52:26 PM
30: WORLD_SIZE=36
25: Run vars: id 1339 gpus 2 mparams ''
16: Run vars: id 1339 gpus 2 mparams ''
 7: STARTING TIMING RUN AT 2023-07-30 12:52:26 PM
28: STARTING TIMING RUN AT 2023-07-30 12:52:26 PM
 7: WORLD_SIZE=36
28: WORLD_SIZE=36
25: STARTING TIMING RUN AT 2023-07-30 12:52:26 PM
25: WORLD_SIZE=36
30: slurm_job_nodelist=node[0400-0401,0403-0418]
30: MASTER_ADDR=node0400
30: HOSTNAME=node0400
20: STARTING TIMING RUN AT 2023-07-30 12:52:26 PM
20: WORLD_SIZE=36
30:      python -u     /workspace/bert/run_pretraining.py         --train_batch_size=36     --learning_rate=0.0006     --opt_lamb_beta_1=0.8     --opt_lamb_beta_2=0.996     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=100000     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data     --init_checkpoint=/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt     --max_pack_factor=3          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.1     --max_samples_termination=1000000000     --eval_iter_start_samples=400000 --eval_iter_samples=200000     --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples=10000     --cache_eval_data     --output_dir=/results     --fp16     --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1     --gradient_accumulation_steps=1     --log_freq=0     --bert_config_path=/wor
30: kspace/phase1/bert_config.json  --local_rank=0  --allreduce_post_accumulation --allreduce_post_accumulation_fp16  --dense_seq_output --pad_fmha --fused_bias_fc --fused_bias_mha --fused_dropout_add --fused_gemm_gelu --packed_samples --cuda_graph_mode 'segmented'   --seed=2356
26: Run vars: id 1339 gpus 2 mparams ''
16: STARTING TIMING RUN AT 2023-07-30 12:52:26 PM
16: WORLD_SIZE=36
28: slurm_job_nodelist=node[0400-0401,0403-0418]
28: MASTER_ADDR=node0400
28: HOSTNAME=node0400
28:      python -u     /workspace/bert/run_pretraining.py         --train_batch_size=36     --learning_rate=0.0006     --opt_lamb_beta_1=0.8     --opt_lamb_beta_2=0.996     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=100000     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data     --init_checkpoint=/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt     --max_pack_factor=3          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.1     --max_samples_termination=1000000000     --eval_iter_start_samples=400000 --eval_iter_samples=200000     --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples=10000     --cache_eval_data     --output_dir=/results     --fp16     --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1     --gradient_accumulation_steps=1     --log_freq=0     --bert_config_path=/wor
20: slurm_job_nodelist=node[0400-0401,0403-0418]
20: MASTER_ADDR=node0400
20: HOSTNAME=node0400
28: kspace/phase1/bert_config.json  --local_rank=0  --allreduce_post_accumulation --allreduce_post_accumulation_fp16  --dense_seq_output --pad_fmha --fused_bias_fc --fused_bias_mha --fused_dropout_add --fused_gemm_gelu --packed_samples --cuda_graph_mode 'segmented'   --seed=18212
25: slurm_job_nodelist=node[0400-0401,0403-0418]
25: MASTER_ADDR=node0400
25: HOSTNAME=node0400
25:      python -u     /workspace/bert/run_pretraining.py         --train_batch_size=36     --learning_rate=0.0006     --opt_lamb_beta_1=0.8     --opt_lamb_beta_2=0.996     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=100000     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data     --init_checkpoint=/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt     --max_pack_factor=3          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.1     --max_samples_termination=1000000000     --eval_iter_start_samples=400000 --eval_iter_samples=200000     --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples=10000     --cache_eval_data     --output_dir=/results     --fp16     --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1     --gradient_accumulation_steps=1     --log_freq=0     --bert_config_path=/wor
20:      python -u     /workspace/bert/run_pretraining.py         --train_batch_size=36     --learning_rate=0.0006     --opt_lamb_beta_1=0.8     --opt_lamb_beta_2=0.996     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=100000     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data     --init_checkpoint=/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt     --max_pack_factor=3          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.1     --max_samples_termination=1000000000     --eval_iter_start_samples=400000 --eval_iter_samples=200000     --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples=10000     --cache_eval_data     --output_dir=/results     --fp16     --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1     --gradient_accumulation_steps=1     --log_freq=0     --bert_config_path=/wor
25: kspace/phase1/bert_config.json  --local_rank=1  --allreduce_post_accumulation --allreduce_post_accumulation_fp16  --dense_seq_output --pad_fmha --fused_bias_fc --fused_bias_mha --fused_dropout_add --fused_gemm_gelu --packed_samples --cuda_graph_mode 'segmented'   --seed=30634
20: kspace/phase1/bert_config.json  --local_rank=0  --allreduce_post_accumulation --allreduce_post_accumulation_fp16  --dense_seq_output --pad_fmha --fused_bias_fc --fused_bias_mha --fused_dropout_add --fused_gemm_gelu --packed_samples --cuda_graph_mode 'segmented'   --seed=6494
18: Run vars: id 1339 gpus 2 mparams ''
16: slurm_job_nodelist=node[0400-0401,0403-0418]
16: MASTER_ADDR=node0400
16: HOSTNAME=node0400
16:      python -u     /workspace/bert/run_pretraining.py         --train_batch_size=36     --learning_rate=0.0006     --opt_lamb_beta_1=0.8     --opt_lamb_beta_2=0.996     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=100000     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data     --init_checkpoint=/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt     --max_pack_factor=3          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.1     --max_samples_termination=1000000000     --eval_iter_start_samples=400000 --eval_iter_samples=200000     --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples=10000     --cache_eval_data     --output_dir=/results     --fp16     --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1     --gradient_accumulation_steps=1     --log_freq=0     --bert_config_path=/wor
16: kspace/phase1/bert_config.json  --local_rank=0  --allreduce_post_accumulation --allreduce_post_accumulation_fp16  --dense_seq_output --pad_fmha --fused_bias_fc --fused_bias_mha --fused_dropout_add --fused_gemm_gelu --packed_samples --cuda_graph_mode 'segmented'   --seed=16527
 7: slurm_job_nodelist=node[0400-0401,0403-0418]
 7: MASTER_ADDR=node0400
 7: HOSTNAME=node0400
 7:      python -u     /workspace/bert/run_pretraining.py         --train_batch_size=36     --learning_rate=0.0006     --opt_lamb_beta_1=0.8     --opt_lamb_beta_2=0.996     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=100000     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data     --init_checkpoint=/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt     --max_pack_factor=3          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.1     --max_samples_termination=1000000000     --eval_iter_start_samples=400000 --eval_iter_samples=200000     --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples=10000     --cache_eval_data     --output_dir=/results     --fp16     --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1     --gradient_accumulation_steps=1     --log_freq=0     --bert_config_path=/wor
 7: kspace/phase1/bert_config.json  --local_rank=1  --allreduce_post_accumulation --allreduce_post_accumulation_fp16  --dense_seq_output --pad_fmha --fused_bias_fc --fused_bias_mha --fused_dropout_add --fused_gemm_gelu --packed_samples --cuda_graph_mode 'segmented'   --seed=24546
34: Run vars: id 1339 gpus 2 mparams ''
26: STARTING TIMING RUN AT 2023-07-30 12:52:26 PM
26: WORLD_SIZE=36
 4: Run vars: id 1339 gpus 2 mparams ''
 8: Run vars: id 1339 gpus 2 mparams ''
11: Run vars: id 1339 gpus 2 mparams ''
34: STARTING TIMING RUN AT 2023-07-30 12:52:26 PM
34: WORLD_SIZE=36
18: STARTING TIMING RUN AT 2023-07-30 12:52:26 PM
18: WORLD_SIZE=36
26: slurm_job_nodelist=node[0400-0401,0403-0418]
26: MASTER_ADDR=node0400
26: HOSTNAME=node0400
26:      python -u     /workspace/bert/run_pretraining.py         --train_batch_size=36     --learning_rate=0.0006     --opt_lamb_beta_1=0.8     --opt_lamb_beta_2=0.996     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=100000     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data     --init_checkpoint=/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt     --max_pack_factor=3          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.1     --max_samples_termination=1000000000     --eval_iter_start_samples=400000 --eval_iter_samples=200000     --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples=10000     --cache_eval_data     --output_dir=/results     --fp16     --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1     --gradient_accumulation_steps=1     --log_freq=0     --bert_config_path=/wor
26: kspace/phase1/bert_config.json  --local_rank=0  --allreduce_post_accumulation --allreduce_post_accumulation_fp16  --dense_seq_output --pad_fmha --fused_bias_fc --fused_bias_mha --fused_dropout_add --fused_gemm_gelu --packed_samples --cuda_graph_mode 'segmented'   --seed=9854
18: slurm_job_nodelist=node[0400-0401,0403-0418]
18: MASTER_ADDR=node0400
18: HOSTNAME=node0400
18:      python -u     /workspace/bert/run_pretraining.py         --train_batch_size=36     --learning_rate=0.0006     --opt_lamb_beta_1=0.8     --opt_lamb_beta_2=0.996     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=100000     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data     --init_checkpoint=/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt     --max_pack_factor=3          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.1     --max_samples_termination=1000000000     --eval_iter_start_samples=400000 --eval_iter_samples=200000     --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples=10000     --cache_eval_data     --output_dir=/results     --fp16     --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1     --gradient_accumulation_steps=1     --log_freq=0     --bert_config_path=/wor
18: kspace/phase1/bert_config.json  --local_rank=0  --allreduce_post_accumulation --allreduce_post_accumulation_fp16  --dense_seq_output --pad_fmha --fused_bias_fc --fused_bias_mha --fused_dropout_add --fused_gemm_gelu --packed_samples --cuda_graph_mode 'segmented'   --seed=30340
 8: STARTING TIMING RUN AT 2023-07-30 12:52:26 PM
 8: WORLD_SIZE=36
11: STARTING TIMING RUN AT 2023-07-30 12:52:26 PM
11: WORLD_SIZE=36
 4: STARTING TIMING RUN AT 2023-07-30 12:52:26 PM
 4: WORLD_SIZE=36
34: slurm_job_nodelist=node[0400-0401,0403-0418]
34: MASTER_ADDR=node0400
34: HOSTNAME=node0400
34:      python -u     /workspace/bert/run_pretraining.py         --train_batch_size=36     --learning_rate=0.0006     --opt_lamb_beta_1=0.8     --opt_lamb_beta_2=0.996     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=100000     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data     --init_checkpoint=/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt     --max_pack_factor=3          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.1     --max_samples_termination=1000000000     --eval_iter_start_samples=400000 --eval_iter_samples=200000     --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples=10000     --cache_eval_data     --output_dir=/results     --fp16     --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1     --gradient_accumulation_steps=1     --log_freq=0     --bert_config_path=/wor
34: kspace/phase1/bert_config.json  --local_rank=0  --allreduce_post_accumulation --allreduce_post_accumulation_fp16  --dense_seq_output --pad_fmha --fused_bias_fc --fused_bias_mha --fused_dropout_add --fused_gemm_gelu --packed_samples --cuda_graph_mode 'segmented'   --seed=7031
 8: slurm_job_nodelist=node[0400-0401,0403-0418]
 8: MASTER_ADDR=node0400
11: slurm_job_nodelist=node[0400-0401,0403-0418]
11: MASTER_ADDR=node0400
 8: HOSTNAME=node0400
11: HOSTNAME=node0400
 8:      python -u     /workspace/bert/run_pretraining.py         --train_batch_size=36     --learning_rate=0.0006     --opt_lamb_beta_1=0.8     --opt_lamb_beta_2=0.996     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=100000     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data     --init_checkpoint=/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt     --max_pack_factor=3          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.1     --max_samples_termination=1000000000     --eval_iter_start_samples=400000 --eval_iter_samples=200000     --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples=10000     --cache_eval_data     --output_dir=/results     --fp16     --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1     --gradient_accumulation_steps=1     --log_freq=0     --bert_config_path=/wor
11:      python -u     /workspace/bert/run_pretraining.py         --train_batch_size=36     --learning_rate=0.0006     --opt_lamb_beta_1=0.8     --opt_lamb_beta_2=0.996     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=100000     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data     --init_checkpoint=/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt     --max_pack_factor=3          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.1     --max_samples_termination=1000000000     --eval_iter_start_samples=400000 --eval_iter_samples=200000     --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples=10000     --cache_eval_data     --output_dir=/results     --fp16     --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1     --gradient_accumulation_steps=1     --log_freq=0     --bert_config_path=/wor
 8: kspace/phase1/bert_config.json  --local_rank=0  --allreduce_post_accumulation --allreduce_post_accumulation_fp16  --dense_seq_output --pad_fmha --fused_bias_fc --fused_bias_mha --fused_dropout_add --fused_gemm_gelu --packed_samples --cuda_graph_mode 'segmented'   --seed=20170
11: kspace/phase1/bert_config.json  --local_rank=1  --allreduce_post_accumulation --allreduce_post_accumulation_fp16  --dense_seq_output --pad_fmha --fused_bias_fc --fused_bias_mha --fused_dropout_add --fused_gemm_gelu --packed_samples --cuda_graph_mode 'segmented'   --seed=6978
23: Run vars: id 1339 gpus 2 mparams ''
 4: slurm_job_nodelist=node[0400-0401,0403-0418]
 4: MASTER_ADDR=node0400
 4: HOSTNAME=node0400
 4:      python -u     /workspace/bert/run_pretraining.py         --train_batch_size=36     --learning_rate=0.0006     --opt_lamb_beta_1=0.8     --opt_lamb_beta_2=0.996     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=100000     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data     --init_checkpoint=/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt     --max_pack_factor=3          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.1     --max_samples_termination=1000000000     --eval_iter_start_samples=400000 --eval_iter_samples=200000     --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples=10000     --cache_eval_data     --output_dir=/results     --fp16     --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1     --gradient_accumulation_steps=1     --log_freq=0     --bert_config_path=/wor
 4: kspace/phase1/bert_config.json  --local_rank=0  --allreduce_post_accumulation --allreduce_post_accumulation_fp16  --dense_seq_output --pad_fmha --fused_bias_fc --fused_bias_mha --fused_dropout_add --fused_gemm_gelu --packed_samples --cuda_graph_mode 'segmented'   --seed=23253
 2: Run vars: id 1339 gpus 2 mparams ''
23: STARTING TIMING RUN AT 2023-07-30 12:52:26 PM
23: WORLD_SIZE=36
32: Run vars: id 1339 gpus 2 mparams ''
31: Run vars: id 1339 gpus 2 mparams ''
 2: STARTING TIMING RUN AT 2023-07-30 12:52:26 PM
 2: WORLD_SIZE=36
23: slurm_job_nodelist=node[0400-0401,0403-0418]
23: MASTER_ADDR=node0400
23: HOSTNAME=node0400
23:      python -u     /workspace/bert/run_pretraining.py         --train_batch_size=36     --learning_rate=0.0006     --opt_lamb_beta_1=0.8     --opt_lamb_beta_2=0.996     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=100000     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data     --init_checkpoint=/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt     --max_pack_factor=3          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.1     --max_samples_termination=1000000000     --eval_iter_start_samples=400000 --eval_iter_samples=200000     --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples=10000     --cache_eval_data     --output_dir=/results     --fp16     --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1     --gradient_accumulation_steps=1     --log_freq=0     --bert_config_path=/wor
23: kspace/phase1/bert_config.json  --local_rank=1  --allreduce_post_accumulation --allreduce_post_accumulation_fp16  --dense_seq_output --pad_fmha --fused_bias_fc --fused_bias_mha --fused_dropout_add --fused_gemm_gelu --packed_samples --cuda_graph_mode 'segmented'   --seed=22358
32: STARTING TIMING RUN AT 2023-07-30 12:52:26 PM
32: WORLD_SIZE=36
 2: slurm_job_nodelist=node[0400-0401,0403-0418]
 2: MASTER_ADDR=node0400
 2: HOSTNAME=node0400
 2:      python -u     /workspace/bert/run_pretraining.py         --train_batch_size=36     --learning_rate=0.0006     --opt_lamb_beta_1=0.8     --opt_lamb_beta_2=0.996     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=100000     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data     --init_checkpoint=/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt     --max_pack_factor=3          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.1     --max_samples_termination=1000000000     --eval_iter_start_samples=400000 --eval_iter_samples=200000     --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples=10000     --cache_eval_data     --output_dir=/results     --fp16     --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1     --gradient_accumulation_steps=1     --log_freq=0     --bert_config_path=/wor
 2: kspace/phase1/bert_config.json  --local_rank=0  --allreduce_post_accumulation --allreduce_post_accumulation_fp16  --dense_seq_output --pad_fmha --fused_bias_fc --fused_bias_mha --fused_dropout_add --fused_gemm_gelu --packed_samples --cuda_graph_mode 'segmented'   --seed=1202
31: STARTING TIMING RUN AT 2023-07-30 12:52:26 PM
31: WORLD_SIZE=36
32: slurm_job_nodelist=node[0400-0401,0403-0418]
32: MASTER_ADDR=node0400
32: HOSTNAME=node0400
32:      python -u     /workspace/bert/run_pretraining.py         --train_batch_size=36     --learning_rate=0.0006     --opt_lamb_beta_1=0.8     --opt_lamb_beta_2=0.996     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=100000     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data     --init_checkpoint=/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt     --max_pack_factor=3          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.1     --max_samples_termination=1000000000     --eval_iter_start_samples=400000 --eval_iter_samples=200000     --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples=10000     --cache_eval_data     --output_dir=/results     --fp16     --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1     --gradient_accumulation_steps=1     --log_freq=0     --bert_config_path=/wor
32: kspace/phase1/bert_config.json  --local_rank=0  --allreduce_post_accumulation --allreduce_post_accumulation_fp16  --dense_seq_output --pad_fmha --fused_bias_fc --fused_bias_mha --fused_dropout_add --fused_gemm_gelu --packed_samples --cuda_graph_mode 'segmented'   --seed=8653
31: slurm_job_nodelist=node[0400-0401,0403-0418]
31: MASTER_ADDR=node0400
31: HOSTNAME=node0400
31:      python -u     /workspace/bert/run_pretraining.py         --train_batch_size=36     --learning_rate=0.0006     --opt_lamb_beta_1=0.8     --opt_lamb_beta_2=0.996     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=100000     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data     --init_checkpoint=/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt     --max_pack_factor=3          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.1     --max_samples_termination=1000000000     --eval_iter_start_samples=400000 --eval_iter_samples=200000     --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples=10000     --cache_eval_data     --output_dir=/results     --fp16     --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1     --gradient_accumulation_steps=1     --log_freq=0     --bert_config_path=/wor
31: kspace/phase1/bert_config.json  --local_rank=1  --allreduce_post_accumulation --allreduce_post_accumulation_fp16  --dense_seq_output --pad_fmha --fused_bias_fc --fused_bias_mha --fused_dropout_add --fused_gemm_gelu --packed_samples --cuda_graph_mode 'segmented'   --seed=14900
27: Run vars: id 1339 gpus 2 mparams ''
27: STARTING TIMING RUN AT 2023-07-30 12:52:26 PM
27: WORLD_SIZE=36
27: slurm_job_nodelist=node[0400-0401,0403-0418]
27: MASTER_ADDR=node0400
27: HOSTNAME=node0400
27:      python -u     /workspace/bert/run_pretraining.py         --train_batch_size=36     --learning_rate=0.0006     --opt_lamb_beta_1=0.8     --opt_lamb_beta_2=0.996     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=100000     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data     --init_checkpoint=/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt     --max_pack_factor=3          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.1     --max_samples_termination=1000000000     --eval_iter_start_samples=400000 --eval_iter_samples=200000     --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples=10000     --cache_eval_data     --output_dir=/results     --fp16     --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1     --gradient_accumulation_steps=1     --log_freq=0     --bert_config_path=/wor
27: kspace/phase1/bert_config.json  --local_rank=1  --allreduce_post_accumulation --allreduce_post_accumulation_fp16  --dense_seq_output --pad_fmha --fused_bias_fc --fused_bias_mha --fused_dropout_add --fused_gemm_gelu --packed_samples --cuda_graph_mode 'segmented'   --seed=3899
10: Run vars: id 1339 gpus 2 mparams ''
10: STARTING TIMING RUN AT 2023-07-30 12:52:26 PM
10: WORLD_SIZE=36
10: slurm_job_nodelist=node[0400-0401,0403-0418]
10: MASTER_ADDR=node0400
10: HOSTNAME=node0400
10:      python -u     /workspace/bert/run_pretraining.py         --train_batch_size=36     --learning_rate=0.0006     --opt_lamb_beta_1=0.8     --opt_lamb_beta_2=0.996     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=100000     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data     --init_checkpoint=/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt     --max_pack_factor=3          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.1     --max_samples_termination=1000000000     --eval_iter_start_samples=400000 --eval_iter_samples=200000     --eval_batch_size=16 --eval_dir=/workspace/evaldata --num_eval_examples=10000     --cache_eval_data     --output_dir=/results     --fp16     --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-ag-pg=1 --dwu-num-blocks=1     --gradient_accumulation_steps=1     --log_freq=0     --bert_config_path=/wor
10: kspace/phase1/bert_config.json  --local_rank=0  --allreduce_post_accumulation --allreduce_post_accumulation_fp16  --dense_seq_output --pad_fmha --fused_bias_fc --fused_bias_mha --fused_dropout_add --fused_gemm_gelu --packed_samples --cuda_graph_mode 'segmented'   --seed=15423
 0: /usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 0:   warnings.warn(msg, DeprecatedFeatureWarning)
 1: /usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 1:   warnings.warn(msg, DeprecatedFeatureWarning)
15: /usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
15:   warnings.warn(msg, DeprecatedFeatureWarning)
14: /usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
14:   warnings.warn(msg, DeprecatedFeatureWarning)
20: /usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
20:   warnings.warn(msg, DeprecatedFeatureWarning)
21: /usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
21:   warnings.warn(msg, DeprecatedFeatureWarning)
29: /usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
29:   warnings.warn(msg, DeprecatedFeatureWarning)
28: /usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
28:   warnings.warn(msg, DeprecatedFeatureWarning)
 6: /usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 6:   warnings.warn(msg, DeprecatedFeatureWarning)
 7: /usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 7:   warnings.warn(msg, DeprecatedFeatureWarning)
16: /usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
16:   warnings.warn(msg, DeprecatedFeatureWarning)
17: /usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
17:   warnings.warn(msg, DeprecatedFeatureWarning)
18: /usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
18:   warnings.warn(msg, DeprecatedFeatureWarning)
19: /usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
19:   warnings.warn(msg, DeprecatedFeatureWarning)
34: /usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
34:   warnings.warn(msg, DeprecatedFeatureWarning)
35: /usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
35:   warnings.warn(msg, DeprecatedFeatureWarning)
24: /usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
24:   warnings.warn(msg, DeprecatedFeatureWarning)
25: /usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
25:   warnings.warn(msg, DeprecatedFeatureWarning)
12: /usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
12:   warnings.warn(msg, DeprecatedFeatureWarning)
13: /usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
13:   warnings.warn(msg, DeprecatedFeatureWarning)
23: /usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
23:   warnings.warn(msg, DeprecatedFeatureWarning)
22: /usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
22:   warnings.warn(msg, DeprecatedFeatureWarning)
 4: /usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 4:   warnings.warn(msg, DeprecatedFeatureWarning)
 5: /usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 5:   warnings.warn(msg, DeprecatedFeatureWarning)
11: /usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
11:   warnings.warn(msg, DeprecatedFeatureWarning)
10: /usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
10:   warnings.warn(msg, DeprecatedFeatureWarning)
33: /usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
33:   warnings.warn(msg, DeprecatedFeatureWarning)
32: /usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
32:   warnings.warn(msg, DeprecatedFeatureWarning)
30: /usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
30:   warnings.warn(msg, DeprecatedFeatureWarning)
31: /usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
31:   warnings.warn(msg, DeprecatedFeatureWarning)
 2: /usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 2:   warnings.warn(msg, DeprecatedFeatureWarning)
 3: /usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 3:   warnings.warn(msg, DeprecatedFeatureWarning)
26: /usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
26:   warnings.warn(msg, DeprecatedFeatureWarning)
27: /usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
27:   warnings.warn(msg, DeprecatedFeatureWarning)
 8: /usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 8:   warnings.warn(msg, DeprecatedFeatureWarning)
 9: /usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 9:   warnings.warn(msg, DeprecatedFeatureWarning)
 0: :::MLLOG {"namespace": "", "time_ms": 1690735952369, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1263}}
 1: :::MLLOG {"namespace": "", "time_ms": 1690735952369, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1263}}
21: :::MLLOG {"namespace": "", "time_ms": 1690735952386, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1263}}
20: :::MLLOG {"namespace": "", "time_ms": 1690735952386, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1263}}
17: :::MLLOG {"namespace": "", "time_ms": 1690735952614, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1263}}
15: :::MLLOG {"namespace": "", "time_ms": 1690735952630, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1263}}
16: :::MLLOG {"namespace": "", "time_ms": 1690735952616, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1263}}
14: :::MLLOG {"namespace": "", "time_ms": 1690735952632, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1263}}
19: :::MLLOG {"namespace": "", "time_ms": 1690735952724, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1263}}
18: :::MLLOG {"namespace": "", "time_ms": 1690735952724, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1263}}
13: :::MLLOG {"namespace": "", "time_ms": 1690735952749, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1263}}
12: :::MLLOG {"namespace": "", "time_ms": 1690735952749, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1263}}
34: :::MLLOG {"namespace": "", "time_ms": 1690735952764, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1263}}
35: :::MLLOG {"namespace": "", "time_ms": 1690735952765, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1263}}
 6: :::MLLOG {"namespace": "", "time_ms": 1690735952776, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1263}}
 7: :::MLLOG {"namespace": "", "time_ms": 1690735952776, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1263}}
29: :::MLLOG {"namespace": "", "time_ms": 1690735952780, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1263}}
28: :::MLLOG {"namespace": "", "time_ms": 1690735952780, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1263}}
23: :::MLLOG {"namespace": "", "time_ms": 1690735952801, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1263}}
22: :::MLLOG {"namespace": "", "time_ms": 1690735952802, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1263}}
11: :::MLLOG {"namespace": "", "time_ms": 1690735952826, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1263}}
10: :::MLLOG {"namespace": "", "time_ms": 1690735952826, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1263}}
32: :::MLLOG {"namespace": "", "time_ms": 1690735952864, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1263}}
33: :::MLLOG {"namespace": "", "time_ms": 1690735952864, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1263}}
24: :::MLLOG {"namespace": "", "time_ms": 1690735952958, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1263}}
25: :::MLLOG {"namespace": "", "time_ms": 1690735952958, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1263}}
 4: :::MLLOG {"namespace": "", "time_ms": 1690735952983, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1263}}
 5: :::MLLOG {"namespace": "", "time_ms": 1690735952984, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1263}}
30: :::MLLOG {"namespace": "", "time_ms": 1690735953069, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1263}}
31: :::MLLOG {"namespace": "", "time_ms": 1690735953069, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1263}}
26: :::MLLOG {"namespace": "", "time_ms": 1690735953116, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1263}}
27: :::MLLOG {"namespace": "", "time_ms": 1690735953116, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1263}}
 9: :::MLLOG {"namespace": "", "time_ms": 1690735953134, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1263}}
 8: :::MLLOG {"namespace": "", "time_ms": 1690735953134, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1263}}
 3: :::MLLOG {"namespace": "", "time_ms": 1690735953220, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1263}}
 2: :::MLLOG {"namespace": "", "time_ms": 1690735953220, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1263}}
 0: device: cuda:0 n_gpu: 36, distributed training: True, 16-bits training: True
 0: :::MLLOG {"namespace": "", "time_ms": 1690735953647, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "bert", "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735953647, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "Clemson Research Computing and Data", "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735953647, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735953647, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 4: device: cuda:0 n_gpu: 36, distributed training: True, 16-bits training: True
 0: :::MLLOG {"namespace": "", "time_ms": 1690735953647, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "18x1xR750xax2A100-80GB-PCIe", "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735953647, "event_type": "POINT_IN_TIME", "key": "seed", "value": 29586, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1272}}
15: device: cuda:1 n_gpu: 36, distributed training: True, 16-bits training: True
 0: :::MLLOG {"namespace": "", "time_ms": 1690735953647, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 2592, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1274}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735953647, "event_type": "POINT_IN_TIME", "key": "d_batch_size", "value": 36, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1276}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735953647, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1278}}
11: device: cuda:1 n_gpu: 36, distributed training: True, 16-bits training: True
 0: :::MLLOG {"namespace": "", "time_ms": 1690735953647, "event_type": "POINT_IN_TIME", "key": "max_predictions_per_seq", "value": 76, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1280}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735953648, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_training_steps", "value": 100000.0, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1282}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735953648, "event_type": "POINT_IN_TIME", "key": "num_warmup_steps", "value": 0, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1284}}
 0: parsed args:
32: device: cuda:0 n_gpu: 36, distributed training: True, 16-bits training: True
 0: Namespace(allreduce_post_accumulation=True, allreduce_post_accumulation_fp16=True, average_packing_rate=2, bert_config_path='/workspace/phase1/bert_config.json', bert_model='bert-large-uncased', bypass_amp=False, cache_eval_data=True, checkpoint_activations=False, cuda_graph_mode='segmented', ddp_type='apex', dense_seq_output=True, device=device(type='cuda', index=0), disable_apex_softmax=False, disable_fuse_mask=False, disable_fuse_qkv=False, disable_fuse_scale=False, distributed_lamb=True, do_train=True, dwu_e5m2_allgather=False, dwu_group_size=0, dwu_num_ag_pg=1, dwu_num_ar_pg=1, dwu_num_blocks=1, dwu_num_chunks=1, dwu_num_rs_pg=1, dwu_overlap_reductions=False, enable_fuse_dropout=False, enable_stream=False, eval_batch_size=16, eval_dir='/workspace/evaldata', eval_iter_samples=200000, eval_iter_start_samples=400000, exchange_padding=False, fp16=True, fused_bias_fc=True, fused_bias_fc_loss_head=False, fused_bias_mha=True, fused_dropout_add=True, fused_gelu_bias=False, fused_gemm_gelu=True, fused_mha=False, 
 0: gradient_accumulation_steps=1, init_checkpoint='/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt', init_tf_checkpoint=None, input_dir='/workspace/data', keep_n_most_recent_checkpoints=20, learning_rate=0.0006, local_rank=0, log_freq=0.0, loss_scale=0.0, max_iterations_per_graph=4, max_pack_factor=3, max_predictions_per_seq=76, max_samples_termination=1000000000.0, max_seq_length=512, max_steps=100000.0, min_samples_to_start_checkpoints=3000000, n_gpu=36, num_epochs_to_generate_seeds_for=2, num_eval_examples=10000, num_samples_per_checkpoint=500000, opt_lamb_beta_1=0.8, opt_lamb_beta_2=0.996, order_samples=False, output_dir='/results', packed_samples=True, pad=False, pad_fmha=True, phase2=True, resume_from_checkpoint=False, seed=29586, skip_checkpoint=True, start_warmup_step=0.0, synthetic_input=False, target_mlm_accuracy=0.72, train_batch_size=36, train_mlm_accuracy_window_size=0, unpad=False, unpad_fmha=False, use_cuda_graph=False, use_ddp=False, use_env=False, use_gradient_as_bucket_view=False, use_transforme
 5: device: cuda:1 n_gpu: 36, distributed training: True, 16-bits training: True
 0: r_engine2=False, warmup_proportion=0.0, warmup_steps=0.0, weight_decay_rate=0.1)
27: device: cuda:1 n_gpu: 36, distributed training: True, 16-bits training: True
13: device: cuda:1 n_gpu: 36, distributed training: True, 16-bits training: True
35: device: cuda:1 n_gpu: 36, distributed training: True, 16-bits training: True
23: device: cuda:1 n_gpu: 36, distributed training: True, 16-bits training: True
30: device: cuda:0 n_gpu: 36, distributed training: True, 16-bits training: True
17: device: cuda:1 n_gpu: 36, distributed training: True, 16-bits training: True
34: device: cuda:0 n_gpu: 36, distributed training: True, 16-bits training: True
10: device: cuda:0 n_gpu: 36, distributed training: True, 16-bits training: True
31: device: cuda:1 n_gpu: 36, distributed training: True, 16-bits training: True
22: device: cuda:0 n_gpu: 36, distributed training: True, 16-bits training: True
29: device: cuda:1 n_gpu: 36, distributed training: True, 16-bits training: True
25: device: cuda:1 n_gpu: 36, distributed training: True, 16-bits training: True
24: device: cuda:0 n_gpu: 36, distributed training: True, 16-bits training: True
33: device: cuda:1 n_gpu: 36, distributed training: True, 16-bits training: True
19: device: cuda:1 n_gpu: 36, distributed training: True, 16-bits training: True
 7: device: cuda:1 n_gpu: 36, distributed training: True, 16-bits training: True
28: device: cuda:0 n_gpu: 36, distributed training: True, 16-bits training: True
 2: device: cuda:0 n_gpu: 36, distributed training: True, 16-bits training: True
 9: device: cuda:1 n_gpu: 36, distributed training: True, 16-bits training: True
20: device: cuda:0 n_gpu: 36, distributed training: True, 16-bits training: True
26: device: cuda:0 n_gpu: 36, distributed training: True, 16-bits training: True
 3: device: cuda:1 n_gpu: 36, distributed training: True, 16-bits training: True
12: device: cuda:0 n_gpu: 36, distributed training: True, 16-bits training: True
21: device: cuda:1 n_gpu: 36, distributed training: True, 16-bits training: True
 1: device: cuda:1 n_gpu: 36, distributed training: True, 16-bits training: True
 8: device: cuda:0 n_gpu: 36, distributed training: True, 16-bits training: True
 6: device: cuda:0 n_gpu: 36, distributed training: True, 16-bits training: True
14: device: cuda:0 n_gpu: 36, distributed training: True, 16-bits training: True
18: device: cuda:0 n_gpu: 36, distributed training: True, 16-bits training: True
16: device: cuda:0 n_gpu: 36, distributed training: True, 16-bits training: True
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957075, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/embeddings/word_embeddings"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957076, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/embeddings/position_embeddings"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957076, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/embeddings/token_type_embeddings"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957076, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/embeddings/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957076, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/embeddings/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957076, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_0/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957076, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_0/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957076, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_0/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957076, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_0/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957076, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_0/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957077, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_0/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957077, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_0/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957077, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_0/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957077, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_0/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957077, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_0/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957077, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_0/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957077, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_0/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957077, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_0/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957077, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_0/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957077, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_0/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957077, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_0/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957077, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_1/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957077, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_1/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957077, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_1/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957077, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_1/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957077, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_1/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957077, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_1/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957077, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_1/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957078, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_1/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957078, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_1/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957078, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_1/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957078, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_1/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957078, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_1/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957078, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_1/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957078, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_1/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957078, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_1/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957078, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_1/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957078, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_2/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957078, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_2/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957078, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_2/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957078, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_2/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957078, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_2/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957078, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_2/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957078, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_2/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957078, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_2/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957078, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_2/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957078, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_2/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957079, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_2/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957079, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_2/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957079, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_2/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957079, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_2/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957079, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_2/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957079, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_2/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957079, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_3/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957079, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_3/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957079, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_3/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957079, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_3/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957079, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_3/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957079, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_3/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957079, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_3/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957079, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_3/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957079, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_3/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957079, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_3/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957079, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_3/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957079, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_3/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957079, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_3/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957080, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_3/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957080, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_3/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957080, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_3/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957080, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_4/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957080, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_4/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957080, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_4/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957080, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_4/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957080, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_4/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957080, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_4/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957080, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_4/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957080, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_4/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957080, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_4/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957080, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_4/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957080, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_4/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957080, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_4/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957080, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_4/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957080, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_4/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957080, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_4/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957081, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_4/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957081, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_5/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957081, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_5/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957081, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_5/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957081, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_5/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957081, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_5/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957081, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_5/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957081, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_5/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957081, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_5/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957081, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_5/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957081, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_5/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957081, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_5/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957081, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_5/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957081, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_5/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957081, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_5/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957081, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_5/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957081, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_5/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957081, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_6/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957081, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_6/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957082, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_6/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957082, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_6/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957082, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_6/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957082, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_6/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957082, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_6/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957082, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_6/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957082, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_6/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957082, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_6/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957082, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_6/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957082, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_6/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957082, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_6/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957082, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_6/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957082, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_6/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957082, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_6/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957082, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_7/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957082, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_7/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957082, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_7/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957082, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_7/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957082, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_7/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957083, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_7/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957083, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_7/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957083, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_7/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957083, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_7/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957083, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_7/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957083, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_7/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957083, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_7/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957083, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_7/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957083, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_7/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957083, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_7/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957083, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_7/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957083, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_8/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957083, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_8/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957083, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_8/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957083, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_8/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957083, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_8/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957083, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_8/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957083, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_8/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957084, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_8/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957084, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_8/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957084, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_8/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957084, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_8/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957084, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_8/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957084, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_8/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957084, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_8/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957084, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_8/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957084, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_8/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957084, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_9/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957084, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_9/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957084, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_9/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957084, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_9/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957084, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_9/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957084, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_9/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957084, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_9/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957084, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_9/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957084, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_9/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957084, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_9/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957085, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_9/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957085, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_9/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957085, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_9/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957085, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_9/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957085, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_9/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957085, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_9/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957085, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_10/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957085, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_10/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957085, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_10/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957085, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_10/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957085, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_10/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957085, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_10/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957085, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_10/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957085, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_10/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957085, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_10/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957085, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_10/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957085, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_10/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957085, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_10/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957085, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_10/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957086, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_10/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957086, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_10/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957086, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_10/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957086, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_11/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957086, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_11/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957086, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_11/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957086, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_11/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957086, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_11/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957086, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_11/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957086, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_11/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957086, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_11/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957086, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_11/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957086, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_11/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957086, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_11/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957086, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_11/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957086, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_11/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957086, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_11/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957086, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_11/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957087, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_11/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957087, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_12/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957087, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_12/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957087, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_12/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957087, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_12/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957087, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_12/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957087, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_12/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957087, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_12/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957087, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_12/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957087, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_12/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957087, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_12/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957087, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_12/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957087, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_12/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957087, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_12/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957087, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_12/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957087, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_12/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957087, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_12/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957087, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_13/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957087, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_13/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957088, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_13/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957088, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_13/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957088, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_13/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957088, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_13/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957088, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_13/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957088, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_13/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957088, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_13/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957088, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_13/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957088, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_13/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957088, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_13/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957088, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_13/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957088, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_13/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957088, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_13/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957088, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_13/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957088, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_14/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957088, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_14/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957088, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_14/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957088, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_14/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957089, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_14/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957089, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_14/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957089, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_14/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957089, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_14/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957089, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_14/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957089, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_14/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957089, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_14/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957089, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_14/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957089, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_14/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957089, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_14/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957089, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_14/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957089, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_14/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957089, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_15/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957089, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_15/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957089, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_15/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957089, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_15/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957089, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_15/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957089, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_15/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957089, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_15/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957090, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_15/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957090, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_15/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957090, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_15/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957090, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_15/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957090, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_15/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957090, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_15/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957090, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_15/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957090, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_15/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957090, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_15/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957090, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_16/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957090, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_16/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957090, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_16/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957090, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_16/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957090, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_16/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957090, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_16/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957090, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_16/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957090, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_16/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957090, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_16/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957090, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_16/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957091, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_16/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957091, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_16/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957091, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_16/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957091, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_16/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957091, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_16/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957091, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_16/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957091, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_17/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957091, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_17/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957091, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_17/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957091, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_17/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957091, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_17/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957091, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_17/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957091, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_17/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957091, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_17/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957091, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_17/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957091, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_17/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957091, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_17/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957091, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_17/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957092, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_17/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957092, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_17/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957092, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_17/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957092, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_17/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957092, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_18/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957092, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_18/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957092, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_18/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957092, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_18/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957092, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_18/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957092, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_18/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957092, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_18/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957092, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_18/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957092, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_18/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957092, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_18/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957092, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_18/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957092, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_18/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957092, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_18/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957092, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_18/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957092, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_18/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957093, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_18/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957093, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_19/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957093, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_19/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957093, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_19/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957093, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_19/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957093, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_19/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957093, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_19/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957093, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_19/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957093, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_19/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957093, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_19/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957093, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_19/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957093, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_19/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957093, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_19/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957093, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_19/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957093, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_19/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957093, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_19/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957093, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_19/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957093, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_20/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957093, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_20/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957094, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_20/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957094, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_20/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957094, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_20/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957094, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_20/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957094, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_20/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957094, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_20/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957094, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_20/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957094, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_20/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957094, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_20/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957094, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_20/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957094, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_20/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957094, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_20/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957094, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_20/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957094, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_20/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957094, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_21/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957094, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_21/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957094, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_21/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957094, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_21/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957094, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_21/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957095, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_21/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957095, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_21/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957095, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_21/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957095, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_21/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957095, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_21/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957095, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_21/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957095, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_21/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957095, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_21/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957095, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_21/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957095, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_21/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957095, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_21/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957095, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_22/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957095, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_22/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957095, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_22/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957095, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_22/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957095, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_22/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957095, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_22/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957095, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_22/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957096, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_22/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957096, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_22/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957096, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_22/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957096, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_22/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957096, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_22/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957096, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_22/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957096, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_22/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957096, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_22/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957096, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_22/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957096, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_23/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957096, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_23/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957096, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_23/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957096, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_23/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957096, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_23/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957096, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_23/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957096, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_23/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957096, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_23/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957096, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_23/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957096, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_23/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957097, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_23/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957097, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_23/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957097, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_23/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957097, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_23/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957097, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_23/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957097, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/encoder/layer_23/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957097, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/pooler/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957097, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "bert/pooler/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957097, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "cls/predictions/output_bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957097, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "cls/predictions/transform/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957097, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "cls/predictions/transform/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957097, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "cls/predictions/transform/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957097, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "cls/predictions/transform/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957097, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "cls/seq_relationship/output_weights"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735957097, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 868, "tensor": "cls/seq_relationship/output_bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735960695, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.0006, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 898}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735962978, "event_type": "POINT_IN_TIME", "key": "opt_epsilon", "value": 1e-06, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 938}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735962979, "event_type": "POINT_IN_TIME", "key": "opt_lamb_beta_1", "value": 0.8, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 940}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735962979, "event_type": "POINT_IN_TIME", "key": "opt_lamb_beta_2", "value": 0.996, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 941}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735962979, "event_type": "POINT_IN_TIME", "key": "opt_lamb_weight_decay_rate", "value": 0.1, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 942}}
 7: /workspace/bert/run_pretraining.py:1000: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
 7:   param_storage = optimizer._new_params.storage()
19: /workspace/bert/run_pretraining.py:1000: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
19:   param_storage = optimizer._new_params.storage()
16: /workspace/bert/run_pretraining.py:1000: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
16:   param_storage = optimizer._new_params.storage()
22: /workspace/bert/run_pretraining.py:1000: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
22:   param_storage = optimizer._new_params.storage()
29: /workspace/bert/run_pretraining.py:1000: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
29:   param_storage = optimizer._new_params.storage()
32: /workspace/bert/run_pretraining.py:1000: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
32:   param_storage = optimizer._new_params.storage()
18: /workspace/bert/run_pretraining.py:1000: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
18:   param_storage = optimizer._new_params.storage()
12: /workspace/bert/run_pretraining.py:1000: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
12:   param_storage = optimizer._new_params.storage()
33: /workspace/bert/run_pretraining.py:1000: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
33:   param_storage = optimizer._new_params.storage()
34: /workspace/bert/run_pretraining.py:1000: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
34:   param_storage = optimizer._new_params.storage()
13: /workspace/bert/run_pretraining.py:1000: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
13:   param_storage = optimizer._new_params.storage()
 8: /workspace/bert/run_pretraining.py:1000: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
 8:   param_storage = optimizer._new_params.storage()
17: /workspace/bert/run_pretraining.py:1000: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
17:   param_storage = optimizer._new_params.storage()
23: /workspace/bert/run_pretraining.py:1000: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
23:   param_storage = optimizer._new_params.storage()
 6: /workspace/bert/run_pretraining.py:1000: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
 6:   param_storage = optimizer._new_params.storage()
 5: /workspace/bert/run_pretraining.py:1000: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
 5:   param_storage = optimizer._new_params.storage()
35: /workspace/bert/run_pretraining.py:1000: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
35:   param_storage = optimizer._new_params.storage()
28: /workspace/bert/run_pretraining.py:1000: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
28:   param_storage = optimizer._new_params.storage()
24: /workspace/bert/run_pretraining.py:1000: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
24:   param_storage = optimizer._new_params.storage()
21: /workspace/bert/run_pretraining.py:1000: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
21:   param_storage = optimizer._new_params.storage()
14: /workspace/bert/run_pretraining.py:1000: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
14:   param_storage = optimizer._new_params.storage()
20: /workspace/bert/run_pretraining.py:1000: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
20:   param_storage = optimizer._new_params.storage()
11: /workspace/bert/run_pretraining.py:1000: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
11:   param_storage = optimizer._new_params.storage()
27: /workspace/bert/run_pretraining.py:1000: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
27:   param_storage = optimizer._new_params.storage()
10: /workspace/bert/run_pretraining.py:1000: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10:   param_storage = optimizer._new_params.storage()
30: /workspace/bert/run_pretraining.py:1000: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
30:   param_storage = optimizer._new_params.storage()
 9: /workspace/bert/run_pretraining.py:1000: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
 9:   param_storage = optimizer._new_params.storage()
15: /workspace/bert/run_pretraining.py:1000: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
15:   param_storage = optimizer._new_params.storage()
26: /workspace/bert/run_pretraining.py:1000: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
26:   param_storage = optimizer._new_params.storage()
 4: /workspace/bert/run_pretraining.py:1000: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
 4:   param_storage = optimizer._new_params.storage()
 2: /workspace/bert/run_pretraining.py:1000: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
 2:   param_storage = optimizer._new_params.storage()
25: /workspace/bert/run_pretraining.py:1000: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
25:   param_storage = optimizer._new_params.storage()
 0: :::MLLOG {"namespace": "", "time_ms": 1690735963014, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_steps", "value": 0, "metadata": {"file": "/workspace/bert/schedulers.py", "lineno": 86}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735963023, "event_type": "POINT_IN_TIME", "key": "opt_lamb_learning_rate_decay_poly_power", "value": 1.0, "metadata": {"file": "/workspace/bert/schedulers.py", "lineno": 87}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735963023, "event_type": "POINT_IN_TIME", "key": "start_warmup_step", "value": 0, "metadata": {"file": "/workspace/bert/schedulers.py", "lineno": 88}}
 3: /workspace/bert/run_pretraining.py:1000: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
 3:   param_storage = optimizer._new_params.storage()
31: /workspace/bert/run_pretraining.py:1000: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
31:   param_storage = optimizer._new_params.storage()
 1: /workspace/bert/run_pretraining.py:1000: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
 1:   param_storage = optimizer._new_params.storage()
 0: /workspace/bert/run_pretraining.py:1000: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
 0:   param_storage = optimizer._new_params.storage()
25: Torch distributed is available.
25: Torch distributed is initialized.
24: Torch distributed is available.
24: Torch distributed is initialized.
16: Torch distributed is available.
16: Torch distributed is initialized.
18: Torch distributed is available.
18: Torch distributed is initialized.
32: Torch distributed is available.
32: Torch distributed is initialized.
19: Torch distributed is available.
19: Torch distributed is initialized.
22: Torch distributed is available.
22: Torch distributed is initialized.
 6: Torch distributed is available.
 6: Torch distributed is initialized.
 4: Torch distributed is available.
 4: Torch distributed is initialized.
29: Torch distributed is available.
29: Torch distributed is initialized.
 8: Torch distributed is available.
 8: Torch distributed is initialized.
13: Torch distributed is available.
13: Torch distributed is initialized.
35: Torch distributed is available.
35: Torch distributed is initialized.
 7: Torch distributed is available.
 7: Torch distributed is initialized.
 1: Torch distributed is available.
 1: Torch distributed is initialized.
33: Torch distributed is available.
33: Torch distributed is initialized.
 5: Torch distributed is available.
 5: Torch distributed is initialized.
17: Torch distributed is available.
17: Torch distributed is initialized.
23: Torch distributed is available.
23: Torch distributed is initialized.
21: Torch distributed is available.
21: Torch distributed is initialized.
12: Torch distributed is available.
12: Torch distributed is initialized.
20: Torch distributed is available.
20: Torch distributed is initialized.
14: Torch distributed is available.
14: Torch distributed is initialized.
 0: Torch distributed is available.
 0: Torch distributed is initialized.
28: Torch distributed is available.
28: Torch distributed is initialized.
 9: Torch distributed is available.
 9: Torch distributed is initialized.
34: Torch distributed is available.
34: Torch distributed is initialized.
27: Torch distributed is available.
27: Torch distributed is initialized.
 2: Torch distributed is available.
 2: Torch distributed is initialized.
11: Torch distributed is available.
11: Torch distributed is initialized.
10: Torch distributed is available.
10: Torch distributed is initialized.
 3: Torch distributed is available.
 3: Torch distributed is initialized.
15: Torch distributed is available.
15: Torch distributed is initialized.
26: Torch distributed is available.
26: Torch distributed is initialized.
30: Torch distributed is available.
30: Torch distributed is initialized.
31: Torch distributed is available.
31: Torch distributed is initialized.
 5: /workspace/bert/fmha.py:77: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
 5:   Wtmp.set_(Wqkv.storage(), Wqkv.storage_offset(), Wqkv.size(), Wqkv.stride())
 5: /workspace/bert/fmha.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
 5:   Btmp.set_(Bqkv.storage(), Bqkv.storage_offset(), Bqkv.size(), Bqkv.stride())
 0: /workspace/bert/fmha.py:77: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
 0:   Wtmp.set_(Wqkv.storage(), Wqkv.storage_offset(), Wqkv.size(), Wqkv.stride())
 0: /workspace/bert/fmha.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
 0:   Btmp.set_(Bqkv.storage(), Bqkv.storage_offset(), Bqkv.size(), Bqkv.stride())
11: /workspace/bert/fmha.py:77: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
11:   Wtmp.set_(Wqkv.storage(), Wqkv.storage_offset(), Wqkv.size(), Wqkv.stride())
11: /workspace/bert/fmha.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
11:   Btmp.set_(Bqkv.storage(), Bqkv.storage_offset(), Bqkv.size(), Bqkv.stride())
 3: /workspace/bert/fmha.py:77: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
 3:   Wtmp.set_(Wqkv.storage(), Wqkv.storage_offset(), Wqkv.size(), Wqkv.stride())
 3: /workspace/bert/fmha.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
 3:   Btmp.set_(Bqkv.storage(), Bqkv.storage_offset(), Bqkv.size(), Bqkv.stride())
28: /workspace/bert/fmha.py:77: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
28:   Wtmp.set_(Wqkv.storage(), Wqkv.storage_offset(), Wqkv.size(), Wqkv.stride())
28: /workspace/bert/fmha.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
28:   Btmp.set_(Bqkv.storage(), Bqkv.storage_offset(), Bqkv.size(), Bqkv.stride())
23: /workspace/bert/fmha.py:77: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
23:   Wtmp.set_(Wqkv.storage(), Wqkv.storage_offset(), Wqkv.size(), Wqkv.stride())
23: /workspace/bert/fmha.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
23:   Btmp.set_(Bqkv.storage(), Bqkv.storage_offset(), Bqkv.size(), Bqkv.stride())
 1: /workspace/bert/fmha.py:77: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
 1:   Wtmp.set_(Wqkv.storage(), Wqkv.storage_offset(), Wqkv.size(), Wqkv.stride())
 1: /workspace/bert/fmha.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
 1:   Btmp.set_(Bqkv.storage(), Bqkv.storage_offset(), Bqkv.size(), Bqkv.stride())
14: /workspace/bert/fmha.py:77: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
14:   Wtmp.set_(Wqkv.storage(), Wqkv.storage_offset(), Wqkv.size(), Wqkv.stride())
14: /workspace/bert/fmha.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
14:   Btmp.set_(Bqkv.storage(), Bqkv.storage_offset(), Bqkv.size(), Bqkv.stride())
10: /workspace/bert/fmha.py:77: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10:   Wtmp.set_(Wqkv.storage(), Wqkv.storage_offset(), Wqkv.size(), Wqkv.stride())
10: /workspace/bert/fmha.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
10:   Btmp.set_(Bqkv.storage(), Bqkv.storage_offset(), Bqkv.size(), Bqkv.stride())
24: /workspace/bert/fmha.py:77: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
24:   Wtmp.set_(Wqkv.storage(), Wqkv.storage_offset(), Wqkv.size(), Wqkv.stride())
24: /workspace/bert/fmha.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
24:   Btmp.set_(Bqkv.storage(), Bqkv.storage_offset(), Bqkv.size(), Bqkv.stride())
 8: /workspace/bert/fmha.py:77: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
 8:   Wtmp.set_(Wqkv.storage(), Wqkv.storage_offset(), Wqkv.size(), Wqkv.stride())
 8: /workspace/bert/fmha.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
 8:   Btmp.set_(Bqkv.storage(), Bqkv.storage_offset(), Bqkv.size(), Bqkv.stride())
22: /workspace/bert/fmha.py:77: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
22:   Wtmp.set_(Wqkv.storage(), Wqkv.storage_offset(), Wqkv.size(), Wqkv.stride())
22: /workspace/bert/fmha.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
22:   Btmp.set_(Bqkv.storage(), Bqkv.storage_offset(), Bqkv.size(), Bqkv.stride())
34: /workspace/bert/fmha.py:77: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
34:   Wtmp.set_(Wqkv.storage(), Wqkv.storage_offset(), Wqkv.size(), Wqkv.stride())
34: /workspace/bert/fmha.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
34:   Btmp.set_(Bqkv.storage(), Bqkv.storage_offset(), Bqkv.size(), Bqkv.stride())
17: /workspace/bert/fmha.py:77: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
17:   Wtmp.set_(Wqkv.storage(), Wqkv.storage_offset(), Wqkv.size(), Wqkv.stride())
17: /workspace/bert/fmha.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
17:   Btmp.set_(Bqkv.storage(), Bqkv.storage_offset(), Bqkv.size(), Bqkv.stride())
16: /workspace/bert/fmha.py:77: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
16:   Wtmp.set_(Wqkv.storage(), Wqkv.storage_offset(), Wqkv.size(), Wqkv.stride())
16: /workspace/bert/fmha.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
16:   Btmp.set_(Bqkv.storage(), Bqkv.storage_offset(), Bqkv.size(), Bqkv.stride())
32: /workspace/bert/fmha.py:77: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
32:   Wtmp.set_(Wqkv.storage(), Wqkv.storage_offset(), Wqkv.size(), Wqkv.stride())
32: /workspace/bert/fmha.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
32:   Btmp.set_(Bqkv.storage(), Bqkv.storage_offset(), Bqkv.size(), Bqkv.stride())
35: /workspace/bert/fmha.py:77: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
35:   Wtmp.set_(Wqkv.storage(), Wqkv.storage_offset(), Wqkv.size(), Wqkv.stride())
35: /workspace/bert/fmha.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
35:   Btmp.set_(Bqkv.storage(), Bqkv.storage_offset(), Bqkv.size(), Bqkv.stride())
30: /workspace/bert/fmha.py:77: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
30:   Wtmp.set_(Wqkv.storage(), Wqkv.storage_offset(), Wqkv.size(), Wqkv.stride())
30: /workspace/bert/fmha.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
30:   Btmp.set_(Bqkv.storage(), Bqkv.storage_offset(), Bqkv.size(), Bqkv.stride())
31: /workspace/bert/fmha.py:77: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
31:   Wtmp.set_(Wqkv.storage(), Wqkv.storage_offset(), Wqkv.size(), Wqkv.stride())
31: /workspace/bert/fmha.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
31:   Btmp.set_(Bqkv.storage(), Bqkv.storage_offset(), Bqkv.size(), Bqkv.stride())
18: /workspace/bert/fmha.py:77: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
18:   Wtmp.set_(Wqkv.storage(), Wqkv.storage_offset(), Wqkv.size(), Wqkv.stride())
18: /workspace/bert/fmha.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
18:   Btmp.set_(Bqkv.storage(), Bqkv.storage_offset(), Bqkv.size(), Bqkv.stride())
 4: /workspace/bert/fmha.py:77: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
 4:   Wtmp.set_(Wqkv.storage(), Wqkv.storage_offset(), Wqkv.size(), Wqkv.stride())
 4: /workspace/bert/fmha.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
 4:   Btmp.set_(Bqkv.storage(), Bqkv.storage_offset(), Bqkv.size(), Bqkv.stride())
 2: /workspace/bert/fmha.py:77: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
 2:   Wtmp.set_(Wqkv.storage(), Wqkv.storage_offset(), Wqkv.size(), Wqkv.stride())
 2: /workspace/bert/fmha.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
 2:   Btmp.set_(Bqkv.storage(), Bqkv.storage_offset(), Bqkv.size(), Bqkv.stride())
12: /workspace/bert/fmha.py:77: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
12:   Wtmp.set_(Wqkv.storage(), Wqkv.storage_offset(), Wqkv.size(), Wqkv.stride())
12: /workspace/bert/fmha.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
12:   Btmp.set_(Bqkv.storage(), Bqkv.storage_offset(), Bqkv.size(), Bqkv.stride())
15: /workspace/bert/fmha.py:77: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
15:   Wtmp.set_(Wqkv.storage(), Wqkv.storage_offset(), Wqkv.size(), Wqkv.stride())
15: /workspace/bert/fmha.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
15:   Btmp.set_(Bqkv.storage(), Bqkv.storage_offset(), Bqkv.size(), Bqkv.stride())
21: /workspace/bert/fmha.py:77: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
21:   Wtmp.set_(Wqkv.storage(), Wqkv.storage_offset(), Wqkv.size(), Wqkv.stride())
21: /workspace/bert/fmha.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
21:   Btmp.set_(Bqkv.storage(), Bqkv.storage_offset(), Bqkv.size(), Bqkv.stride())
 7: /workspace/bert/fmha.py:77: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
 7:   Wtmp.set_(Wqkv.storage(), Wqkv.storage_offset(), Wqkv.size(), Wqkv.stride())
 7: /workspace/bert/fmha.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
 7:   Btmp.set_(Bqkv.storage(), Bqkv.storage_offset(), Bqkv.size(), Bqkv.stride())
 9: /workspace/bert/fmha.py:77: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
 9:   Wtmp.set_(Wqkv.storage(), Wqkv.storage_offset(), Wqkv.size(), Wqkv.stride())
 9: /workspace/bert/fmha.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
 9:   Btmp.set_(Bqkv.storage(), Bqkv.storage_offset(), Bqkv.size(), Bqkv.stride())
19: /workspace/bert/fmha.py:77: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
19:   Wtmp.set_(Wqkv.storage(), Wqkv.storage_offset(), Wqkv.size(), Wqkv.stride())
19: /workspace/bert/fmha.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
19:   Btmp.set_(Bqkv.storage(), Bqkv.storage_offset(), Bqkv.size(), Bqkv.stride())
27: /workspace/bert/fmha.py:77: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
27:   Wtmp.set_(Wqkv.storage(), Wqkv.storage_offset(), Wqkv.size(), Wqkv.stride())
27: /workspace/bert/fmha.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
27:   Btmp.set_(Bqkv.storage(), Bqkv.storage_offset(), Bqkv.size(), Bqkv.stride())
20: /workspace/bert/fmha.py:77: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
20:   Wtmp.set_(Wqkv.storage(), Wqkv.storage_offset(), Wqkv.size(), Wqkv.stride())
20: /workspace/bert/fmha.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
20:   Btmp.set_(Bqkv.storage(), Bqkv.storage_offset(), Bqkv.size(), Bqkv.stride())
26: /workspace/bert/fmha.py:77: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
26:   Wtmp.set_(Wqkv.storage(), Wqkv.storage_offset(), Wqkv.size(), Wqkv.stride())
26: /workspace/bert/fmha.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
26:   Btmp.set_(Bqkv.storage(), Bqkv.storage_offset(), Bqkv.size(), Bqkv.stride())
25: /workspace/bert/fmha.py:77: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
25:   Wtmp.set_(Wqkv.storage(), Wqkv.storage_offset(), Wqkv.size(), Wqkv.stride())
25: /workspace/bert/fmha.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
25:   Btmp.set_(Bqkv.storage(), Bqkv.storage_offset(), Bqkv.size(), Bqkv.stride())
13: /workspace/bert/fmha.py:77: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
13:   Wtmp.set_(Wqkv.storage(), Wqkv.storage_offset(), Wqkv.size(), Wqkv.stride())
13: /workspace/bert/fmha.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
13:   Btmp.set_(Bqkv.storage(), Bqkv.storage_offset(), Bqkv.size(), Bqkv.stride())
 6: /workspace/bert/fmha.py:77: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
 6:   Wtmp.set_(Wqkv.storage(), Wqkv.storage_offset(), Wqkv.size(), Wqkv.stride())
 6: /workspace/bert/fmha.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
 6:   Btmp.set_(Bqkv.storage(), Bqkv.storage_offset(), Bqkv.size(), Bqkv.stride())
33: /workspace/bert/fmha.py:77: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
33:   Wtmp.set_(Wqkv.storage(), Wqkv.storage_offset(), Wqkv.size(), Wqkv.stride())
33: /workspace/bert/fmha.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
33:   Btmp.set_(Bqkv.storage(), Bqkv.storage_offset(), Bqkv.size(), Bqkv.stride())
29: /workspace/bert/fmha.py:77: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
29:   Wtmp.set_(Wqkv.storage(), Wqkv.storage_offset(), Wqkv.size(), Wqkv.stride())
29: /workspace/bert/fmha.py:81: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
29:   Btmp.set_(Bqkv.storage(), Bqkv.storage_offset(), Bqkv.size(), Bqkv.stride())
26: /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:342: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
26:   warnings.warn(
19: /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:342: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
19:   warnings.warn(
23: /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:342: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
23:   warnings.warn(
24: /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:342: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
24:   warnings.warn(
28: /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:342: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
28:   warnings.warn(
22: /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:342: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
22:   warnings.warn(
20: /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:342: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
20:   warnings.warn(
18: /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:342: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
18:   warnings.warn(
21: /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:342: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
21:   warnings.warn(
25: /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:342: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
25:   warnings.warn(
29: /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:342: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
29:   warnings.warn(
16: /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:342: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
16:   warnings.warn(
14: /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:342: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
14:   warnings.warn(
13: /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:342: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
13:   warnings.warn(
15: /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:342: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
15:   warnings.warn(
27: /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:342: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
27:   warnings.warn(
17: /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:342: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
17:   warnings.warn(
30: /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:342: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
30:   warnings.warn(
10: /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:342: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
10:   warnings.warn(
 2: /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:342: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 2:   warnings.warn(
32: /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:342: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
32:   warnings.warn(
12: /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:342: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
12:   warnings.warn(
 8: /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:342: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 8:   warnings.warn(
 6: /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:342: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 6:   warnings.warn(
 0: /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:342: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 0:   warnings.warn(
31: /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:342: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
31:   warnings.warn(
 4: /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:342: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 4:   warnings.warn(
11: /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:342: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
11:   warnings.warn(
34: /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:342: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
34:   warnings.warn(
 1: /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:342: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 1:   warnings.warn(
 9: /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:342: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 9:   warnings.warn(
33: /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:342: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
33:   warnings.warn(
 7: /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:342: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 7:   warnings.warn(
35: /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:342: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
35:   warnings.warn(
 5: /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:342: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 5:   warnings.warn(
 3: /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:342: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 3:   warnings.warn(
 0: :::MLLOG {"namespace": "", "time_ms": 1690735971808, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1606}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735971808, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1606}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735971829, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1619, "epoch_num": 0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690735971830, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1621, "first_epoch_num": 1, "epoch_count": 1}}
 0: parsed args:
 0: Namespace(allreduce_post_accumulation=True, allreduce_post_accumulation_fp16=True, average_packing_rate=2, bert_config_path='/workspace/phase1/bert_config.json', bert_model='bert-large-uncased', bypass_amp=False, cache_eval_data=True, checkpoint_activations=False, cuda_graph_mode='segmented', ddp_type='apex', dense_seq_output=True, device=device(type='cuda', index=0), disable_apex_softmax=False, disable_fuse_mask=False, disable_fuse_qkv=False, disable_fuse_scale=False, distributed_lamb=True, do_train=True, dwu_e5m2_allgather=False, dwu_group_size=0, dwu_num_ag_pg=1, dwu_num_ar_pg=1, dwu_num_blocks=1, dwu_num_chunks=1, dwu_num_rs_pg=1, dwu_overlap_reductions=False, enable_fuse_dropout=False, enable_stream=False, eval_batch_size=16, eval_dir='/workspace/evaldata', eval_iter_samples=200000, eval_iter_start_samples=400000, exchange_padding=False, fp16=True, fused_bias_fc=True, fused_bias_fc_loss_head=False, fused_bias_mha=True, fused_dropout_add=True, fused_gelu_bias=False, fused_gemm_gelu=True, fused_mha=False, 
 0: gradient_accumulation_steps=1, init_checkpoint='/workspace/phase1/tf1_ckpt/model.ckpt-28252.pt', init_tf_checkpoint=None, input_dir='/workspace/data', keep_n_most_recent_checkpoints=20, learning_rate=0.0006, local_rank=0, log_freq=0.0, loss_scale=0.0, max_iterations_per_graph=4, max_pack_factor=3, max_predictions_per_seq=76, max_samples_termination=1000000000.0, max_seq_length=512, max_steps=100000.0, min_samples_to_start_checkpoints=3000000, n_gpu=36, num_epochs_to_generate_seeds_for=2, num_eval_examples=10000, num_samples_per_checkpoint=500000, opt_lamb_beta_1=0.8, opt_lamb_beta_2=0.996, order_samples=False, output_dir='/results', packed_samples=True, pad=False, pad_fmha=True, phase2=True, resume_from_checkpoint=False, resume_step=0, seed=29586, skip_checkpoint=True, start_warmup_step=0.0, synthetic_input=False, target_mlm_accuracy=0.72, train_batch_size=36, train_mlm_accuracy_window_size=0, unpad=False, unpad_fmha=False, use_cuda_graph=False, use_ddp=False, use_env=False, use_gradient_as_bucket_view=False,
 0:  use_transformer_engine2=False, warmup_proportion=0.0, warmup_steps=0.0, weight_decay_rate=0.1)
 0: epoch: 1
 0: :::MLLOG {"namespace": "", "time_ms": 1690735971830, "event_type": "POINT_IN_TIME", "key": "data_file", "value": "/workspace/data/part_00502.hdf5", "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1655}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690736031863, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.38316813111305237, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1836, "epoch_num": 401632}}
 0: {'global_steps': 155, 'eval_loss': 4.01049280166626, 'eval_mlm_accuracy': 0.38316813111305237}
 0: :::MLLOG {"namespace": "", "time_ms": 1690736061306, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.3931135833263397, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1836, "epoch_num": 600661}}
 0: {'global_steps': 232, 'eval_loss': 3.9053189754486084, 'eval_mlm_accuracy': 0.3931135833263397}
 0: :::MLLOG {"namespace": "", "time_ms": 1690736090746, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.41561993956565857, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1836, "epoch_num": 800349}}
 0: {'global_steps': 309, 'eval_loss': 3.6679155826568604, 'eval_mlm_accuracy': 0.41561993956565857}
 0: :::MLLOG {"namespace": "", "time_ms": 1690736120216, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.48555582761764526, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1836, "epoch_num": 999785}}
 0: {'global_steps': 386, 'eval_loss': 3.102304220199585, 'eval_mlm_accuracy': 0.48555582761764526}
 0: :::MLLOG {"namespace": "", "time_ms": 1690736149937, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.5761952996253967, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1836, "epoch_num": 1199203}}
 0: {'global_steps': 463, 'eval_loss': 2.37488055229187, 'eval_mlm_accuracy': 0.5761952996253967}
 0: :::MLLOG {"namespace": "", "time_ms": 1690736180089, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.6454353332519531, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1836, "epoch_num": 1401022}}
 0: {'global_steps': 541, 'eval_loss': 1.8296051025390625, 'eval_mlm_accuracy': 0.6454353332519531}
 0: :::MLLOG {"namespace": "", "time_ms": 1690736209578, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.6988966464996338, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1836, "epoch_num": 1600727}}
 0: {'global_steps': 618, 'eval_loss': 1.4402083158493042, 'eval_mlm_accuracy': 0.6988966464996338}
 0: :::MLLOG {"namespace": "", "time_ms": 1690736239054, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7088817954063416, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1836, "epoch_num": 1800340}}
 0: {'global_steps': 695, 'eval_loss': 1.3729941844940186, 'eval_mlm_accuracy': 0.7088817954063416}
 0: :::MLLOG {"namespace": "", "time_ms": 1690736268556, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7125106453895569, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1836, "epoch_num": 2000586}}
 0: {'global_steps': 772, 'eval_loss': 1.3515766859054565, 'eval_mlm_accuracy': 0.7125106453895569}
 0: :::MLLOG {"namespace": "", "time_ms": 1690736298050, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7142830491065979, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1836, "epoch_num": 2199911}}
 0: {'global_steps': 849, 'eval_loss': 1.3369909524917603, 'eval_mlm_accuracy': 0.7142830491065979}
 0: :::MLLOG {"namespace": "", "time_ms": 1690736327546, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7159783840179443, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1836, "epoch_num": 2399745}}
 0: {'global_steps': 926, 'eval_loss': 1.3277006149291992, 'eval_mlm_accuracy': 0.7159783840179443}
 0: :::MLLOG {"namespace": "", "time_ms": 1690736357458, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7169124484062195, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1836, "epoch_num": 2601853}}
 0: {'global_steps': 1004, 'eval_loss': 1.3220138549804688, 'eval_mlm_accuracy': 0.7169124484062195}
 0: :::MLLOG {"namespace": "", "time_ms": 1690736387464, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7179772853851318, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1836, "epoch_num": 2801231}}
 0: {'global_steps': 1081, 'eval_loss': 1.3188241720199585, 'eval_mlm_accuracy': 0.7179772853851318}
 0: :::MLLOG {"namespace": "", "time_ms": 1690736416955, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7187408804893494, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1836, "epoch_num": 3001037}}
 0: {'global_steps': 1158, 'eval_loss': 1.312148094177246, 'eval_mlm_accuracy': 0.7187408804893494}
 0: :::MLLOG {"namespace": "", "time_ms": 1690736446428, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7190187573432922, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1836, "epoch_num": 3200330}}
 0: {'global_steps': 1235, 'eval_loss': 1.3094927072525024, 'eval_mlm_accuracy': 0.7190187573432922}
 0: :::MLLOG {"namespace": "", "time_ms": 1690736475890, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7198944687843323, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1836, "epoch_num": 3399538}}
 0: {'global_steps': 1312, 'eval_loss': 1.3072712421417236, 'eval_mlm_accuracy': 0.7198944687843323}
 0: :::MLLOG {"namespace": "", "time_ms": 1690736505401, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7201676368713379, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1836, "epoch_num": 3598626}}
 0: {'global_steps': 1389, 'eval_loss': 1.304614782333374, 'eval_mlm_accuracy': 0.7201676368713379}
 0: 0.720168 > 0.720000, Target MLM Accuracy reached at 1389
 0: (1, 1389.0) {'final_loss': 0.0}
 0: :::MLLOG {"namespace": "", "time_ms": 1690736505402, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1966, "first_epoch_num": 1}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690736505402, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1969, "epoch_num": 3598626}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690736505402, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 3598626, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1977}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690736505402, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 10000, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1980}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690736505402, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1983, "status": "success"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1690736505402, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 6744.128822001365, "epoch_num": 3598626}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1987, "step": [2, 1389]}}
 0: {'e2e_time': 553.21248960495, 'training_sequences_per_second': 481926.6455334993, 'final_loss': 0.0, 'raw_train_time': 537.8411889076233}
12: ENDING TIMING RUN AT 2023-07-30 01:01:47 PM
12: RESULT,bert,4617,561,nnisbet,2023-07-30 12:52:26 PM
 2: ENDING TIMING RUN AT 2023-07-30 01:01:47 PM
 2: RESULT,bert,1202,561,nnisbet,2023-07-30 12:52:26 PM
24: ENDING TIMING RUN AT 2023-07-30 01:01:47 PM
24: RESULT,bert,12319,561,nnisbet,2023-07-30 12:52:26 PM
34: ENDING TIMING RUN AT 2023-07-30 01:01:47 PM
34: RESULT,bert,7031,561,nnisbet,2023-07-30 12:52:26 PM
26: ENDING TIMING RUN AT 2023-07-30 01:01:47 PM
26: RESULT,bert,9854,561,nnisbet,2023-07-30 12:52:26 PM
 6: ENDING TIMING RUN AT 2023-07-30 01:01:47 PM
 6: RESULT,bert,28063,561,nnisbet,2023-07-30 12:52:26 PM
16: ENDING TIMING RUN AT 2023-07-30 01:01:47 PM
16: RESULT,bert,16527,561,nnisbet,2023-07-30 12:52:26 PM
28: ENDING TIMING RUN AT 2023-07-30 01:01:47 PM
28: RESULT,bert,18212,561,nnisbet,2023-07-30 12:52:26 PM
20: ENDING TIMING RUN AT 2023-07-30 01:01:47 PM
20: RESULT,bert,6494,561,nnisbet,2023-07-30 12:52:26 PM
10: ENDING TIMING RUN AT 2023-07-30 01:01:47 PM
10: RESULT,bert,15423,561,nnisbet,2023-07-30 12:52:26 PM
 0: ENDING TIMING RUN AT 2023-07-30 01:01:47 PM
 0: RESULT,bert,29586,562,nnisbet,2023-07-30 12:52:25 PM
30: ENDING TIMING RUN AT 2023-07-30 01:01:47 PM
30: RESULT,bert,2356,561,nnisbet,2023-07-30 12:52:26 PM
15: ENDING TIMING RUN AT 2023-07-30 01:01:47 PM
15: RESULT,bert,5811,561,nnisbet,2023-07-30 12:52:26 PM
18: ENDING TIMING RUN AT 2023-07-30 01:01:47 PM
18: RESULT,bert,30340,561,nnisbet,2023-07-30 12:52:26 PM
 7: ENDING TIMING RUN AT 2023-07-30 01:01:47 PM
 7: RESULT,bert,24546,561,nnisbet,2023-07-30 12:52:26 PM
 1: ENDING TIMING RUN AT 2023-07-30 01:01:47 PM
 1: RESULT,bert,7036,562,nnisbet,2023-07-30 12:52:25 PM
22: ENDING TIMING RUN AT 2023-07-30 01:01:47 PM
22: RESULT,bert,23777,561,nnisbet,2023-07-30 12:52:26 PM
13: ENDING TIMING RUN AT 2023-07-30 01:01:47 PM
13: RESULT,bert,2756,561,nnisbet,2023-07-30 12:52:26 PM
21: ENDING TIMING RUN AT 2023-07-30 01:01:47 PM
21: RESULT,bert,26427,561,nnisbet,2023-07-30 12:52:26 PM
 3: ENDING TIMING RUN AT 2023-07-30 01:01:47 PM
 3: RESULT,bert,23471,561,nnisbet,2023-07-30 12:52:26 PM
35: ENDING TIMING RUN AT 2023-07-30 01:01:47 PM
35: RESULT,bert,8428,561,nnisbet,2023-07-30 12:52:26 PM
27: ENDING TIMING RUN AT 2023-07-30 01:01:47 PM
27: RESULT,bert,3899,561,nnisbet,2023-07-30 12:52:26 PM
19: ENDING TIMING RUN AT 2023-07-30 01:01:47 PM
19: RESULT,bert,7919,561,nnisbet,2023-07-30 12:52:26 PM
11: ENDING TIMING RUN AT 2023-07-30 01:01:47 PM
11: RESULT,bert,6978,561,nnisbet,2023-07-30 12:52:26 PM
29: ENDING TIMING RUN AT 2023-07-30 01:01:47 PM
29: RESULT,bert,3869,561,nnisbet,2023-07-30 12:52:26 PM
17: ENDING TIMING RUN AT 2023-07-30 01:01:47 PM
17: RESULT,bert,16256,561,nnisbet,2023-07-30 12:52:26 PM
14: ENDING TIMING RUN AT 2023-07-30 01:01:47 PM
14: RESULT,bert,11473,561,nnisbet,2023-07-30 12:52:26 PM
31: ENDING TIMING RUN AT 2023-07-30 01:01:47 PM
31: RESULT,bert,14900,561,nnisbet,2023-07-30 12:52:26 PM
 4: ENDING TIMING RUN AT 2023-07-30 01:01:47 PM
 4: RESULT,bert,23253,561,nnisbet,2023-07-30 12:52:26 PM
32: ENDING TIMING RUN AT 2023-07-30 01:01:47 PM
32: RESULT,bert,8653,561,nnisbet,2023-07-30 12:52:26 PM
 8: ENDING TIMING RUN AT 2023-07-30 01:01:47 PM
 8: RESULT,bert,20170,561,nnisbet,2023-07-30 12:52:26 PM
23: ENDING TIMING RUN AT 2023-07-30 01:01:48 PM
23: RESULT,bert,22358,562,nnisbet,2023-07-30 12:52:26 PM
25: ENDING TIMING RUN AT 2023-07-30 01:01:48 PM
25: RESULT,bert,30634,562,nnisbet,2023-07-30 12:52:26 PM
33: ENDING TIMING RUN AT 2023-07-30 01:01:48 PM
33: RESULT,bert,30789,562,nnisbet,2023-07-30 12:52:26 PM
 5: ENDING TIMING RUN AT 2023-07-30 01:01:48 PM
 5: RESULT,bert,15650,562,nnisbet,2023-07-30 12:52:26 PM
 9: ENDING TIMING RUN AT 2023-07-30 01:01:48 PM
 9: RESULT,bert,17537,562,nnisbet,2023-07-30 12:52:26 PM
